{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'>RAG: Indexing</h2>\n",
    "\n",
    "<p align='center'>\n",
    "  <img src='./indexing_and_retrieval.png' width=480 style='border-radius: 14px;' />\n",
    "</p>\n",
    "\n",
    "<p align='justify'>&nbsp;&nbsp;&nbsp;Neste notebook, pretendo focar na parte de Indexação desse mergulho em RAG.</p>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setting Up</h3>\n",
    "\n",
    "> Notas:\n",
    "> - [Contador de tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) considerando [~4 char / token](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n",
    "> - [Modelos de embedding](https://python.langchain.com/docs/integrations/text_embedding/openai)\n",
    "> - [Similaridade dos Cossenos](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) recomendado (1 indica que e identico) para embeddings da OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\info\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Deep Learning/Diver')\n",
    "\n",
    "from config.constants import *\n",
    "from config.config import *\n",
    "from config.utils import *\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = f'RAG'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = settings.LANGSMITH_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokens Counter</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question tokens: 8\n"
     ]
    }
   ],
   "source": [
    "question = 'What kind of movies do I like?'\n",
    "document = 'Naomi Lago loves sci-fi movies, as well as thrillers and comedy.'\n",
    "\n",
    "def tokens_counter(text: str, encoding: str) -> int:\n",
    "  '''Returns the number of tokens in a given text.'''\n",
    "  \n",
    "  encoding = tiktoken.get_encoding(encoding)\n",
    "  \n",
    "  return len(encoding.encode(text))\n",
    "\n",
    "print(f'Question tokens: {tokens_counter(question, \"cl100k_base\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a href='https://python.langchain.com/docs/integrations/text_embedding/openai'>Embeddings and Similarity</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.48588228298678326\n"
     ]
    }
   ],
   "source": [
    "def similarity_calculator(vector_a: Sequence, vector_b: Sequence) -> float:\n",
    "  '''Returns the calculated similarity between two vectors.'''\n",
    "\n",
    "  dot_product = np.dot(vector_a, vector_b)\n",
    "  norm_a = np.linalg.norm(vector_a)\n",
    "  norm_b = np.linalg.norm(vector_b)\n",
    "  \n",
    "  return dot_product / (norm_a * norm_b)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "query_result = embeddings.embed_query(question)\n",
    "document_result = embeddings.embed_query(document)\n",
    "\n",
    "similarity = similarity_calculator(query_result, document_result)\n",
    "\n",
    "print(f'Cosine similarity: {similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a href='https://python.langchain.com/docs/integrations/document_loaders/'>Document Loaders</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\n\\nNaomi Lago - Data dives and beyond\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nData dives and beyond\\n\\n\\nWelcome to my personal blog. Here I’ll share my learning notes and some great resources. Join me as we journey through this fascinating realm, uncovering valuable resources together.\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCategoriesAll (9)COMPUTER VISION (1)DATA SCIENCE (5)DEEP LEARNING (3)GEOLOCATION (1)NLP (3)VECTOR SEARCH (2)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPreprocessing Unstructured Data\\n\\n\\n\\n\\n\\n\\nDATA SCIENCE\\n\\n\\nNLP\\n\\n\\n\\nReady for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.\\n\\n\\n\\n\\n\\nMay 1, 2024\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRecognizing Handwriting Digits\\n\\n\\n\\n\\n\\n\\nDEEP LEARNING\\n\\n\\nDATA SCIENCE\\n\\n\\n\\nToday, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.\\n\\n\\n\\n\\n\\nSep 11, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVector Search with Facebook AI\\n\\n\\n\\n\\n\\n\\nNLP\\n\\n\\nVECTOR SEARCH\\n\\n\\n\\nIn today’s post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.\\n\\n\\n\\n\\n\\nSep 10, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImage classification with Deep Learning made easy\\n\\n\\n\\n\\n\\n\\nCOMPUTER VISION\\n\\n\\nDEEP LEARNING\\n\\n\\n\\nToday is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.\\n\\n\\n\\n\\n\\nSep 9, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInformation Retrieval with Vector Search\\n\\n\\n\\n\\n\\n\\nVECTOR SEARCH\\n\\n\\n\\nLet’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.\\n\\n\\n\\n\\n\\nSep 8, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPerceptron: Fundamentals and Applications\\n\\n\\n\\n\\n\\n\\nDEEP LEARNING\\n\\n\\nDATA SCIENCE\\n\\n\\n\\nDive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.\\n\\n\\n\\n\\n\\nSep 7, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGetting geographic coordinates with Python and Google\\n\\n\\n\\n\\n\\n\\nGEOLOCATION\\n\\n\\nDATA SCIENCE\\n\\n\\n\\nHave you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.\\n\\n\\n\\n\\n\\nSep 6, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Natural Language Processing?\\n\\n\\n\\n\\n\\n\\nNLP\\n\\n\\n\\nIn this post I’ll be presenting an exciting subfield of AI that joins Computational Linguistics and Computer Science.\\n\\n\\n\\n\\n\\nSep 5, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Data Science, after all?\\n\\n\\n\\n\\n\\n\\nDATA SCIENCE\\n\\n\\n\\nI bet you may have heard about Data Sciece. But do you really understand what is it? In this post we’ll be exploring this field and its applications.\\n\\n\\n\\n\\n\\nSep 4, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\nNo matching items\\n\\n\\n\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n', metadata={'source': 'https://naomilago.com/', 'title': 'Naomi Lago - Data dives and beyond', 'description': ' Welcome to my personal blog. Here I’ll share my learning notes and some great resources. Join me as we journey through this fascinating realm, uncovering valuable resources together.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\nOops, something lost\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOops, looks like the page is lost.\\nThis is not a fault, just an accident that was not intentional.\\n\\n\\n\\n\\n', metadata={'source': 'https://naomilago.com/about.htmlhttps://naomilago.com/posts/so-what-is-data-science/', 'title': 'Oops, something lost', 'description': 'Oops, looks like the page is lost. Start your website on the cheap.', 'language': 'en-us'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago - What is Natural Language Processing?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nWhat is Natural Language Processing?\\n\\n\\nIn this post I’ll be presenting an exciting subfield of AI that joins Computational Linguistics and Computer Science.\\n        \\n\\n\\nNLP\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 5, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Have you ever heard about NLP? This acronym refer to Natural Language Processing and this is a well discussed field nowadays. That’s what we’ll be talking today.\\n\\n\\n\\xa0\\xa0\\xa0This is a field of research in Computer Science and Artificial Intelligence joined with Computational Linguistics. It can be abstracted to processing human languages - like English, Portuguese, Germna etc. This process involves translating the natural language to the machine language, with techniques that transform text into number for example.\\n\\n\\n\\xa0\\xa0\\xa0Using metadata, the computer can persist knowledge about our world and help us to optimize amazing things. Let’s see this example using the famous technology GPT-3:\\n\\n\\n\\n\\n\\nAsking for a dinamic code for a specific task\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0GPT-3 can answer questions more complex than the shown above, but we can note the precision in your answers, i.e.\\xa0knowing the user wants a code built with Python, not JavaScript, and writing the answer based on the language syntax the user requested. In addition, this model made sure to explain each part of what it delivered and all of this using an ethical and respectful voice.\\n\\n\\n\\xa0\\xa0\\xa0Well, NLP is not only applied in chatbots as we saw for sure. Now, let’s take a look at this table that show us a variety of applications in this field:\\n\\n\\n\\n\\n\\nNatural Language Processing in Action published by Manning\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Natural Language Processing is an amazing and complex field. There are many cool concepts behind and I’d like to share all that I’ve been seeing and learning.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n', metadata={'source': 'https://naomilago.com/posts/what-is-natural-language-processing/', 'title': 'Naomi Lago - What is Natural Language Processing?', 'description': ' In this post I’ll be presenting an exciting subfield of AI that joins Computational Linguistics and Computer Science.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago - Getting geographic coordinates with Python and Google\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nGetting geographic coordinates with Python and Google\\n\\n\\nHave you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.\\n        \\n\\n\\nGEOLOCATION\\nDATA SCIENCE\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 6, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Recently I faced a project that, in the Exploratory Data Analysis (EDA) step, I decided to plot a choropleth map that would show the population distribution by Brazilian states. In order to do so, I prior get the coordinates and follow by defining the libraries for the plot.\\n\\n\\n\\xa0\\xa0\\xa0Keeping in mind that there are two main processes, in this post I describe how I went through the process of getting the data by using a Google API at Google Maps Platform.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nEnvironment preparation\\n\\n\\nAPI integration\\n\\n\\nCoordinates collection\\n\\n\\nData Export\\n\\n\\nData handling\\n\\n\\nConclusion\\n\\n\\nReferences\\n\\n\\n\\n\\nEnvironment prepatation\\n\\n\\n\\n\\xa0\\xa0\\xa0For this content, I’ll be using a generic dataset containing two columns: UF and MUNICIPIO that refers to state and city respectfully.\\n\\n\\nThe shape is (82391, 2).\\n\\n\\n\\n\\n\\nSample with 5 entries\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0We want our query to the API request to do not contain null values and, for that reason, I’ll be verifying how many are unfilled and delete them in case are a fine amount.\\n\\n\\n\\n\\n\\nHandling null values\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0There were only one value in each column and they were removed. Now, let’s make the API integration.\\n\\n\\n\\nAPI integration\\n\\n\\n\\n\\xa0\\xa0\\xa0Before starting, it’s important to make sure that the library googlemaps is installed - what I’ll be responsible for the integration. Therefore, I can run the following snippet on my terminal:\\n\\n\\n\\n%pip install -Uqq googlemaps\\n\\nNote: you may need to restart the kernel to use updated packages.\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0After configuring your cloud environment at Google, grab your API key. I recommend storing it in a file .json instead of inserting directly to your code. Another option is using the environment variables on your machine. For this example, I stored in my path /credentials/api_keys.json with the following format:\\n\\n\\n\\n\\n\\nJSON format for storing the API key\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0By doing that, we can now make our first request. Initially, I’ll be requesting only one static defined request as for ‘São Paulo, SP’ and store the results in the latitude and longitude variables.\\n\\n\\n\\n\\n\\nSample request\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0In te first lines, after importing the googlemaps library, I read the file where the key was stored and authenticate through the client. I also define the query and wait for the respons - storing in a variable called result. In order to get the proper coordinates, I can filter by getting only these attributes.\\n\\n\\n\\xa0\\xa0\\xa0Now that the API was tested and integrated - the core of our task - we can keep going by collecting for all the entries of the dataset.\\n\\n\\n\\nCoordinates collection\\n\\n\\n\\n\\xa0\\xa0\\xa0The idea now consists in get through each line in our entries and use city and state as origins for the queries. In order to do that, there was defined two empty lists to store the responses on latitude and longitude and the same previous logic was set in a loop; that’ll be responsible for getting cities and states of each line, create the query, make the request and return the data we need.\\n\\n\\n\\n\\n\\nRunning through every entry and storing the results\\n\\n\\n\\n\\nNote that I added a try/except just in case a request on a specific line doesn’t find the coordinates, fill with a null value and don’t stop the run - that it’ll make the code crash.\\n\\n\\n\\nData export\\n\\n\\n\\n\\xa0\\xa0\\xa0In the end, we’ll have the coordinated stored via the two lists we defined earlier. To prevent this data to be lost and also use them in another place on another time, I’ll be exporting the results in a .json file.\\n\\n\\n\\n\\n\\nSaving the results\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0So, a dictionary as defined containig the two lists. We can now successfully treat our data and have this task done.\\n\\n\\n\\nData handling\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that we have a json file with the coordinates, we can initially import this file as dataframe and concatenate in our main one:\\n\\n\\n\\n\\n\\nJoining the results to the main dataframe\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, let’s verify if there are any null value and remove them just in case. These values are coming when the request didn’t return any coordinates, falling in the exception and filling with None. We saw this here.\\n\\n\\n\\n\\n\\nHandling null values from the API\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0As we can see, we got 192 cases where it didn’t return any result. They were removed as the proportion is still small compared to the dataframe size.\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0In the end, we were able to complete the task and now we have two new columns: latitude and longitude. We can use them to further geographic analyses and ploting te coordinates in maps.\\n\\n\\n\\n\\n\\nFinal dataframe with results\\n\\n\\n\\n\\nReferences\\n\\n\\n\\n\\xa0\\xa0\\xa0Python: A programming language that allows working fastly and integrate systenms in a optimized way.\\n\\n\\n\\xa0\\xa0\\xa0Pandas: A tool fast, powerful, flexible and easy to use for analyses and data wrangling - open source and built on top of Python.\\n\\n\\n\\xa0\\xa0\\xa0NumPy: A Python library that offers a multidimensional matrix object, many derived objects and a variety of routines for matrices operations.\\n\\n\\n\\xa0\\xa0\\xa0Google Maps Platform: A Cloud Computing platform by Google that offers mapping services, including geolocation APIs.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n', metadata={'source': 'https://naomilago.com/posts/getting-geographic-coordinates-with-python-and-google/', 'title': 'Naomi Lago - Getting geographic coordinates with Python and Google', 'description': ' Have you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago - Perceptron: Fundamentals and Applications\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nPerceptron: Fundamentals and Applications\\n\\n\\nDive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.\\n        \\n\\n\\nDEEP LEARNING\\nDATA SCIENCE\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 7, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0The Artificial Neural Networks (ANN), inspired by the human brain operation, are inovative computational models and they perform fundamental roles in a variety of field in Computer Science and Artificial Intelligence (AI). They consists of multiple processing units, known as artificial neurons, interconnected to make information processing - and, with the constant advances and the accessibility to Big Data, these networks has been extremely effective when handling complex tasks in Machine Learning.\\n\\n\\n\\xa0\\xa0\\xa0In this post, I’ll talk about a fundamental concept in this field: the Perceptron. We’ll be exploring its characteristics, fundamentals and applications, despite getting to know its importance in AI. Understand the Perceptron is essential to the comprehension when developing ANN, offering valuable insights about its role in complex problem solvings.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\nWeight and Bias\\n\\n\\nPerceptron operation\\n\\n\\nImplementing a classifier\\n\\n\\n\\nSetting up\\n\\n\\nForward\\n\\n\\nUpdate\\n\\n\\nTraining\\n\\n\\nEvaluation\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\n\\xa0\\xa0\\xa0The Perceptron is a concept that was developed by Frank Rosenlatt back in 1958 as a physical analog computer capable of calculating a weighted sum and adapting its weights.\\n\\n\\n\\xa0\\xa0\\xa0The basis is an approximate imitation of the functioning of a live neuron cell. As electrical signals flow into the cell through dentrites towards the nucleous, an electric charge begins to accumulate. When the cell reaches a certain level of charge, it fire - sending an electrical signal down the axon. However, because dentrites are not all the same, the cell is more “sensitive” to signals through certain dentrites than others, so a smaller amount of signal in those pathways is required to trigger the axon.\\n\\n\\n\\n\\n\\nSource: Manning Publisher\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Indeed, the biology that governs these relationships is beyong the scope of this text, but the key concept to be noted here is how the cell weights the received signals when deciding when to fire. The neuron will dynamically adjust these weights in the decision-making process throughout its life, and in this post, we will mimic this process.\\n\\n\\n\\nWeight and Bias\\n\\n\\n\\n\\xa0\\xa0\\xa0Let’s imagine that you are trying to predict the outcome of a badminton game based in various factors such as the wheather, player quality, and coach experience. With a bias and the weights assigned to each factor, a perceptron could be a way to make this decision.\\n\\n\\n\\xa0\\xa0\\xa0We can think of the weight as the importance we assign to each factor in predictiong the outcome. For example, you can give a higher weight to the weather if you believe it is a crucial factor - since, even though badminton is often played indoors, when played outdoors, wind can have a significant impact on the shuttlecock’s trajectory. In this way, the weight represents the relative relevance of each factor in the decision-making process.\\n\\n\\n\\xa0\\xa0\\xa0The bias, on the other hand, is a kind of adjustment or tendency we apply to our prediction, regardless of the specific factors involved. Continuing with the example of the badminton game, we can think of bias as the personal inclination we have when making a prediction - even without considering external factors. For instance, if you are a fan of a particular team, you may have a positive bias towards it, meaning that your prediction tends to be more optimistic than that of the others.\\n\\n\\n\\xa0\\xa0\\xa0These concepts are applied in a perceptron to aid the classification or prediction of outcomes based on different inputs.\\n\\n\\n\\nPerceptron operation\\n\\n\\n\\n\\xa0\\xa0\\xa0The operation of a Perceptron can be divided into 6 stages, ranging from receiving inputs to the output. Below, we can observe the basic formula and understand each of them.\\n\\n\\n\\n\\n\\nSource: Nomidl (adapted)\\n\\n\\n\\n\\n\\nInput:\\xa0Receives a vector of inputs, representing the attributes or characteristics of the problem at hand.\\n\\n\\nWeighting:\\xa0Each input is multiplied by its corresponding weight.\\n\\n\\nSummation:\\xa0The weighted values of the inputs are summed.\\n\\n\\nBias:\\xa0The sum value is added to a bias.\\n\\n\\nActivation:\\xa0The total value obtained passes through a function that defined the threshold for neuron activation. This function can be binary, returning 0 or 1, or continuous, returning values within a range.\\n\\n\\nOutput:\\xa0The output is determined by the activation function. Depening on the task at hand, the output can represent a class or a numeric estimate.\\n\\n\\n\\n\\xa0\\xa0\\xa0During training, the weights are adjusted iteratively based on classification errors or deviations between the output and the desired value. This process of weight adjustment can be performed by algorithms such as the Perceptron or backpropagation.\\n\\n\\n\\nImplementing a classifier\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, using the concepts we’ve discussed earlier, let’s implement a binary classifier in Python. I’ll follow the hypothetical example of a Badminton match, considering temperature and player experience. Te data has been normalized within a range of -4 to 4, taking into account that neural networks can be sensitive to large values, although this is often scaled down to a smaller range, like -1 to 1.\\n\\n\\n\\xa0\\xa0\\xa0Below, for reference, I provide the general formula ready to pass through the activation function.\\n\\n\\\\[b + \\\\sum_{i=0}^{n}x_iw_i\\\\]\\n\\n\\nSetting up\\n\\n\\n\\n\\xa0\\xa0\\xa0Let’s start importing the libraries and reading our data using pandas. Below, you can find the code, a sample of 3 records and a table representing the data.\\n\\n\\n\\nSetting up code\\nimport matplotlib.pyplot as plt\\nfrom loguru import logger\\nimport pandas as pd\\nimport numpy as np\\n\\n%matplotlib inline\\n\\nuri = \\'assets/badminton_data.txt\\'\\ndf = pd.read_csv(uri, sep=\\'\\\\t\\')\\ndf = df.sample(len(df), random_state=20)\\n\\nif len(df) != 0:\\n  print(f\\'Dataset imported successfully with a shape of {df.shape}\\')\\nelse:\\n  logger.error(f\\'Something went wrong when importing the dataset.\\')\\n    \\ndf.sample(3, random_state=10)\\n\\n\\nDataset imported successfully with a shape of (20, 3)\\n\\n\\n\\n\\n\\n\\n\\n\\ntemperature\\nexperience\\nclass\\n\\n\\n\\n\\n12\\n0.83\\n3.94\\n1\\n\\n\\n8\\n-0.10\\n-3.43\\n0\\n\\n\\n14\\n1.14\\n3.91\\n1\\n\\n\\n\\n\\n\\n\\n\\n\\nThe temperature is measured in degrees Fahrenheit, the experience in years, and the class refers to the whether the player on the match or not.\\n\\n\\n\\xa0\\xa0\\xa0Observing the plot and looking at our hypothetical situation, we can see that those with more experience tend to win, for example.\\n\\n\\n\\nClass dispersion code\\nX_train = df[[\"temperature\", \"experience\"]].values\\ny_train = df[\"class\"].values\\n\\nplt.figure(figsize=(4, 3.5))\\n\\nplt.plot(\\n    X_train[y_train == 0, 0],\\n    X_train[y_train == 0, 1],\\n    marker=\"D\",\\n    markersize=6,\\n    linestyle=\"\",\\n    label=\"Class 0\",\\n)\\n\\nplt.plot(\\n    X_train[y_train == 1, 0],\\n    X_train[y_train == 1, 1],\\n    marker=\"^\",\\n    markersize=6,\\n    linestyle=\"\",\\n    label=\"Class 1\",\\n)\\n\\nplt.legend(loc=2)\\n\\nplt.xlim([-5, 5])\\nplt.ylim([-5, 5])\\n\\nplt.xlabel(\"\\\\nFeature $x_1$\\\\n\", fontsize=12)\\nplt.ylabel(\"\\\\nFeature $x_2$\\\\n\", fontsize=12)\\n\\nplt.suptitle(\\'Classes dispersion\\')\\n\\nplt.grid()\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that we have our data in hand, let’s initialize a class that will represent our model. First, I will define the constructor and its attributes.\\n\\n\\nclass Perceptron:\\n  def __init__(self, n_attributes):\\n    self.n_attributes = n_attributes\\n    self.weights = [0.0 for _ in range(n_attributes)]\\n    self.bias = 0.0\\n    \\nmodel = Perceptron(n_attributes=2)\\nmodel\\n\\n<__main__.Perceptron at 0x7f5b5ff7cd30>\\n\\n\\n\\n\\nForward\\n\\n\\n\\n\\xa0\\xa0\\xa0This method is responsible for calculating the weighted sum, adding the bias and applying the activation function. Let’s see how this can be implemented.\\n\\n\\nclass Perceptron:\\n  def __init__(self, n_attributes):\\n    self.n_attributes = n_attributes\\n    self.weights = [0.0 for _ in range(n_attributes)]\\n    self.bias = 0.0\\n    \\n  def forward(self, x):\\n    weighted_sum = self.bias\\n    for i, _ in enumerate(self.weights):\\n      weighted_sum += x[i] * self.weights[i]\\n      \\n    if weighted_sum > 0.0:\\n      prediction = 1\\n    else:\\n      prediction = 0\\n      \\n    return prediction\\n      \\nmodel = Perceptron(n_attributes=2)\\n\\nx = [0.77, -1.14]\\nprint(\\'Expectation: 0\\')\\nprint(f\\'Prediction: {model.forward(x)}\\')\\n\\nExpectation: 0\\nPrediction: 0\\n\\n\\n\\n\\nUpdate\\n\\n\\n\\n\\xa0\\xa0\\xa0Anoter important method for a Perceptron is the update method. The objective here is to adjust the synaptic weights of the model to improve its ability to make correct predictions. The update is performed based on the comparison between the output and the expected value, and one of the most commonly used rules is the Widrow-Hoff rule, known as the Delta rule. To demonstrate its application, I will use a simplified form:\\n\\n\\nclass Perceptron:\\n  def __init__(self, n_attributes):\\n    self.n_attributes = n_attributes\\n    self.weights = [0.0 for _ in range(n_attributes)]\\n    self.bias = 0.0\\n    \\n  def forward(self, x):\\n    weighted_sum = self.bias\\n    for i, _ in enumerate(self.weights):\\n      weighted_sum += x[i] * self.weights[i]\\n      \\n    if weighted_sum > 0.0:\\n      prediction = 1\\n    else:\\n      prediction = 0\\n      \\n    return prediction\\n  \\n  def update(self, x, y_true):\\n    prediction = self.forward(x)\\n    error = y_true - prediction\\n    \\n    self.bias += error\\n    for i, _ in enumerate (self.weights):\\n      self.weights[i] += error * x[i]\\n      \\n    return error\\n      \\nmodel = Perceptron(n_attributes=2)\\n\\nx = [0.77, -1.14]\\nprint(\\'Expectation: 0\\')\\nprint(f\\'Prediction: {model.forward(x)}\\')\\nprint(\\'\\\\n\\')\\n\\nprint(f\\'Update with WRONG prediction: {model.update(x, y_true=1)}\\')\\nprint(f\\'Weights: {model.weights}\\')\\nprint(f\\'Bias: {model.bias}\\')\\nprint(\\'-\\' * 35, \\'\\\\n\\')\\n\\nprint(f\\'Update with CORRECT prediction: {model.update(x, y_true=0)}\\')\\nprint(f\\'Weights: {model.weights}\\')\\nprint(f\\'Bias: {model.bias}\\')\\nprint(\\'-\\' * 35, \\'\\\\n\\')\\n\\nExpectation: 0\\nPrediction: 0\\n\\n\\nUpdate with WRONG prediction: 1\\nWeights: [0.77, -1.14]\\nBias: 1.0\\n----------------------------------- \\n\\nUpdate with CORRECT prediction: -1\\nWeights: [0.0, 0.0]\\nBias: 0.0\\n----------------------------------- \\n\\n\\n\\n\\n\\nTraining\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that our model is built, let’s move towards finalization by starting the training - which will iterate through the defined number of epochs, making predictions, and updating the parameters to minimize errors.\\n\\n\\ndef training(model, x_values, y_values, epochs):\\n    print(f\\'STARTING THE TRAINING WITH {epochs} EPOCHS\\')\\n\\n    for epoch in range(epochs):\\n        error_count = 0\\n\\n        for x, y in zip(x_values, y_values):\\n            error = model.update(x, y)\\n            error_count += abs(error)\\n\\n        print(f\\'Epoch: {epoch + 1} | Errors: {error_count}\\')\\n        print(\\'-\\'*25)\\n\\n        if error_count == 0:\\n            break\\n\\ntraining(\\n    model=model,\\n    x_values=X_train,\\n    y_values=y_train,\\n    epochs=5\\n)\\n\\nSTARTING THE TRAINING WITH 5 EPOCHS\\nEpoch: 1 | Errors: 3\\n-------------------------\\nEpoch: 2 | Errors: 2\\n-------------------------\\nEpoch: 3 | Errors: 1\\n-------------------------\\nEpoch: 4 | Errors: 1\\n-------------------------\\nEpoch: 5 | Errors: 0\\n-------------------------\\n\\n\\n\\n\\nEvaluation\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally, let’s evaluate our model using accuracy - a simple and intuitive measure that indicates the success rate of a model in relation to the expected results. In an ideal and hypothetical world like this problem, we achieved an accuracy of 100%, but it’s important to remember that in real-world data, this value can often fall within a range of 80-90%.\\n\\n\\ndef compute_accuracy(model, x_values, y_values):\\n  correct = 0.0\\n  \\n  for x, y in zip(x_values, y_values):\\n    prediction = model.forward(x)\\n    correct += int(prediction == y)\\n    \\n  return correct / len(y_values)\\n\\ntraining_accuracy = compute_accuracy(model, X_train, y_train)\\nprint(f\\'The training accuracy was {training_accuracy  *100}% ⭐\\')\\n\\nThe training accuracy was 100.0% ⭐\\n\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0In tis article, we explored the fundamental concept of the Perceptron and its significance in the field of AI. We discussed how it works, covering the steps of weighting, summation, bias, activation, and output. Additionally, we examined the concepts of weight and bias, which play a crucial role in its decision-making process.\\n\\n\\n\\xa0\\xa0\\xa0Next, we provided a practical implementation of a binary classifier using a Perceptron and Python. We used a hypothetical example of prediction the outcomes of a Badminton match, considering the attributes of temperature and player experience. We demonstrated the process of initialization, forward pass, update, training, and model evaluation.\\n\\n\\n\\xa0\\xa0\\xa0In conclusion, I state that the Perceptron is a powerful and versatile computational model that can be applied in various fields. Understanding it is essential for the development and application of neural networks, providing valuable insights into machine learning and its ability to solve complex problems.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago - Information Retrieval with Vector Search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nInformation Retrieval with Vector Search\\n\\n\\nLet’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.\\n        \\n\\n\\nVECTOR SEARCH\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 8, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Natural Language Processing (NLP) plays a crucial role in information retrieval systems, enabling users to find relevant documents based on their queries. In this text, we will explore how NLP techniques can enhance the accuracy and efficiency of these systems, with a specific focus on the use of Vector Search. We will discuss key concepts, challenges, and advancements - as well as the role of technologies like FAISS, Pinecone, and Weaviate in improving the search experience.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\nThe role of vector search in NLP\\n\\n\\n\\nChallenges in information retrieval\\n\\n\\n\\nAd-hoc retrieval\\n\\n\\nShort query\\n\\n\\n\\nAdvancements in NLP\\n\\n\\n\\nVector Space models\\n\\n\\nNeural Networks and Deep Learning\\n\\n\\nSemantic Analysis and Natural Language Understanding\\n\\n\\n\\nVector Search technologies\\n\\n\\n\\nPinecone\\n\\n\\nFAISS\\n\\n\\nWeaviate\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\n\\xa0\\xa0\\xa0The primary goal of an information retrieval system is to retrieve the most relevant documents that match the user’s query. However, traditional systems often struggle to accurately identify relevant documents, especially when queries are short or lack sufficient context.\\n\\n\\n\\xa0\\xa0\\xa0NLP techniques have emerged as a powerful solution to enhance these aspects, and by leveraging these algorithms, systems can analyze and understand natural language queries—enabling them to retrieve more satisfactory results. This is particularly important in scenarios where users provide short queries, such as in web search engines.\\n\\n\\n\\nThe role of vector search in NLP\\n\\n\\n\\n\\xa0\\xa0\\xa0One of the key advancements in NLP is the adoption of Vector Search. This is because, by mapping text into a vector space, the system can perform efficient similarity searches for identification. This approach leverages semantic relationships between words, enabling a more nuanced retrieval based on context and meaning. Instead of relying solely on exact string matches, it considers the proximity of terms, discourse structure, and other linguistic features to determine relevance.\\n\\n\\n\\nChallenges in information retrieval\\n\\n\\n\\n\\xa0\\xa0\\xa0While NLP techniques have shown promise in enhancing this task, there are several challenges that need to be addressed for optimal performance. Let’s explore some of these challenges in more detail.\\n\\n\\n\\nAd-hoc retrieval\\n\\n\\n\\n\\xa0\\xa0\\xa0One of the classic problems is the Ad-hoc retrieval problem. In this retrieval, users enter natural language queries to describe the information they are seeking. However, traditional systems can return both relevant and non-relevant documents due to the non-discriminatory nature of simple criteria like exact string matching.\\n\\n\\n\\xa0\\xa0\\xa0To overcome this problem, these systems employ techniques such as relevance feedback. Relevance feedback allows users to provide feedback on the initial search results, enabling the system to refine and improve the query formulation, resulting in more relevant documents.\\n\\n\\n\\nShort query\\n\\n\\n\\n\\xa0\\xa0\\xa0Short queries pose another challenge when users often provide queries with only a few words, making it difficult for traditional systems to accurately understand their intent. NLP techniques such as query expansion can help address this challenge by adding additional context and relevant documents to the original query.\\n\\n\\n\\xa0\\xa0\\xa0Expansion techniques leverage semantic relationships between words and documents to broaden the scope of the search. By incorporating synonyms, related terms, and contextual information, the system can retrieve more relevant documents even with short queries. This approach requires sophisticated algorithms for context analysis, understanding, and selecting appropriate expansion terms.\\n\\n\\n\\nAdvencements in NLP\\n\\n\\n\\n\\xa0\\xa0\\xa0 The field of NLP has witnessed significant advancements in recent years, leading to improved information retrieval systems. Let’s explore some of these advancements below:\\n\\n\\n\\nVector Space models\\n\\n\\n\\n\\xa0\\xa0\\xa0ChatGPT These models have revolutionized this field by enabling efficient similarity searches. They represent documents and queries as vectors in a high-dimensional space, where the distance between vectors indicates their semantic similarity. This approach allows these models to perform fast and accurate retrieval based on term proximity and their relationships.\\n\\n\\n\\xa0\\xa0\\xa0A popular algorithm used in vector space models is TF-IDF (Term Frequency-Inverse Document Frequency) weighting, which assigns weights to terms based on their frequency in a document and their inverse frequency across the entire document collection. By incorporating this weighting, systems can assign greater importance to terms that are rare in the collection but frequent in a specific document, thereby improving the relevance of retrieved documents.\\n\\n\\n\\nNeural Networks and Deep Learning\\n\\n\\n\\n\\xa0\\xa0\\xa0Neural networks and deep learning techniques have also made significant contributions to NLP-based information retrieval. Models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) can effectively capture complex patterns and dependencies in textual data.\\n\\n\\n\\xa0\\xa0\\xa0These models can be trained on large volumes of data to learn representations that encode semantic information. By leveraging these representations, systems can perform more accurate matching and classification of documents based on their relevance to a query.\\n\\n\\n\\nSemantic Analysis and Natural Language Understanding\\n\\n\\n\\n\\xa0\\xa0\\xa0Semantic analysis and Natural Language Understanding (NLU) play a vital role in improving the relevance of retrieval systems. These techniques enable the system to interpret the meaning and context of queries, facilitating more precise retrieval.\\n\\n\\n\\xa0\\xa0\\xa0Semantic analysis techniques, such as named entity recognition and entity linking, allow the system to identify and understand mentioned entities. This understanding helps refine the formulation with more relevant returns for the user’s intent.\\n\\n\\n\\nNeural Networks and Deep Learning\\n\\n\\n\\n\\xa0\\xa0\\xa0To implement NLP-based information retrieval systems, several vector search technologies have emerged. Let’s explore three prominent ones in this space.\\n\\n\\n\\nPinecone\\n\\n\\n\\n\\xa0\\xa0\\xa0Pinecone is a Vector Search service that provides a scalable and efficient solution for NLP-based retrievals. It offers a managed service for indexing and searching high-dimensional vectors, making it easy for developers to incorporate these capabilities into their applications.\\n\\n\\n\\xa0\\xa0\\xa0Pinecone leverages advanced algorithms and indexing techniques to enable fast similarity search. By using this service, developers can focus on building their NLP models and applications without worrying about the complexities of managing and scaling the underlying infrastructure.\\n\\n\\n\\nFAISS\\n\\n\\n\\n\\xa0\\xa0\\xa0FAISS (Facebook AI Similarity Search) is an open-source library for efficient similarity search and clustering of dense vectors. It offers a variety of indexing and search algorithms optimized for large-scale vector retrieval. FAISS supports acceleration both by CPU and GPU, making it suitable for various implementation scenarios.\\n\\n\\n\\xa0\\xa0\\xa0With FAISS, developers can build systems that can efficiently handle millions or even billions of vectors. The library offers various indexing strategies, including IVF (Inverted File Structure), PCA (Principal Component Analysis), and HNSW (Hierarchical Navigable Small World) to optimize the balance between memory usage and search speed.\\n\\n\\n\\nWeaviate\\n\\n\\n\\n\\xa0\\xa0\\xa0Weaviate is an open-source vector search engine and knowledge graph. It combines the power of semantic knowledge representation with efficient vector search capabilities.\\n\\n\\n\\xa0\\xa0\\xa0Weaviate allows users to represent entities and relationships in a graph-like structure, enabling complex semantic queries. By leveraging vector search, Weaviate can efficiently retrieve documents that match the user’s query based on their semantic similarity. This combination of knowledge graph and search makes it a powerful tool for building intelligent information retrieval systems.\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0NLP techniques, especially with the adoption of vector search, have revolutionized information retrieval systems. By leveraging semantic relationships between words and documents, these systems can enhance the accuracy and relevance of search results. Technologies like the ones presented have further advanced the field, providing scalable and efficient solutions.\\n\\n\\n\\xa0\\xa0\\xa0As NLP continues to evolve, we can expect more advancements in these systems, and the future looks promising, opening up new possibilities for efficient and intelligent search experiences. Whether you’re building a search engine, recommendation system, or any application that requires retrieving information based on user queries, consider incorporating these techniques and technologies to enhance relevance and efficiency.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago - Image classification with Deep Learning made easy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nImage classification with Deep Learning made easy\\n\\n\\nToday is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.\\n        \\n\\n\\nCOMPUTER VISION\\nDEEP LEARNING\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 9, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0You may have heard about Computer Vision before - that refers to a field of Artificial Intelligence that trains computers to interpret and understand the visual world. It enables computers to derive information from images, videos, and other inputs. Today we’ll dive in the practical manner in a high level by classifying images and using Fast AI, a library built on top of Pytorch.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\nGetting the images\\n\\n\\nData block\\n\\n\\nTraining\\n\\n\\nEvaluation\\n\\n\\nTesting\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\n\\xa0\\xa0\\xa0To classify images, I will not be using vanilla PyTorch or TensorFlow. Instead, I will introduce the amazing library from FastAI, which makes deep learning more accessible. The goal is to accomplish a task to understand the capabilities of computers today and generate enough interest to dive deeper into the subject. For that, we’ll be classifying whether an image is a watermelon or a strawberry.\\n\\n\\nI am currently using an NVIDIA RTX A6000 GPU, but I believe it can also be accomplished with a less powerful GPU. It is recommended to run this task on a GPU rather than a CPU, though.\\n\\n\\n\\xa0\\xa0\\xa0So, without any more further ado, let’s start by downloading some libraries and importing them:\\n\\n\\n\\n%pip install -Uqq loguru fastai duckduckgo_search\\n\\nNote: you may need to restart the kernel to use updated packages.\\n\\n\\n\\nfrom fastdownload import download_url\\nfrom duckduckgo_search import DDGS\\nfrom fastai.vision.all import *\\nfrom fastcore.all import *\\nfrom loguru import logger\\nfrom time import sleep\\nimport fastcore\\nimport warnings\\nimport socket\\n\\nwarnings.filterwarnings(\"ignore\")\\n\\ntry:\\n  socket.setdefaulttimeout(1)\\n  socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((\\'1.1.1.1\\', 53))\\n  logger.success(\\'Socket configured ✔\\')\\nexcept socket.error as ex:\\n  raise Exception(\\'No internet connection...\\')\\n\\n2023-09-09 15:28:41.891 | SUCCESS  | __main__:<module>:16 - Socket configured ✔\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Below, you can find the description of each library installed.\\n\\n\\n\\nLoguru:\\xa0A Python logging library that simplifies and enhances the logging process with a clean and intuitive syntax, making it easier to set up and customize logging in your applications.\\n\\n\\nFastai:\\xa0A high-level deep learning library built on top of PyTorch that provides easy-to-use APIs for training and deploying deep neural networks, making it accessible for both beginners and experts in machine learning.\\n\\n\\nDuckDuckGo Search:\\xa0 An internet search engine that emphasizes user privacy by not tracking user activities or storing personal information, offering a privacy-focused alternative to other search engines like Google.\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Another important thing to mention is the try/except part on this setup. It is important because it performs a simple check to determine if the host has internet connectivity before proceeding with further execution of the program. This is vital as I’ll be downloading images later.\\n\\n\\n\\nGetting the images\\n\\n\\n\\n\\xa0\\xa0\\xa0So, in order to train a model, we’ll need some data so it can learn from them, as this is a supervised learning problem. I’ll be creating a function to search for images and returning an L type.\\n\\n\\nAn L type is is a function that converts a regular Python list into a fastai list, offering extra functionality compared to a regular list, such as filtering and mapping\\n\\n\\n\\ndef search_images(keyword: str, max: int = 50) -> fastcore.foundation.L:\\n  print(f\\'{max} images of {keyword} coming...\\')\\n  \\n  return L(DDGS().images(keyword, safesearch=\\'On\\')).itemgot(\\'image\\')[:max]\\n\\n\\n\\n\\xa0\\xa0\\xa0Let’s now try downloading a strawberry and a watermelon photo and check if our function was correctly implemented.\\n\\n\\n\\nurls = search_images(keyword = \\'strawberry\\', max = 3)\\ndestination = \\'./assets/strawberry/strawberry.jpg\\'\\n\\ndownload_url(urls[0], destination, show_progress=False)\\n\\nimg = Image.open(destination)\\nimg.to_thumb(256, 256)\\n\\n3 images of strawberry coming...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nurls = search_images(keyword = \\'watermelon\\', max = 10)\\ndestination = \\'./assets/watermelon/watermelon.jpg\\'\\n\\ndownload_url(urls[6], destination, show_progress=False)\\n\\nimg = Image.open(destination)\\nimg.to_thumb(256, 256)\\n\\n10 images of watermelon coming...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Awesome, now let’s properly download these images in a greater amount and in two diferent folder - these folder will be the class names later on.\\n\\n\\nNote that I used a sleep of 5 second, so I don’t get request troubles.\\n\\n\\n\\nsearches: tuple = \\'strawberry\\', \\'watermelon\\'\\npath = Path(\\'./assets\\')\\n\\nfor search in searches:\\n  destination = (path/search)\\n  destination.mkdir(exist_ok = True, parents = True)\\n  \\n  download_images(destination, urls = search_images(f\\'{search} photos\\'))\\n  sleep(5)\\n  download_images(destination, urls = search_images(f\\'{search} black and white\\'))\\n  sleep(5)\\n  download_images(destination, urls = search_images(f\\'{search} cartoon\\'))\\n  sleep(5)\\n  download_images(destination, urls = search_images(f\\'{search} AI\\'))\\n  sleep(5)\\n  \\n  resize_images(path/search, max_size = 400, dest = path/search)\\n\\n50 images of strawberry photos coming...\\n50 images of strawberry black and white coming...\\n50 images of strawberry cartoon coming...\\n50 images of strawberry AI coming...\\n50 images of watermelon photos coming...\\n50 images of watermelon black and white coming...\\n50 images of watermelon cartoon coming...\\n50 images of watermelon AI coming...\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Let’s also ensure that all our images are valid, so I’ll make a quick check and unlink the broken ones.\\n\\n\\n\\nfailed = verify_images(get_image_files(path))\\nfailed.map(Path.unlink)\\n\\nif len(failed) > 0:\\n  print(f\\'There were {len(failed)} failed images.\\')\\nelse:\\n  print(\\'There were no failed images.\\')\\n\\nThere were 4 failed images.\\n\\n\\n\\n\\nData block\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that we have the images downloaded, let’s use a DataBlock to define the data processing pipeline for creating data loaders - that are important for several key reasons including: efficient data loading, batching, data augmentation, shuffling, paralellism etc.\\n\\n\\n\\ndls = DataBlock(\\n  blocks = (ImageBlock, CategoryBlock),\\n  get_items = get_image_files,\\n  splitter = RandomSplitter(valid_pct = 0.2, seed = 20),\\n  get_y = parent_label,\\n  item_tfms = [Resize(192, method = \\'squish\\')]\\n).dataloaders(path, bs = 32)\\n\\ndls.show_batch(max_n = 6)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTraining\\n\\n\\n\\n\\xa0\\xa0\\xa0Alright, so far we have downloaded our images and created our dataloader. Now it’s time to train our model and, for that, we’ll see how simplified it is to use Fast AI.\\n\\n\\n\\nlearn = vision_learner(dls, resnet18, metrics = [error_rate, accuracy])\\nlearn.fine_tune(15)\\n\\n\\n\\n\\n\\n\\n\\n\\nepoch\\ntrain_loss\\nvalid_loss\\nerror_rate\\naccuracy\\ntime\\n\\n\\n\\n\\n0\\n0.575720\\n0.110310\\n0.034483\\n0.965517\\n00:04\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nepoch\\ntrain_loss\\nvalid_loss\\nerror_rate\\naccuracy\\ntime\\n\\n\\n\\n\\n0\\n0.115316\\n0.086936\\n0.025862\\n0.974138\\n00:03\\n\\n\\n1\\n0.065104\\n0.045375\\n0.021552\\n0.978448\\n00:02\\n\\n\\n2\\n0.055632\\n0.150614\\n0.030172\\n0.969828\\n00:03\\n\\n\\n3\\n0.071165\\n0.094440\\n0.030172\\n0.969828\\n00:03\\n\\n\\n4\\n0.070642\\n0.131956\\n0.017241\\n0.982759\\n00:02\\n\\n\\n5\\n0.052290\\n0.057936\\n0.017241\\n0.982759\\n00:03\\n\\n\\n6\\n0.046295\\n0.045453\\n0.017241\\n0.982759\\n00:03\\n\\n\\n7\\n0.045184\\n0.042271\\n0.017241\\n0.982759\\n00:02\\n\\n\\n8\\n0.043151\\n0.054159\\n0.034483\\n0.965517\\n00:02\\n\\n\\n9\\n0.035403\\n0.045328\\n0.021552\\n0.978448\\n00:02\\n\\n\\n10\\n0.029595\\n0.028453\\n0.017241\\n0.982759\\n00:03\\n\\n\\n11\\n0.022235\\n0.046433\\n0.017241\\n0.982759\\n00:02\\n\\n\\n12\\n0.017884\\n0.039667\\n0.017241\\n0.982759\\n00:03\\n\\n\\n13\\n0.017663\\n0.032925\\n0.017241\\n0.982759\\n00:03\\n\\n\\n14\\n0.020569\\n0.036629\\n0.017241\\n0.982759\\n00:02\\n\\n\\n\\n\\n\\n\\n\\n\\nEvaluation\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally we have our model trained to recognize images and say whether it’s a strawberry or watermelon picture. Let’s go through some visualizations on its performance by viewing the confusion matrix, the classification report and a loss plot.\\n\\n\\n\\ninterp = ClassificationInterpretation.from_learner(learn)\\nlosses,idxs = interp.top_losses()\\n\\nassert  len(dls.valid_ds)==len(losses)==len(idxs)\\n\\ninterp.plot_confusion_matrix(figsize=(4, 4.3), cmap=\\'Reds\\')\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ninterp.print_classification_report()\\n\\n\\n\\n\\n\\n\\n              precision    recall  f1-score   support\\n\\n  strawberry       1.00      0.97      0.98       117\\n  watermelon       0.97      1.00      0.98       115\\n\\n    accuracy                           0.98       232\\n   macro avg       0.98      0.98      0.98       232\\nweighted avg       0.98      0.98      0.98       232\\n\\n\\n\\n\\n\\ninterp = ClassificationInterpretation.from_learner(learn)\\nlosses, idxs = interp.top_losses()\\n\\nassert len(dls.valid_ds) == len(losses) == len(idxs)\\n\\nplt.figure(figsize=(4.5, 4.3))\\nplt.plot(losses[:20], c=\\'#9a031e\\')\\nplt.title(\\'Losses throughout the steps\\')\\nplt.xlabel(\\'Step\\')\\nplt.ylabel(\\'Loss\\')\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTesting\\n\\n\\n\\n\\xa0\\xa0\\xa0We’re getting to the end, as we’ve already developed a model that can differentiate images from two distinct fruits. Now I want to finish by downloading a test image that the model hasn’t seen before and even get the probabilities.\\n\\n\\n\\xa0\\xa0\\xa0I’ll be choosing an image from Lexica Art and the image is as follow:\\n\\n\\n\\ndownload_url(\\n  \\'https://image.lexica.art/full_jpg/d814231d-332d-4569-84d7-820ff4742e38\\', \\n  \\'./assets/test/test_image.jpg\\', \\n  show_progress=False)\\n\\nImage.open(\\'./assets/test/test_image.jpg\\').to_thumb(256, 256)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ncategory, _, probabilities = learn.predict(\\n  PILImage.create(\\'./assets/test/test_image.jpg\\')\\n  )\\n\\nprint(f\\'PREDICTION REPORT:\\\\n\\')\\nprint(f\\'Category: {category}\\')\\nprint(f\\'Probability: {torch.max(probabilities):.3f}\\')\\n\\n\\n\\n\\n\\n\\nPREDICTION REPORT:\\n\\nCategory: strawberry\\nProbability: 1.000\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n', metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago - Vector Search with Facebook AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nVector Search with Facebook AI\\n\\n\\nIn today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.\\n        \\n\\n\\nNLP\\nVECTOR SEARCH\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 10, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0In the last article, I talked about information retrieval with Vector Search, and one of the technologies I mentioned was the FAISS library from Facebook. It‚Äôs an open-source library for efficient similarity search and clustering of dense vectors.\\n\\n\\n\\xa0\\xa0\\xa0 In the last article, I talked about information retrieval with Vector Search, and one of the technologies I mentioned was the FAISS library from Facebook. It‚Äôs an open-source library for efficient similarity search and clustering of dense vectors. Today, let‚Äôs explore an implementation of this masterpiece on a set of texts - exemplifying, but not limiting, the use of this tool that can bring excellent results.\\n\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\nImplementation\\n\\n\\n\\nSetting up\\n\\n\\nVectorization\\n\\n\\nFAISS configuration\\n\\n\\nSearch\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\xa0\\xa0\\xa0Well, as I said, we will talk about FAISS - which stands for Facebook AI Similarity Search. This is a technology developed by Facebook‚Äôs Artificial Intelligence Research team (FAIR) and was released in March 2017. It is delivered in the form of a Python library and designed to be efficient in searching for and retrieving similar vectors in large datasets, making it very useful in recommendation systems, computer vision, natural language processing (NLP), or anomaly detection.\\n\\n\\n\\xa0\\xa0\\xa0FAISS offers a wide range of indexing methods, including exhaustive search, k-means, product quantization, and HNSW. These methods enable faster and more accurate searches, even in high-dimensional spaces.\\n\\n\\n\\xa0\\xa0\\xa0Below, we can observe a representation of how this works under the hood. After indexing our vectorized training data, we can perform searches with new vectors that will use cosine distance, inner product, or L2 distance for the search, which then gives us an output in ascending order with the smallest similarities (indicating they are close and related).\\n\\n\\n\\n\\n\\nFAISS workflow\\n\\n\\n\\n\\nImplementation\\n\\n\\n\\n\\xa0\\xa0\\xa0Alright, let‚Äôs now get practical. As I mentioned before, today we‚Äôre going to implement this algorithm on a set of texts - which means we‚Äôll be searching for texts that are similar to each other.\\n\\n\\n\\nSetting up\\n\\n\\nThe codes are available under an open-source license on GitHub.\\n\\n\\n\\xa0\\xa0\\xa0To get started, I recommend creating a virtual environment for your project using tools like pip, conda, poetry, etc. Next, install the necessary libraries, which in this case will be:\\n\\n\\n\\nsentence_transformers:\\xa0Responsible for vectorizing our texts\\n\\n\\nfaiss:\\xa0Responsible for vector search\\n\\n\\n\\n\\xa0\\xa0\\xa0Then, you can import the libraries, load your dataframe, and assign an incremental identification column, which can even be the index.\\n\\n\\n\\n%pip install faiss-gpu -Uqq\\n\\nNote: you may need to restart the kernel to use updated packages.\\n\\n\\n\\nfrom sentence_transformers import SentenceTransformer\\nfrom sentence_transformers import InputExample\\nimport pandas as pd\\nimport numpy as np\\nimport faiss\\nimport torch\\n\\ndf = pd.read_csv(\\'./assets/similarity_search.csv\\')[[\\'text\\', \\'id\\']]\\n\\nif len(df) != 0:\\n  print(f\\'Dataset imported succcesfully with a shape of {df.shape} \\uf8ffüéâ\\')\\n\\nif df.id.min() > 0:\\n  df.id = df.id.apply(lambda x: x - 1)\\nelse:\\n  print(\\'ID Starting with zero!\\')\\n  \\ndf.head(5)\\n\\nDataset imported succcesfully with a shape of (280, 2) \\uf8ffüéâ\\n\\n\\n\\n\\n\\n\\n\\n\\ntext\\nid\\n\\n\\n\\n\\n0\\nThe COVID-19 pandemic has had a significant im...\\n0\\n\\n\\n1\\nArtificial intelligence is transforming variou...\\n1\\n\\n\\n2\\nSocial media platforms play a crucial role in ...\\n2\\n\\n\\n3\\nRenewable energy sources like solar and wind p...\\n3\\n\\n\\n4\\nCryptocurrencies such as Bitcoin have gained w...\\n4\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVectorization\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that we have our data, let‚Äôs instantiate our vectorization model using SentenceTransformer. It takes the model‚Äôs name as an argument, the device being used, or a cache directory.\\n\\n\\n\\xa0\\xa0\\xa0The model‚Äôs name is the only mandatory argument, and you can find it on the Hugging Face model hub if you‚Äôre unsure which one to choose. The device used is important because if you‚Äôre working with large volumes or vector operations, a GPU can be beneficial, and the cache directory is used to store model information to avoid downloading it every time you call it.\\n\\n\\n\\ndevice = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\n\\nprint(f\\'Using the {str.upper(device)} device!\\')\\n\\nmodel = SentenceTransformer(\\n  \\'distilbert-base-nli-stsb-mean-tokens\\',\\n  device = device,\\n  cache_folder = \\'./assets/cache/\\'\\n)\\n\\nUsing the CUDA device!\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0To test our vectorization model, let‚Äôs vectorize our first 5 texts. Note that the vectors, also called embeddings, have already been generated:\\n\\n\\n\\ntexts = df.text.values.tolist()\\ntexts[:3]\\n\\n[\\'The COVID-19 pandemic has had a significant impact on global economies.\\',\\n \\'Artificial intelligence is transforming various industries, including healthcare and finance.\\',\\n \\'Social media platforms play a crucial role in connecting people around the world.\\']\\n\\n\\n\\n\\nembeddings = model.encode(texts)\\nembeddings[:3]\\n\\narray([[ 0.94750345, -1.0846483 , -0.22848324, ..., -0.06836035,\\n        -0.14919858,  0.6607197 ],\\n       [ 0.28361621, -0.14619698,  0.76421624, ..., -0.09583294,\\n        -0.00354804,  0.03140309],\\n       [ 0.0471359 , -0.09198052,  0.03990437, ..., -0.05552321,\\n        -1.0880417 , -0.33173537]], dtype=float32)\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0With that done, we are ready for the FAISS configuration that comes next.\\n\\n\\n\\nFAISS configuration\\n\\n\\n\\n\\xa0\\xa0\\xa0First, let‚Äôs use our ‚Äúid‚Äù column as the index and save this transformation in a variable, as in line 1. Next, we‚Äôll create an index identifier that contains the values from the ‚Äúid‚Äù column, which will serve as a mapping to the FAISS index‚Äôs indices - helping us associate the dataframe‚Äôs identifiers with the vector points.\\n\\n\\n\\xa0\\xa0\\xa0As a next step, let‚Äôs normalize the vectors generated earlier using L2 normalization - preserving the relative direction between the vectors. In line 4, I made a copy to preserve the original array in case it‚Äôs needed later.\\n\\n\\n\\xa0\\xa0\\xa0Finally, let‚Äôs create a FAISS index of type IndexFlatIP, which will be used to perform queries on the points using the inner product. The parameter defines the dimension of the space, and the choice of this index type was simply because it is efficient and common.\\n\\n\\n\\ndf_to_index = df.set_index([\\'id\\'], drop = False)\\nid_index = np.array(df_to_index.id.values).flatten().astype(\\'int\\')\\n\\nnormalized_embeddings = embeddings.copy()\\nfaiss.normalize_L2(normalized_embeddings)\\nindex_flat = faiss.IndexFlatIP(len(embeddings[0]))\\n\\n\\n\\n\\xa0\\xa0\\xa0Next, we‚Äôll create an IndexIDMap object, which will be created from the index_flat and will allow us to associate identifiers with indexed vectors. This association is useful for retrieving specific information from the indexed vectors based on the identifiers.\\n\\n\\n\\xa0\\xa0\\xa0To complete the FAISS configuration, we‚Äôll add the normalized vectors and their corresponding identifiers to the IndexIDMap object, enabling queries and specific information retrieval.\\n\\n\\n\\nindex_content = faiss.IndexIDMap(index_flat)\\nindex_content.add_with_ids(normalized_embeddings, id_index)\\n\\n\\n\\nSearch\\n\\n\\n\\n\\xa0\\xa0\\xa0Indeed, when it comes to vector search, data must be prepared to match the FAISS standard before conducting searches. This also involves formatting and organizing the output for a final delivery. Preprocessing and post-processing steps are essential to ensure the effectiveness and usability of vector search systems.\\n\\n\\n\\xa0\\xa0\\xa0With this, we define a function that will take care of these steps for us. As parameters, I‚Äôve defined query for the search texts/terms and k for the number of results we want.\\n\\n\\n\\xa0\\xa0\\xa0First, we vectorize our text using SentenceTransformer, and then we normalize it using the FAISS normalizer itself, as shown in lines 2 and 3. Now, we search for the top-k results similar to our query term, as demonstrated in line 5.\\n\\n\\n\\xa0\\xa0\\xa0The identifiers and similarities are then extracted from the result stored in top_k, and a message with the search text is printed on the screen, as seen in lines 6 and 7. Finally, in lines 11 and 12, the results are obtained from the dataframe that had the identifiers transformed into indices, and a new column corresponding to the similarities of the points found with the query is added.\\n\\n\\n\\xa0\\xa0\\xa0In conclusion, the results are returned as a new dataframe with redefined indices, discarding the previous ones.\\n\\n\\n\\ndef search(query: str, k: int = 5) -> pd.core.frame.DataFrame:\\n  vector = model.encode([query])\\n  faiss.normalize_L2(vector)\\n  \\n  top_k = index_content.search(vector, k)\\n  ids = top_k[1][0].tolist()\\n  similarities = top_k[0][0].tolist()\\n  \\n  print(f\\'Searching for \"{query}\"...\\')\\n  \\n  results = df_to_index.loc[ids]\\n  results[\\'similarity\\'] = similarities\\n  output = results.reset_index(drop = True)[[\\'id\\', \\'text\\', \\'similarity\\']]\\n  \\n  return output\\n\\n\\n\\n\\xa0\\xa0\\xa0Let‚Äôs see it in practice. Below, I conducted three different searches with different numbers of returns. Let‚Äôs see how it behaved.\\n\\n\\n\\nsearch(\\'What am I thinking? I love cars\\', 3)\\n\\nSearching for \"What am I thinking? I love cars\"...\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\ntext\\nsimilarity\\n\\n\\n\\n\\n0\\n6\\nSelf-driving cars have the potential to enhanc...\\n0.456869\\n\\n\\n1\\n91\\nThe ethical considerations of AI-powered auton...\\n0.395022\\n\\n\\n2\\n228\\nThe use of AI in personalized entertainment re...\\n0.382918\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsearch(\\'Biology is interesting, send me 5 related topics please\\', 5)\\n\\nSearching for \"Biology is interesting, send me 5 related topics please\"...\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\ntext\\nsimilarity\\n\\n\\n\\n\\n0\\n67\\nThe exploration of deep-sea ecosystems reveals...\\n0.528142\\n\\n\\n1\\n160\\nAdvancements in bioinformatics enable faster a...\\n0.474495\\n\\n\\n2\\n38\\nEthical considerations in gene editing and clo...\\n0.464431\\n\\n\\n3\\n64\\nThe ethical implications of gene editing and C...\\n0.449795\\n\\n\\n4\\n132\\nThe potential of bioprinting technology in reg...\\n0.427724\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsearch(\\'Can you suggest me something about Deep Learning?\\', 6)\\n\\nSearching for \"Can you suggest me something about Deep Learning?\"...\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\ntext\\nsimilarity\\n\\n\\n\\n\\n0\\n255\\nThe challenges of interpretability in deep lea...\\n0.564732\\n\\n\\n1\\n197\\nAdvancements in quantum machine learning algor...\\n0.487619\\n\\n\\n2\\n184\\nThe integration of AI in education transforms ...\\n0.471418\\n\\n\\n3\\n113\\nThe potential of quantum sensors opens up new ...\\n0.467105\\n\\n\\n4\\n106\\nThe role of AI in personalized education and a...\\n0.454311\\n\\n\\n5\\n137\\nThe potential of 3D bioprinting in tissue engi...\\n0.447400\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0We have explored the FAISS tool and seen that it offers various indexing methods that enable fast and accurate searches, making it applicable in various domains. We demonstrated practical implementation on a set of texts, but we know that vector search can be applied to any type of data source as long as vectorization is possible.\\n\\n\\xa0\\xa0\\xa0FAISS stands out for its ability to handle high-dimensional spaces, so remember that our implementation was just a test and usage example. Its flexibility and performance make it a promising option, and its use in conjunction with vectorization techniques, such as SentenceTransformer, can bring significant benefits in terms of efficiency and accuracy.\\n\\n\\xa0\\xa0\\xa0Below, I provide the link to the GitHub repository with the project and the data used, as well as to the official website and documentation.\\n\\n\\n\\n\\n\\n\\n\\nReferences\\n\\n\\n\\n\\nGithub repository\\n\\n\\nFAISS official webpage\\n\\n\\nFAISS documentation\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I‚Äôll see you in the next one ‚≠ê\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago - Recognizing Handwriting Digits\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nRecognizing Handwriting Digits\\n\\n\\nToday, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.\\n        \\n\\n\\nDEEP LEARNING\\nDATA SCIENCE\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 11, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0In the ever-evolving realm of machine learning and image processing, one dataset has stood the test of time as a benchmark for countless algorithms and models: the MNIST dataset. Short for the Modified National Institute of Standards and Technology database, MNIST is a treasure trove of handwritten digits, ranging from 0 to 9, encapsulated within a repository of 70,000 grayscale images. In this blog post, we will delve into the intricacies of MNIST, exploring its significance, structure, and applications.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nSetting up\\n\\n\\nOne Hot Encoder (OHE)\\n\\n\\nDataset class\\n\\n\\nData loader object\\n\\n\\nCross-entropy loss\\n\\n\\nNeural network\\n\\n\\nTraining\\n\\n\\nTesting\\n\\n\\nConclusion\\n\\n\\n\\n\\nSetting up\\n\\n\\n\\n\\xa0\\xa0\\xa0Before properly starting, I‚Äôll be importing the necessary libraries and also loading our data.\\n\\n\\n\\n%pip install -Uqq plotly seaborn\\n\\nNote: you may need to restart the kernel to use updated packages.\\n\\n\\n\\n\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom sklearn.metrics import confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport torch.nn.functional as F\\nimport plotly.express as px\\nfrom torch.optim import SGD\\nfrom tqdm import tqdm\\nimport seaborn as sns\\nimport torch.nn as nn\\nimport numpy as np\\nimport torchvision\\nimport torch\\n\\n%matplotlib inline\\n\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\\nprint(f'Using {str.upper(device)} device \\uf8ffüåü')\\n\\nx, y = torch.load('./MNIST/processed/training.pt')\\nprint(x.shape, y.shape)\\n\\nUsing CUDA device \\uf8ffüåü\\ntorch.Size([60000, 28, 28]) torch.Size([60000])\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0We can then visualize a sample:\\n\\n\\n\\nfig = plt.figure(figsize=(4, 3.5))\\n\\ndef show_number(i: int = 0):\\n    plt.imshow(x[i].numpy(), cmap='Blues')\\n    plt.title(f'The number is {y[i].numpy()}')\\n    plt.colorbar()\\n\\n    plt.show()\\n            \\nshow_number(20)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOne Hot Encoder\\n\\n\\n\\n\\xa0\\xa0\\xa0A next step is using One Hot Encoder to transform categorical data into numerical data. In this case, representing the digits 0-9 it‚Äôll allow each label to be represented as a binary vector that is all 0 except the index of the integer (label) itself that‚Äôd be marked as 1.\\n\\n\\n\\n# Performing a test before properly apply to our dataset\\n\\noriginal_y = torch.tensor([2, 4, 3, 0, 1])\\nnew_y = F.one_hot(original_y)\\nnew_y\\n\\ntensor([[0, 0, 1, 0, 0],\\n        [0, 0, 0, 0, 1],\\n        [0, 0, 0, 1, 0],\\n        [1, 0, 0, 0, 0],\\n        [0, 1, 0, 0, 0]])\\n\\n\\n\\n\\n# Applying in the MNIST data\\n\\nnew_y = F.one_hot(y, num_classes=10)\\n\\ny.reshape(-1, 1).shape[-1], new_y.shape[-1] # Expected (1, 10)\\n\\n(1, 10)\\n\\n\\n\\n\\nDataset class\\n\\n\\n\\n\\xa0\\xa0\\xa0This is another important step as a class is used to represent a collection of data that can be used for training a model. It is an abstract class that can be inherited to create a custom dataset that can be used to load and manipulate sets containing data in many forms.\\n\\n\\n\\nclass CTDataset(Dataset):\\n    def __init__(self, filepath):\\n        self.x, self.y = torch.load(filepath)\\n        self.x = self.x / 255.\\n        self.y = F.one_hot(self.y, num_classes=10).to(float)\\n    \\n    def __len__(self): \\n        return self.x.shape[0]\\n    \\n    def __getitem__(self, ix): \\n        return self.x[ix], self.y[ix]\\n\\ntrain_dataset = CTDataset('./MNIST/processed/training.pt')\\ntest_dataset = CTDataset('./MNIST/processed/test.pt')\\n\\n\\n\\nData loader object\\n\\n\\n\\n\\xa0\\xa0\\xa0Here‚Äôs the time when we create the dataloader objects. These are important components of Deep Learning pipelines that help to load and preprocess data for training. They are used to handle large datasets and perform data ugmentation, shuffling, and other preprocessing tasks.\\n\\n\\nThey are importat for some reasons including: Standardisation, Efficiency and Flexibility.\\n\\n\\n\\ntrain_dataloader = DataLoader(train_dataset, batch_size=5)\\n\\nfor x, y in train_dataloader:\\n    x = x.to(device)\\n    y = y.to(device)\\n    print(x.shape)\\n    print(y.shape)\\n    break    \\n    \\nprint(f'\\\\n{len(train_dataloader)}')\\n\\ntorch.Size([5, 28, 28])\\ntorch.Size([5, 10])\\n\\n12000\\n\\n\\n\\n\\nCross-entropy loss\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, Cross-entropy loss is a widely used loss function in classification tasks. It has several advantages that make it popular, including:\\n\\n\\n\\nPenalizes incorrect predictions\\n\\n\\nMeasures model performance\\n\\n\\nWorks with multiple clases\\n\\n\\nPopular and well-understood\\n\\n\\n\\n\\nL = nn.CrossEntropyLoss().to(device)\\nL\\n\\nCrossEntropyLoss()\\n\\n\\n\\n\\nNeural network\\n\\n\\n\\n\\xa0\\xa0\\xa0As we‚Äôre approaching this problem using Deep Learning, a neural network is important to keep it functional - so, here I declare the network for this classifier:\\n\\n\\n\\nclass DigitsClassifier(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.Matrix1 = nn.Linear(28**2,100)\\n        self.Matrix2 = nn.Linear(100,50)\\n        self.Matrix3 = nn.Linear(50,10)\\n        self.R = nn.ReLU()\\n        \\n    def forward(self,x):\\n        x = x.view(-1,28**2)\\n        x = self.R(self.Matrix1(x))\\n        x = self.R(self.Matrix2(x))\\n        x = self.Matrix3(x)\\n        \\n        return x.squeeze()\\n        \\n        # output = x.squeeze()\\n        \\n        # return output.argmax(axis = 1)\\n\\nmodel = DigitsClassifier().to(device)\\nmodel\\n\\nDigitsClassifier(\\n  (Matrix1): Linear(in_features=784, out_features=100, bias=True)\\n  (Matrix2): Linear(in_features=100, out_features=50, bias=True)\\n  (Matrix3): Linear(in_features=50, out_features=10, bias=True)\\n  (R): ReLU()\\n)\\n\\n\\n\\n\\nxs, ys = train_dataset[0:5]\\n\\nmodel(xs.to(device))\\n\\ntensor([[-0.0759, -0.0578, -0.0325,  0.1515, -0.0489, -0.0079, -0.0817,  0.1410,\\n         -0.0657, -0.0913],\\n        [-0.0584, -0.0314, -0.0718,  0.1930, -0.0209, -0.0429, -0.0243,  0.1449,\\n         -0.0411, -0.0707],\\n        [-0.0568, -0.0475, -0.0563,  0.1828, -0.0044, -0.0150, -0.0350,  0.1090,\\n         -0.0087, -0.0581],\\n        [-0.0622, -0.0535, -0.0551,  0.1234, -0.0365, -0.0321, -0.0423,  0.1067,\\n         -0.0618, -0.0971],\\n        [-0.0458, -0.0897, -0.0495,  0.1648,  0.0038, -0.0291, -0.0159,  0.0708,\\n         -0.0066, -0.0851]], device='cuda:0', grad_fn=<SqueezeBackward0>)\\n\\n\\n\\n\\nL(model(xs.to(device)), ys.to(device))\\n\\ntensor(2.3381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward1>)\\n\\n\\n\\n\\nTraining\\n\\n\\n\\n\\xa0\\xa0\\xa0Another essential part of a machine learning process is training. It involves optimizing the parameters of a model to minimize the difference between the predicted output and the true output. In the given code, I‚Äôll be defining a function to do it for us that takes as input a data loader, a model, and the number of epochs.\\n\\n\\nDuring training, the function iterates over the data loader and updates the model parameters using backpropagation and gradient descent. The loss and epoch data are recorded for each iteration, which will be used to visualize the training progress and evaluate the performance.\\n\\n\\n\\ndef train_model(\\n    dataloader: torch.utils.data.dataloader.DataLoader, \\n    model: DigitsClassifier, \\n    n_epochs: int = 30) -> tuple[np.ndarray, np.ndarray]:\\n    \\n    optimizer = SGD(model.parameters(), lr=1e-2)\\n    L = nn.CrossEntropyLoss()\\n\\n    losses = []\\n    epochs = []\\n    \\n    for epoch in range(n_epochs):\\n        print(f'Epoch: {epoch + 1}/{n_epochs} | Loss: {np.mean(losses)}')\\n        N = len(dataloader)\\n                \\n        for i, (x, y) in enumerate(dataloader):\\n            x = x.to(device)\\n            y = y.to(device)\\n            \\n            optimizer.zero_grad() \\n            loss_value = L(model(x), y) \\n            \\n            if loss_value > torch.max(y).item():\\n                torch.save(model.state_dict(), './MNIST/models/mnist_model.pth')\\n            \\n            loss_value.backward() \\n            optimizer.step() \\n\\n            epochs.append(epoch + i / N)\\n            losses.append(loss_value.item())\\n            \\n    return np.array(epochs), np.array(losses)\\n\\nepoch_data, loss_data = train_model(train_dataloader, model)\\n\\n/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\\n  return _methods._mean(a, axis=axis, dtype=dtype,\\n/usr/local/lib/python3.9/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\\n  ret = ret.dtype.type(ret / rcount)\\n\\n\\nEpoch: 1/30 | Loss: nan\\nEpoch: 2/30 | Loss: 0.444299907778694\\nEpoch: 3/30 | Loss: 0.30499113273745887\\nEpoch: 4/30 | Loss: 0.2411346833651296\\nEpoch: 5/30 | Loss: 0.20264087833581762\\nEpoch: 6/30 | Loss: 0.17611513000308468\\nEpoch: 7/30 | Loss: 0.15636798159883528\\nEpoch: 8/30 | Loss: 0.1409401671701855\\nEpoch: 9/30 | Loss: 0.1283711091479482\\nEpoch: 10/30 | Loss: 0.1178500554705612\\nEpoch: 11/30 | Loss: 0.10884905771381158\\nEpoch: 12/30 | Loss: 0.10104153238275904\\nEpoch: 13/30 | Loss: 0.09419568914679677\\nEpoch: 14/30 | Loss: 0.08814844876493146\\nEpoch: 15/30 | Loss: 0.08278729691344265\\nEpoch: 16/30 | Loss: 0.07798079398823612\\nEpoch: 17/30 | Loss: 0.073632377445332\\nEpoch: 18/30 | Loss: 0.06971223861976696\\nEpoch: 19/30 | Loss: 0.06615196071459868\\nEpoch: 20/30 | Loss: 0.06291801628939395\\nEpoch: 21/30 | Loss: 0.05995343980754616\\nEpoch: 22/30 | Loss: 0.057242643725737494\\nEpoch: 23/30 | Loss: 0.0547542955197936\\nEpoch: 24/30 | Loss: 0.052467638791425374\\nEpoch: 25/30 | Loss: 0.050359719916244224\\nEpoch: 26/30 | Loss: 0.04841136845960758\\nEpoch: 27/30 | Loss: 0.04660600281967505\\nEpoch: 28/30 | Loss: 0.04492906776820902\\nEpoch: 29/30 | Loss: 0.04336751992388834\\nEpoch: 30/30 | Loss: 0.04190996822760929\\n\\n\\n\\n\\nfig = plt.figure(figsize=(4, 3.5))\\n\\nplt.plot(epoch_data, loss_data, color='darkblue')\\nplt.xlabel('Epoch Number')\\nplt.ylabel('Cross Entropy')\\nplt.title('Cross Entropy (per batch)')\\n\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nepoch_data_avgd = epoch_data.reshape(20,-1).mean(axis=1)\\nloss_data_avgd = loss_data.reshape(20,-1).mean(axis=1)\\n\\nfig = plt.figure(figsize=(4, 3.5))\\n\\nplt.plot(epoch_data_avgd, loss_data_avgd, 'o--', color='darkblue')\\nplt.xlabel('Epoch Number')\\nplt.ylabel('Cross Entropy')\\nplt.title('Cross Entropy (avgd per epoch)')\\n\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nxs, ys = train_dataset[0: 2000]\\nxs = xs.to(device)\\nys = ys.to(device)\\n\\nyhats = model(xs).argmax(axis = 1)\\nyhats\\n\\ntensor([5, 0, 4,  ..., 5, 2, 0], device='cuda:0')\\n\\n\\n\\n\\nfig, axes = plt.subplots(3, 4, figsize=(12, 9))\\n\\nfor i in range(12):\\n    row = i // 4\\n    col = i % 4\\n    \\n    ax = axes[row, col]\\n    \\n    ax.imshow(xs.to('cpu')[i], cmap='Blues')\\n    ax.set_title(f'Predicted Digit: {yhats[i]}')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTesting\\n\\n\\n\\n\\xa0\\xa0\\xa0Now it‚Äôs time to test our model with unseen data. For this, I‚Äôll be using the test data loader and also plotting the predictions and the confusion matrix.\\n\\n\\n\\nxs, ys = test_dataset[:2000]\\nyhats = model(xs.to(device)).argmax(axis = 1)\\n\\n\\n\\nfig, axes = plt.subplots(3, 4, figsize=(12, 9))\\n\\nfor i in range(12):\\n    row = i // 4\\n    col = i % 4\\n    \\n    ax = axes[row, col]\\n    \\n    ax.imshow(xs.to('cpu')[i], cmap='Blues')\\n    ax.set_title(f'Predicted Digit: {yhats[i]}')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA confusion matrix is an important tool in machine learning that is used to evaluate the performance of a classification model. It is a table that compares the predicted values with the actual values and shows how many predictions are correct and incorrect per class.\\n\\n\\n\\nimages, true = test_dataset[:]\\npredictions = model(images.to(device)).argmax(axis = 1)\\ntrue = torch.argmax(true, dim=1)\\n\\n\\n\\nscm = confusion_matrix(true.tolist(), predictions.tolist())\\nscm_normalized = np.round(scm/np.sum(scm, axis=1).reshape(-1, 1), 2)\\n\\nplt.figure(figsize=(8, 5))\\n\\nsns.heatmap(\\n    scm_normalized, \\n    cmap='Blues', \\n    annot=True, \\n    cbar_kws={\\n        'orientation': 'vertical'\\n    }\\n)\\n\\nplt.xticks(fontsize=12)\\nplt.yticks(fontsize=12)\\n\\nplt.title('Confusion Matrix\\\\n', fontsize=18)\\nplt.xlabel('\\\\nPrediction\\\\n', fontsize=16)\\nplt.ylabel('\\\\nTrue\\\\n', fontsize=16)\\n\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0Today, we‚Äôve explored how to develop a solution for the MNIST dataset, and it‚Äôs important to note that there are also many other ways to approach this task. We could use TensorFlow instead, apply early stopping techniques, delve deeper into the evaluations, and more. However, we‚Äôve learned how to utilize PyTorch to aid in our deep learning development.\\n\\n\\nThe code developed here is available on my Github.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I‚Äôll see you in the next one ‚≠ê\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago - Preprocessing Unstructured Data\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nPreprocessing Unstructured Data\\n\\n\\nReady for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.\\n        \\n\\n\\nDATA SCIENCE\\nNLP\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nMay 1, 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Extracting and normalizing content from a diverse document types - like images, PDFs, PowerPoints, Word, etc., is a crucial step extensively used in RAG systems to enhance their performance by ensuring that the information fed into them is not only relevant but also contextually rich and well structured. By diving into this text, we’ll explore how this preprocessing step can be done using an API to set up a pipeline that handles three types of documents: Images, PDFs, and HTML.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\nFine-Tuning vs RAG\\n\\n\\nUnstructured API\\n\\n\\n\\nPreprocessing a PowerPoint file\\n\\n\\n\\n\\nModularizing\\n\\n\\n\\nDefining the core components\\n\\n\\n\\nImage component\\n\\n\\nPDF component\\n\\n\\nHTML component\\n\\n\\n\\nAgnostic function\\n\\n\\nNext Steps\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\n\\xa0\\xa0\\xa0In the field of information processing and retrieval, the ability to extract, normalize, and structure content from various document types is essential for Retrieved-Augmented Generation (RAG) systems, requiring specialized methods for extraction and normalization of each type. Before we dive into the main goal of this dive, let’s first make a clear difference among LLMs, Fine-Tuning, and RAGs - which are things that people still mix up a bit.\\n\\n\\n\\nFine-Tuning vs RAG\\n\\n\\n\\n\\xa0\\xa0\\xa0Before getting to know more about each particularity between these two concepts, let’s state that Large Language Models (LLMs) are pre-trained models on vast datasets, enabling them to understand language structure, context, and semantics. They are foundation for both Fine-Tuning and RAG, offering a broad understanding of language - hence they can be fine-tuned for specific tasks or combined with RAG to enhance their performance.\\n\\n\\n\\xa0\\xa0\\xa0When talking about Fine-Tuning, we’re talking about adapting an LLM behavior, writing style, or domain-specific knowledge to specific nuances, tones, or terminologies. It is particularly effective for achieving a deep alignment with vocabulary and conventions, making it ideal for specialized applications requiring high expertise. Hence, it is best when seeking to tailor the outputs closely to specific requirements and having extensive specific data.\\n\\n\\n\\n\\n\\nFine-tuning workflow\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Now RAG, in other hand, focus on enhancing LLMs by connecting them to external knowledge sources through retrieval mechanisms - excelling at incorporating dynamic external data, making it suitable for scenarios requiring up-to-date information or where context is crucial. It offers transparency in response generation by breaking down the process into distinct stages, which is beneficial for applications where interpretability is a priority.\\n\\n\\n\\n\\n\\nRAG workflow\\n\\n\\n\\n\\nUnstructured API\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, let’s talk about the solution we’re exploring today: the Unstructured API - a powerful tool that provides advanced document understanding and preprocessing capabilities, enabling RAG systems to better retrieve abd generate information from a wide range of document types across various domains. I’ve chosen this service because it offers a comprehensive set of features that can be easily integrated into existing pipelines, it’s used by leaders in AI such as Weaviate, LangChain, Yurts AI, etc., but most importantly, it has a free API for study purposes.\\n\\n\\n\\n\\nYour browser does not support the video tag.\\n\\n\\nUnstructured\\n\\n\\n\\n\\nFor further guidance on how to install and set up your environment, please refer to their documentation.\\n\\n\\n\\nPreprocessing a PowerPoint file\\n\\n\\n\\n\\xa0\\xa0\\xa0Alright, now let’s get a feel of how we can use the API on a PowerPoint file. Now, using a sample file I made on how to multiply matrices, I’ll import the necessary modules, read the secrets, and partition the file.\\n\\n\\nYou can download the PowerPoint file used here.\\n\\n\\n\\nfrom unstructured.partition.pptx import partition_pptx\\nfrom unstructured_client import UnstructuredClient\\nfrom loguru import logger\\nimport json\\n\\nkeys = json.load(open(\\'keys.json\\'))\\n\\nunstructured_api_key = keys[\\'UNSTRUCTURED_API_KEY\\']\\nunstructured_base_url = keys[\\'UNSTRUCTURED_BASE_URL\\']\\n\\ns = UnstructuredClient(\\n    api_key_auth=unstructured_api_key,\\n    server_url=unstructured_base_url,\\n)\\n\\npath = \\'./how-to-multiply-matrices.pptx\\'\\n\\nres = partition_pptx(file=open(path, \\'rb\\'))\\n\\nres[1:4]\\n\\n[<unstructured.documents.elements.PageBreak at 0x196b15facb0>,\\n <unstructured.documents.elements.Title at 0x196b15f9120>,\\n <unstructured.documents.elements.NarrativeText at 0x196b15f91e0>]\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0We can see that it’s pretty much it! We get a response from the API with a a list of Unstructured elements - which already spoils us that there are things related to page breaks, titles and narrative texts. Before seeing this result more clearly, refer to this table of available types of elements that can be processed:\\n\\n\\n\\n\\n\\nElement types\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally, let’s print the content of an element to see what we got here:\\n\\n\\n\\nprint(json.dumps(res[4].to_dict(), indent=2))\\n\\n{\\n  \"type\": \"NarrativeText\",\\n  \"element_id\": \"d7eb94d4cac7d41d58597655b1da567c\",\\n  \"text\": \"Not every pair of matrices can be multiplied. If you have two matrices, the number of columns in the first matrix MUST equal the number of rows in the second matrix\",\\n  \"metadata\": {\\n    \"category_depth\": 0,\\n    \"page_number\": 2,\\n    \"languages\": [\\n      \"eng\"\\n    ],\\n    \"parent_id\": \"5bc89aa807bab5a1ab4499371c48d959\",\\n    \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\\n  }\\n}\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0By transforming into a Python dictionary, we can see some valuable information:\\n\\n\\n\\ntype: The element type (referred from the table above).\\n\\n\\nelement_id: A unique identifier for the document element.\\n\\n\\ntext: The actual content or text of the element, if any.\\n\\n\\nmetadata: Additional information about the element, divided into:\\n\\n\\n\\npage_number: The page number where the document is located.\\n\\n\\nlanguages: A sequence of language codes indicated the languages used in the element.\\n\\n\\nparent_id: The identifier of the parent element, if any.\\n\\n\\nfiletype: The element’s MIME type, indicating the file format.\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0How about we apply some conditions to gather what we are looking for? Maybe some e-mail, or even an element talking about the impossibility of multiplying some matrices?\\n\\n\\n\\n# Trying to find an e-mail, if any.\\n\\nresult = [elem for elem in res if elem.to_dict()[\\'type\\'] == \\'EmailAddress\\']\\n\\nif len(result) > 0: print(f\\'Found {len(result)} e-mail address elements! \\\\n\\')\\n\\nprint(json.dumps(result[0].to_dict(), indent=2))\\n\\nFound 1 e-mail address elements! \\n\\n{\\n  \"type\": \"EmailAddress\",\\n  \"element_id\": \"53b62c1be18aa256c1a9891d48c3bab1\",\\n  \"text\": \"info@naomilago.com\",\\n  \"metadata\": {\\n    \"languages\": [\\n      \"eng\"\\n    ],\\n    \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\\n  }\\n}\\n\\n\\n\\n\\n# Trying to find something about the impossibility of multiplying matrices.\\n\\nfrom pprint import pprint\\n\\nresult = [elem for elem in res if \\'cannot be multiplied\\' in elem.to_dict()[\\'text\\'].lower()]\\n\\nif len(result) > 0: \\n  print(f\\'\\'\\'Found {len(result)} result on page {\\n          result[0].to_dict()[\"metadata\"][\"page_number\"]\\n        }! \\\\n\\'\\'\\')\\n  \\npprint(result[0].to_dict()[\\'text\\'], width=132)\\n\\nFound 1 result on page 4! \\n\\n(\\'On the other hand, a 5 x 2 matrix cannot be multiplied by another 5 x 2 matrix even though they both have the same dimensions. \\'\\n \\'Here is a visual representation:\\')\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Alright, alright! This was funny but still hard coded as the search for a specific case for matrices multiplication was matched exactly with our query. Maybe in the future we can dive into embeddings, and join knowledges from the vector databases that I talked in some of the previous dives we’ve had, such as:\\n\\n\\n\\nInformation Retrieval with Vector Search\\n\\n\\nVector Search with Facebook AI\\n\\n\\n\\n\\nModularizing\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, let’s make something more fun: modules. With them, we can package code into functional units, making it easier to understand, maintain, and reuse.\\n\\n\\n\\nDefining the core components\\n\\n\\n\\n\\xa0\\xa0\\xa0Before making the main agnostic function, that will be the module that preprocess files whether they are Images, PDFs, or HTML, we need to look into these individual components to check if there’s any particular need, as well as also build a core function for its type.\\n\\n\\n\\nImage component\\n\\n\\n\\n\\xa0\\xa0\\xa0The first I’ll do is an image preprocessor - that uses this image below as a sample, but can be used with any other image files.\\n\\n\\n\\n\\n\\nSample image\\n\\n\\n\\n\\nfrom unstructured_client import shared\\n\\npath = \\'./data-preparation-process.jpg\\'\\n\\nreq = shared.PartitionParameters(\\n  files=shared.Files(\\n    content=open(path, \\'rb\\').read(), \\n    file_name=path\\n  )\\n)\\n\\nres = s.general.partition(req)\\n\\n[_ for _ in res.elements if _[\\'type\\'] == \\'ListItem\\'][:2]\\n\\n[{\\'type\\': \\'ListItem\\',\\n  \\'element_id\\': \\'24ff8a800f313d0d3ce3eb8713d36748\\',\\n  \\'text\\': \\'- Articulating the problem\\',\\n  \\'metadata\\': {\\'filetype\\': \\'image/jpeg\\',\\n   \\'languages\\': [\\'eng\\'],\\n   \\'page_number\\': 1,\\n   \\'parent_id\\': \\'ca8e9b8913f7d28ac3bff568efbc7f3d\\',\\n   \\'filename\\': \\'data-preparation-process.jpg\\'}},\\n {\\'type\\': \\'ListItem\\',\\n  \\'element_id\\': \\'bd5eebbcc1c1948d37d0f883c5c450d5\\',\\n  \\'text\\': \\'- Defining data required\\',\\n  \\'metadata\\': {\\'filetype\\': \\'image/jpeg\\',\\n   \\'languages\\': [\\'eng\\'],\\n   \\'page_number\\': 1,\\n   \\'parent_id\\': \\'ca8e9b8913f7d28ac3bff568efbc7f3d\\',\\n   \\'filename\\': \\'data-preparation-process.jpg\\'}}]\\n\\n\\n\\ndef image_handler(\\n    path: str, \\n    client: UnstructuredClient, \\n    verbose: bool = False\\n  ) -> list[dict]:\\n  \\n  \\'\\'\\'\\n  Handles images and returns a list of elements from the Unstructured API.\\n  \\n  Parameters: \\n    path (str, required): A valid path to an image file.\\n    client (UnstructuredClient, required): An instance of the UnstructuredClient class.\\n    verbose (bool, optional): Whether to log the process and its results.\\n    \\n  Returns:\\n    list[dict]: A list of dictionary elements from the API.\\n  \\'\\'\\'\\n  \\n  if verbose: logger.info(\\'Partitioning the image parameters...\\')\\n  \\n  req = shared.PartitionParameters(\\n    files=shared.Files(\\n      content=open(path, \\'rb\\').read(), \\n      file_name=path\\n    )\\n  )\\n  \\n  if verbose: logger.info(\\'Sending the request to the API...\\')\\n\\n  res = client.general.partition(req)\\n\\n  if verbose: logger.success(\\'The image was successfully partitioned! \\\\n\\')\\n\\n  return list(res.elements)\\n\\n\\nres = image_handler(\\n  path=\\'./badminton.png\\', \\n  client=s, \\n  verbose=True\\n)\\n\\nlogger.info(f\\'The partitioned Image has {len(res):,} elements!\\')\\nres[3:4]\\n\\n2024-05-01 19:10:56.231 | INFO     | __main__:image_handler:19 - Partitioning the image parameters...\\n2024-05-01 19:10:56.232 | INFO     | __main__:image_handler:28 - Sending the request to the API...\\n2024-05-01 19:11:00.598 | SUCCESS  | __main__:image_handler:32 - The image was successfully partitioned! \\n\\n2024-05-01 19:11:00.599 | INFO     | __main__:<module>:7 - The partitioned Image has 16 elements!\\n\\n\\n[{\\'type\\': \\'NarrativeText\\',\\n  \\'element_id\\': \\'a9e65672c3378de02797f52e2b127f79\\',\\n  \\'text\\': \\'WHAT DO WE KNOW ABOUT BADMINTON? It 1s a racket sport played using rackets to hit a shuttlecock across a net\\',\\n  \\'metadata\\': {\\'filetype\\': \\'image/png\\',\\n   \\'languages\\': [\\'eng\\'],\\n   \\'page_number\\': 1,\\n   \\'parent_id\\': \\'1ea170c90190d980e2355ac3f4d4bbb4\\',\\n   \\'filename\\': \\'badminton.png\\'}}]\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0In the above code, I first tried outside of any function just to make sure I was using the right imports - needing to also use the shared module from the client in order to partition the parameters and then pass into the client API. Finally, I turned it into a function component, containing a docstring and loggers for further documentation and debugging.\\n\\n\\n\\nPDF component\\n\\n\\n\\n\\xa0\\xa0\\xa0Now it’s time for the PDF component - one of the most common type of unstructured data used for this purpose. As I’m a real passionate about Deep Learning, I’ll be using the famous open-source Dive into Deep Learning book that can be found here.\\n\\n\\nNOTE: Since the book has +1k pages, it can take a while to process it all with the free API.\\n\\n\\n\\ndef pdf_handler(\\n    path: str, \\n    client: UnstructuredClient, \\n    verbose: bool = False\\n  ) -> list[dict]:\\n  \\n  \\'\\'\\'\\n  Handles PDFs and returns a list of elements from the Unstructured API.\\n  \\n  Parameters: \\n    path (str, required): A valid path to a PDF file.\\n    client (UnstructuredClient, required): An instance of the UnstructuredClient class.\\n    verbose (bool, optional): Whether to log the process and its results.\\n    \\n  Returns:\\n    list[dict]: A list of dictionary elements from the API.\\n  \\'\\'\\'\\n  \\n  if verbose: logger.info(\\'Partitioning the PDF parameters...\\')\\n  \\n  req = shared.PartitionParameters(\\n    files=shared.Files(\\n      content=open(path, \\'rb\\').read(), \\n      file_name=path\\n    )\\n  )\\n  \\n  if verbose: logger.info(\\'Sending the request to the API...\\')\\n\\n  res = client.general.partition(req)\\n\\n  if verbose: logger.success(\\'The PDF was successfully partitioned! \\\\n\\')\\n\\n  return list(res.elements)\\n\\n\\nres = pdf_handler(\\n  path=\\'./dive-into-deep-learning.pdf\\', \\n  client=s, \\n  verbose=True\\n)\\n\\nlogger.info(f\\'The partitioned PDF has {len(res):,} elements!\\')\\n[_ for _ in res if _[\\'type\\'] == \\'ListItem\\'][5:6]\\n\\n2024-05-01 19:11:00.637 | INFO     | __main__:pdf_handler:19 - Partitioning the PDF parameters...\\n2024-05-01 19:11:00.653 | INFO     | __main__:pdf_handler:28 - Sending the request to the API...\\n2024-05-01 19:14:59.602 | SUCCESS  | __main__:pdf_handler:32 - The PDF was successfully partitioned! \\n\\n2024-05-01 19:14:59.609 | INFO     | __main__:<module>:7 - The partitioned PDF has 23,042 elements!\\n\\n\\n[{\\'type\\': \\'ListItem\\',\\n  \\'element_id\\': \\'60de89542339e0337ca6cfbd58127812\\',\\n  \\'text\\': \\'3. Tweak the knobs to make the model perform better as assessed on those examples.\\',\\n  \\'metadata\\': {\\'languages\\': [\\'eng\\'],\\n   \\'page_number\\': 44,\\n   \\'parent_id\\': \\'b605350bc00209520b7cd8f546322663\\',\\n   \\'filename\\': \\'dive-into-deep-learning.pdf\\',\\n   \\'filetype\\': \\'application/pdf\\'}}]\\n\\n\\n\\n\\nHTML component\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally, we can make our HTML component handler. For this, I’ll use a sample from the own Unstructured documentation welcome page.\\n\\n\\n\\nfrom unstructured.partition.html import partition_html\\n\\ndef html_handler(\\n    path: str, \\n    verbose: bool = False\\n  ) -> list[dict]:\\n  \\n  \\'\\'\\'\\n  Handles HTMLs and returns a list of elements from the Unstructured API.\\n  \\n  Parameters: \\n    path (str, required): A valid path to an HTML file.\\n    verbose (bool, optional): Whether to log the process and its results.\\n    \\n  Returns:\\n    list[dict]: A list of dictionary elements from the API.\\n  \\'\\'\\'\\n  \\n  if verbose: logger.info(\\'Partitioning the HTML content...\\')\\n  \\n  res = [_.to_dict() for _ in partition_html(file=open(path, \\'rb\\'))]\\n\\n  if verbose: logger.success(\\'The HTML was successfully partitioned! \\\\n\\')\\n\\n  return res\\n\\n\\nres = html_handler(\\n  path=\\'./unstructured-documentation.html\\', \\n  verbose=True\\n)\\n\\nlogger.info(f\\'The partitioned HTML has {len(res):,} elements!\\')\\n[_ for _ in res if _[\\'type\\'] == \\'ListItem\\'][0]\\n\\n2024-05-01 19:14:59.645 | INFO     | __main__:html_handler:19 - Partitioning the HTML content...\\n2024-05-01 19:14:59.744 | SUCCESS  | __main__:html_handler:23 - The HTML was successfully partitioned! \\n\\n2024-05-01 19:14:59.749 | INFO     | __main__:<module>:6 - The partitioned HTML has 27 elements!\\n\\n\\n{\\'type\\': \\'ListItem\\',\\n \\'element_id\\': \\'6669f7a5f388bd16dbfb213a8649e875\\',\\n \\'text\\': \\'Unstructured-IO/unstructured\\',\\n \\'metadata\\': {\\'category_depth\\': 1,\\n  \\'link_texts\\': [None],\\n  \\'link_urls\\': [\\'https://github.com/Unstructured-IO/unstructured\\'],\\n  \\'page_number\\': 1,\\n  \\'languages\\': [\\'eng\\'],\\n  \\'parent_id\\': \\'e52b003250d3fee283c6679e2ebb8427\\',\\n  \\'filetype\\': \\'text/html\\'}}\\n\\n\\n\\n\\nAgnostic function\\n\\n\\n\\n\\xa0\\xa0\\xa0Last but not least, we can create our main agnostic function that will be able to handle all the types of files we’ve seen above, and will be the one that could be used to preprocess these data.\\n\\n\\nNOTE: These file extensions are just examples, and you can use any other type of file that the API supports. For a complete list of supported file types, please refer to the documentation.\\n\\n\\n\\n\\ndef data_preprocessor(\\n    path: str, type: str,\\n    client: UnstructuredClient, \\n    verbose: bool = True\\n  ) -> list[dict]:\\n  \\n  \\'\\'\\'\\n  Preprocesses unstructured data and returns a list of elements from the Unstructured API.\\n  \\n  Parameters: \\n    path (str, required): A valid path to an HTML file.\\n    type (str, required): The type of the unstructured data.\\n    client (UnstructuredClient, required): An instance of the UnstructuredClient class.\\n    verbose (bool, optional): Whether to log the process and its results.\\n    \\n  Returns:\\n    list[dict]: A list of dictionary elements from the API.\\n  \\'\\'\\'\\n  \\n  if str.lower(type) in {\\'image\\', \\'pdf\\', \\'html\\'}:\\n    if str.lower(type) == \\'image\\':\\n      if verbose: logger.info(f\\'Processing an image...\\')\\n      return image_handler(path=path, client=client, verbose=verbose)\\n    elif str.lower(type) == \\'pdf\\':\\n      if verbose: logger.info(f\\'Processing a PDF...\\')\\n      return pdf_handler(path=path, client=client, verbose=verbose)\\n    elif str.lower(type) == \\'html\\':\\n      if verbose: logger.info(f\\'Processing an HTML...\\')\\n      return html_handler(path=path, verbose=verbose)\\n  else:\\n    if verbose: logger.error(f\\'{str.upper(type)} is not a currently supported file!\\')\\n    raise SystemError\\n\\n\\n\\n\\xa0\\xa0\\xa0That’s pretty amuch it, and now we can test it out on other files - to make sure it works well on a variety of files within our list of possible extensions.\\n\\n\\n\\nimage_res = data_preprocessor(\\n  path=\\'./technical-writer.png\\',\\n  type=\\'image\\',\\n  client=s,\\n)\\n\\npdf_res = data_preprocessor(\\n  path=\\'./snow-white-and-the-seven-dwarfs.pdf\\',\\n  type=\\'pdf\\',\\n  client=s,\\n)\\n\\nhtml_res = data_preprocessor(\\n  path=\\'./dia-do-trabalhador.html\\',\\n  type=\\'html\\',\\n  client=s,\\n)\\n\\n2024-05-01 19:14:59.783 | INFO     | __main__:data_preprocessor:22 - Processing an image...\\n2024-05-01 19:14:59.784 | INFO     | __main__:image_handler:19 - Partitioning the image parameters...\\n2024-05-01 19:14:59.784 | INFO     | __main__:image_handler:28 - Sending the request to the API...\\n2024-05-01 19:15:02.577 | SUCCESS  | __main__:image_handler:32 - The image was successfully partitioned! \\n\\n2024-05-01 19:15:02.578 | INFO     | __main__:data_preprocessor:25 - Processing a PDF...\\n2024-05-01 19:15:02.579 | INFO     | __main__:pdf_handler:19 - Partitioning the PDF parameters...\\n2024-05-01 19:15:02.580 | INFO     | __main__:pdf_handler:28 - Sending the request to the API...\\n2024-05-01 19:15:03.094 | SUCCESS  | __main__:pdf_handler:32 - The PDF was successfully partitioned! \\n\\n2024-05-01 19:15:03.095 | INFO     | __main__:data_preprocessor:28 - Processing an HTML...\\n2024-05-01 19:15:03.095 | INFO     | __main__:html_handler:19 - Partitioning the HTML content...\\n2024-05-01 19:15:03.341 | SUCCESS  | __main__:html_handler:23 - The HTML was successfully partitioned! \\n\\n\\n\\n\\n\\nimage_res[4]\\n\\n{\\'type\\': \\'Image\\',\\n \\'element_id\\': \\'2634c3dd82e6aad631e772d02e4c46ef\\',\\n \\'text\\': \\'How might | prepare for What skills may be necessary? this career? -Build a writing portfolio Strong writing skills Good interviewing skills -Practice writing by doing unpaid or freelance worl Ability to explain math and science in simple language\\',\\n \\'metadata\\': {\\'filetype\\': \\'image/png\\',\\n  \\'languages\\': [\\'eng\\'],\\n  \\'page_number\\': 1,\\n  \\'filename\\': \\'technical-writer.png\\'}}\\n\\n\\n\\n\\npdf_res[3]\\n\\n{\\'type\\': \\'NarrativeText\\',\\n \\'element_id\\': \\'49f8e5d385825f66d315ece00dc9f77a\\',\\n \\'text\\': \\'“Snow White”’s historical credentials are well known: as the Disney studio’s first feature-length film, it marked a significant turning point for Walt Disney himself, for the Disney studio, for the art of anima- tion, and to some extent for American films in gen- eral. Like most celebrated “firsts,” it wasn’t really the first animated feature. But it’s fair to say that no earli- er feature had showcased the full range of animation technique in the way that “Snow White” did, nor so combined it with rich color, an infectious musical score, and an absorbing, carefully developed story. Instead of creating an art-house curio, bidding for attention on its novelty value alone, Walt boldly jumped into the center of the arena, crafting an ani- mated feature that could compete with the major stu- dios’ live-action features on their own terms. The sheer audacity of this concept in 1937 is impressive enough, but Walt didn’t stop with the concept. So fully did “Snow White” realize its goals that it scored a spectacular worldwide success at the box office, forcing the rest of the film industry to pay attention, and forever changing the course of the Disney stu- dio.\\',\\n \\'metadata\\': {\\'languages\\': [\\'eng\\'],\\n  \\'page_number\\': 1,\\n  \\'parent_id\\': \\'d7452354e7eacad5675c5086fe277cc9\\',\\n  \\'filename\\': \\'snow-white-and-the-seven-dwarfs.pdf\\',\\n  \\'filetype\\': \\'application/pdf\\'}}\\n\\n\\n\\n\\nhtml_res[-51]\\n\\n{\\'type\\': \\'NarrativeText\\',\\n \\'element_id\\': \\'dd03218422acdfe04a165e071f121caf\\',\\n \\'text\\': \\'Os cinquenta mil manifestantes escutaram então os discursos de vários líderes social-democratas e liberais, entre os quais August Palm e Hjalmar Branting.[13]\\',\\n \\'metadata\\': {\\'emphasized_text_contents\\': [\\'[\\', \\']\\'],\\n  \\'emphasized_text_tags\\': [\\'span\\', \\'span\\'],\\n  \\'link_texts\\': [\\'August Palm\\', \\'Hjalmar Branting\\', None],\\n  \\'link_urls\\': [\\'/wiki/August_Palm\\',\\n   \\'/wiki/Hjalmar_Branting\\',\\n   \\'#cite_note-forstamaj-13\\'],\\n  \\'page_number\\': 1,\\n  \\'languages\\': [\\'por\\'],\\n  \\'parent_id\\': \\'b4b4563a9eb59295e2032feb8d15ae05\\',\\n  \\'filetype\\': \\'text/html\\'}}\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Yay! We got our agnostic function ready-to-use. I also made sure to include another language (portuguese) in the HTML content to remember that the Unstructured services handles different languages very well, and the content itself is related to the International Workers’ Day that is celebrated today.\\n\\n\\n\\n\\nNext Steps\\n\\n\\n\\n\\xa0\\xa0\\xa0Before we finish today’s dive, I would like to talk about the next steps that could be followed after this preprocessing - such as connecting to a vector database, generating embeddings to feed RAG systems, or even fine-tune some LLMs around this world. For these cases, chunking is also an important thing and using parents’ information within each element is also very useful for making a strong knowledge base. Now, let’s see the next steps that could be undertaken post-processing:\\n\\n\\n\\nConnecting to a Vector Database: After preprocessing, the data is prepared for efficient storage and retrieval. A vector database is crucial for managing these embeddings, facilitating quick and accurate information retrieval.\\n\\n\\nGenerating Embeddings for RAG Systems: The preprocessed data is transformed into embeddings, which are semantically rich representations. These embeddings are vital for understanding the context and relevance of the information within the RAG system.\\n\\n\\nFine-Tuning Large Language Models (LLMs): With the embeddings generated and stored in the vector database, the next step involves fine-tuning LLMs. This process enhances the model’s ability to generate coherent and contextually accurate responses to user queries.\\n\\n\\n\\n\\xa0\\xa0\\xa0Each of these plays a pivotal role in the overall process, contributing to the development of a robust and efficient information retrieval and generation system. By following these steps, we can leverage the power of preprocessing, vector databases, embeddings, and LLMs to create a comprehensive solution for handling and processing large volumes of data.\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0What a dive, huh?! We started by understanding the difference between Fine-Tuning and RAG systems, then talked about this amazing service called Unstructured as a powerful tool for advanced document preprocessing and understanding. With the practical examples, we could see, through demonstrations, how to handle different file types using their API - using a modular approach for efficient content extraction and setting the stage for further enhancements.\\n\\n\\n\\xa0\\xa0\\xa0Looking ahead, we can already outline potential next steps that are essential for enhancing these systems’ performance, enabling them to retrieve and generate contextually rich responses from a vast array of data sources. Personally, I love Information Retrieval, so I would be really glad to bring one or two of these steps in a future dive - so, stay tuned :)\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences\\n\\n      \\xa0\\n    \\n\\n\\n', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnaomilago (Naomi Lago) ¬∑ GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nCopilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nFor\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Education\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy Solution\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n              Sign in\\n            \\n\\n\\n              Sign up\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nnaomilago\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    256\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nnaomilago\\n \\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Naomi Lago\\n        \\n\\n          naomilago\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\nData Scientist @ Nestl√© \\n\\n\\n\\n\\n\\n\\n177\\n          followers\\n        · \\n206\\n          following\\n\\n\\n\\n\\n\\nNestl√©\\n\\n\\nS√£o Paulo, BR\\n\\n\\n\\n  13:11\\n  (UTC -03:00)\\n\\n\\n\\n\\n\\nhttps://naomilago.com\\n\\nX\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n@naomilago\\n\\n\\n\\n\\nAchievementsBetaSend feedbackAchievementsBetaSend feedback\\nHighlights\\n\\n\\n\\n\\n  Pro\\n\\n\\n\\n\\nOrganizations\\n\\n\\n \\n\\n\\n\\n\\n        Block or Report\\n      \\n\\n\\n\\n\\n\\n\\n\\nBlock or report naomilago\\n\\n\\n\\n\\n\\n\\nBlock user\\n\\n            Prevent this user from interacting with your repositories and sending you notifications.\\n          Learn more about blocking users.\\n        \\n\\n              You must be logged in to block users.\\n            \\n\\n\\n\\n        Add an optional note:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you.\\n\\n\\n\\n          Block user\\n        \\n \\n\\nReport abuse\\n\\n        Contact GitHub support about this user‚Äôs behavior.\\n        Learn more about reporting abuse.\\n      \\nReport abuse\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    256\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnaomilago/README.md\\n\\n\\nHey there, welcome to my Github profile \\uf8ffüëã\\n\\nMy name is Naomi Lago and my pronouns are she/her. I study web development since 2018 and I'm always looking for new ways to improve. I'm currently undergraduating in Mathematics and working as a Data Scientist at Nestl√©. I'm also interested in Cybersecurity and Cloud.\\n\\uf8ffüå± I‚Äôm currently studying Python, Deep Learning, Machine Learning, and focusing on Natural Language Processing (NLP).\\n\\uf8ffüì´ You can find me @\\n\\n\\n\\uf8ffüåé Website\\n\\n\\n\\uf8ffüë©\\uf8ffüèº‚Äç\\uf8ffüíª LinkedIn\\n\\n\\n\\uf8ffüí¨ Telegram\\n\\n\\n\\uf8ffüë• Facebook\\n\\n\\n\\uf8ffüì© Email\\n\\n\\n‚ö° Fun facts:\\n\\n\\n\\uf8ffüß† I love Psychology. I love the idea of studying human minds and behaviors, so me as a social psychologist may still be a possible thing.\\n\\n\\n\\uf8ffüéª And I'm also passionate about violins. I studied for one year long when I was 15 and I take it as a hobby.\\n\\n\\n\\n\\n\\n\\n\\n\\n      Popular repositories\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Pet-Paws\\n              \\nPet-Paws \\nPublic\\n\\n\\n\\n            This project is a bridge between those who want to donate or adopt pets. Don't buy, ADOPT!\\n          \\n\\n\\n\\nTypeScript\\n\\n\\n\\n\\n\\n                15\\n              \\n\\n\\n\\n\\n                1\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                BeTheHero\\n              \\nBeTheHero \\nPublic\\n\\n\\n\\n            Projeto desenvolvido paralelo a Semana Omnistack 11.0 pela Rocketeat \\n          \\n\\n\\n\\nJavaScript\\n\\n\\n\\n\\n\\n                6\\n              \\n\\n\\n\\n\\n                1\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                DesafiosLaunchbase\\n              \\nDesafiosLaunchbase \\nPublic\\n\\n\\n\\n\\n\\n\\n\\nJavaScript\\n\\n\\n\\n\\n\\n                4\\n              \\n\\n\\n\\n\\n                2\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                NaomiLago\\n              \\nNaomiLago \\nPublic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                4\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                fluttery\\n              \\nfluttery \\nPublic\\n\\n\\n\\n            This repository aims to store the source code of a quiz app about Flutter.\\n          \\n\\n\\n\\nDart\\n\\n\\n\\n\\n\\n                4\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                sentimetrix\\n              \\nsentimetrix \\nPublic\\n\\n\\n\\n            Sentimetrix is a sentiment analysis tool that provides real-time insights into your audience's sentiment. Our AI-powered algorithms categorize opinions, attitudes, and emotions from various sources‚Ä¶\\n          \\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n                4\\n              \\n\\n\\n\\n\\n                1\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Something went wrong, please refresh the page to try again.\\n          If the problem persists, check the GitHub status page\\n          or contact support.\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can‚Äôt perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnaomilago (naomilago) / Repositories ¬∑ GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nCopilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nFor\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Education\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy Solution\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n              Sign in\\n            \\n\\n\\n              Sign up\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nnaomilago\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    256\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nnaomilago\\n \\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Naomi Lago\\n        \\n\\n          naomilago\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\nData Scientist @ Nestl√© \\n\\n\\n\\n\\n\\n\\n177\\n          followers\\n        · \\n206\\n          following\\n\\n\\n\\n\\n\\nNestl√©\\n\\n\\nS√£o Paulo, BR\\n\\n\\n\\n  13:12\\n  (UTC -03:00)\\n\\n\\n\\n\\n\\nhttps://naomilago.com\\n\\nX\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n@naomilago\\n\\n\\n\\n\\nAchievementsBetaSend feedbackAchievementsBetaSend feedback\\nHighlights\\n\\n\\n\\n\\n  Pro\\n\\n\\n\\n\\nOrganizations\\n\\n\\n \\n\\n\\n\\n\\n        Block or Report\\n      \\n\\n\\n\\n\\n\\n\\n\\nBlock or report naomilago\\n\\n\\n\\n\\n\\n\\nBlock user\\n\\n            Prevent this user from interacting with your repositories and sending you notifications.\\n          Learn more about blocking users.\\n        \\n\\n              You must be logged in to block users.\\n            \\n\\n\\n\\n        Add an optional note:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you.\\n\\n\\n\\n          Block user\\n        \\n \\n\\nReport abuse\\n\\n        Contact GitHub support about this user‚Äôs behavior.\\n        Learn more about reporting abuse.\\n      \\nReport abuse\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    256\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Type\\n\\n                All\\n              \\n\\n\\n\\n\\n\\nSelect type\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n\\nSources\\n\\n\\n\\n\\n\\n\\nForks\\n\\n\\n\\n\\n\\n\\nArchived\\n\\n\\n\\n\\n\\n\\nCan be sponsored\\n\\n\\n\\n\\n\\n\\nMirrors\\n\\n\\n\\n\\n\\n\\nTemplates\\n\\n\\n\\n\\n\\n\\n Language\\n\\n                  All\\n                \\n\\n\\n\\n\\n\\nSelect language\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n\\nHTML\\n\\n\\n\\n\\n\\n\\nJavaScript\\n\\n\\n\\n\\n\\n\\nCSS\\n\\n\\n\\n\\n\\n\\nTypeScript\\n\\n\\n\\n\\n\\n\\nPHP\\n\\n\\n\\n\\n\\n\\nDart\\n\\n\\n\\n\\n\\n\\n Sort\\n\\n                Last updated\\n              \\n\\n \\n\\n\\nSelect order\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLast updated\\n\\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        diver\\nPublic\\n\\n\\n\\n\\n          This repository houses the code for Diver, my AI-powered bot designed to assist users in exploring the world of Data Science - with some technical knowledge and insights from my own blog at your fi‚Ä¶\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n        Updated May 9, 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        fitness-tracker\\nPublic\\n\\n\\n\\n\\n          Processing, analysing and modeling fitness data to classify barbell exercises and count repetitions\\n        \\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Feb 14, 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Falconize\\nPublic\\n\\n\\n\\n\\n          An Applied Data Science Capstone project offered by IBM on Coursera.\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Dec 27, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        NaomiLago\\nPublic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          4\\n        \\n\\n\\n\\n        Updated Oct 6, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Mentoria_Python\\nPublic\\n\\n\\n\\n\\n          This repo aims to be a place where I'll be sharing notebooks for the mentorships on Python programming\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Oct 6, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        detecting-programming-lanugages\\nPublic\\n\\n\\n\\n\\n          This repo aims to be a place where I can practice SpaCy for NER of programming languages\\n        \\n\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Sep 18, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        news_classifier\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for an NLP project that aims to classify news headlines\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Aug 23, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        naomilago.com\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for my personal website\\n        \\n\\n\\n\\n\\nHTML\\n\\n\\n\\n\\n\\n        Updated Aug 19, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        spam_classification\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for a spam_classification model\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Aug 18, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        lr_with_nn\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for a Linear Regression using Gradient Descent\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Aug 13, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        BERTIFIER\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the page of BERTIFIER - a simple page that hosts a notebook on BERTimbau (BERT in Portuguese) project\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n        Updated Aug 5, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        NLP-Text-Generation\\nPublic\\n\\n\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n        Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        micrograd\\nPublic\\n\\n\\n\\n\\n          This repo aims to store code for a refresher of backprob, gradients and mlp concepts\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        derivatives\\nPublic\\n\\n\\n\\n\\n          This repo aims to be a place where I can train/practice derivative fundamentals\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        mnist\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for the MNIST image (of numbers) classification\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        similarity_search\\nPublic\\n\\n\\n\\n\\n          This repository aims to be a place where I can test/explore similarity search, using FAISS and ChromaDB\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jul 9, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        sentimetrix\\nPublic\\n\\n\\n\\n\\n          Sentimetrix is a sentiment analysis tool that provides real-time insights into your audience's sentiment. Our AI-powered algorithms categorize opinions, attitudes, and emotions from various sources‚Ä¶\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n          4\\n        \\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jun 30, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        topify\\nPublic\\n\\n\\n\\n\\n          Topify is an open-source project that aims to simplify and automate topic classification for text-based data. This GitHub repository hosts the codebase for the Topify app, a powerful tool that util‚Ä¶\\n        \\n\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Jun 22, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        zero-shot-classification\\nPublic\\n\\n\\n\\n\\n          This repository is a place where I can practice Zero Shot Classification using Python\\n        \\n\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jun 22, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        translatable\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for a multilingual translation app\\n        \\n\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Jun 19, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        text-classification\\nPublic\\n\\n\\n\\n\\n          This repository aims to be a place to store code of a text-classification app. There's no name defined initially, but later there'll be a name, a description and a screenshot showing how to use.\\n        \\n\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Jun 18, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        pt_lemmatizer\\nPublic\\n\\n\\n\\n\\n          This repo aims to store code for a Portuguese Lemmatizer, a PyPI package.\\n        \\n\\n\\n\\n  python\\n\\n\\n  nlp\\n\\n\\n  lemmatizer\\n\\n\\n  portuguese\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Jun 8, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        chatbot-nao-web\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the code for Nao chatbot working for web.\\n        \\n\\n\\n\\n\\nJavaScript\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Apr 9, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        text-summarizer\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the code of a text summarizer app\\n        \\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Mar 7, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        codex_chatbot\\nPublic\\n\\n\\n\\n\\n          This repository aims to store the code of a chatbot using the Codex from OpenAI\\n        \\n\\n\\n\\n\\nCSS\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Mar 7, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        buscador_de_similaridades\\nPublic\\n\\n\\n\\n\\n          Esta ferramenta de intelig√™ncia artificial foi projetada para ajudar os usu√°rios a identificar e explorar conex√µes entre palavras, frases e conceitos - com a mais recente tecnologia de processament‚Ä¶\\n        \\n\\n\\n\\n\\nHTML\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Mar 5, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        chatbot-nao\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the code of a chatbot called Nao. This is my first chatbot ever and I'm happy to share it open source.\\n        \\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 26, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        pride-research\\nPublic\\n\\n\\n\\n\\n          This notebook aims to store the notebook and the data I used to analyse demographic and social data around LGBTQ+ and black people. \\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 20, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        salary-predictor\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the notebook of a simple linear regression - where predicts the salary based on the years of experience. The dataset was provided by Allena on Kaggle.\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 18, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        mental-health-classifier\\nPublic\\n\\n\\n\\n\\n          This notebook aims to classify sentences on mental health - whether is healthy or not. It is based on the Mental Health Corpus dataset provided by Reihaneh Namdari on Kaggle.\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 17, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious Next\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can‚Äôt perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "  web_paths=[\n",
    "    'https://naomilago.com/',\n",
    "    'https://naomilago.com/about.html'\n",
    "    'https://naomilago.com/posts/so-what-is-data-science/',\n",
    "    'https://naomilago.com/posts/what-is-natural-language-processing/',\n",
    "    'https://naomilago.com/posts/getting-geographic-coordinates-with-python-and-google/',\n",
    "    'https://naomilago.com/posts/perceptron-fundamentals-and-applications/',\n",
    "    'https://naomilago.com/posts/information-retrieval-with-vector-search/',\n",
    "    'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/',\n",
    "    'https://naomilago.com/posts/vector-search-with-facebook-ai/',\n",
    "    'https://naomilago.com/posts/recognizing-handwriting-digits/',\n",
    "    'https://naomilago.com/posts/preprocessing-unstructured-data/',\n",
    "    'https://github.com/naomilago/',\n",
    "    'https://github.com/naomilago?tab=repositories'\n",
    "  ], \n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a href='https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter'>Splitter</a></h3>\n",
    "\n",
    "> - Este divisor de texto é o recomendado para texto genérico. Ele é parametrizado por uma lista de caracteres. Tenta dividir com base neles na ordem até que os pedaços sejam pequenos o suficiente. A lista padrão é `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. Isso tem o efeito de tentar manter todos os parágrafos (e então as sentenças, e então as palavras) juntos o máximo possível, já que esses seriam genericamente os pedaços de texto mais fortemente relacionados semanticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Naomi Lago - Data dives and beyond\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nData dives and beyond\\n\\n\\nWelcome to my personal blog. Here I’ll share my learning notes and some great resources. Join me as we journey through this fascinating realm, uncovering valuable resources together.\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCategoriesAll (9)COMPUTER VISION (1)DATA SCIENCE (5)DEEP LEARNING (3)GEOLOCATION (1)NLP (3)VECTOR SEARCH (2)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPreprocessing Unstructured Data\\n\\n\\n\\n\\n\\n\\nDATA SCIENCE\\n\\n\\nNLP\\n\\n\\n\\nReady for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.\\n\\n\\n\\n\\n\\nMay 1, 2024\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRecognizing Handwriting Digits\\n\\n\\n\\n\\n\\n\\nDEEP LEARNING\\n\\n\\nDATA SCIENCE', metadata={'source': 'https://naomilago.com/', 'title': 'Naomi Lago - Data dives and beyond', 'description': ' Welcome to my personal blog. Here I’ll share my learning notes and some great resources. Join me as we journey through this fascinating realm, uncovering valuable resources together.', 'language': 'en'}),\n",
       " Document(page_content='May 1, 2024\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRecognizing Handwriting Digits\\n\\n\\n\\n\\n\\n\\nDEEP LEARNING\\n\\n\\nDATA SCIENCE\\n\\n\\n\\nToday, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.\\n\\n\\n\\n\\n\\nSep 11, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVector Search with Facebook AI\\n\\n\\n\\n\\n\\n\\nNLP\\n\\n\\nVECTOR SEARCH\\n\\n\\n\\nIn today’s post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.\\n\\n\\n\\n\\n\\nSep 10, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImage classification with Deep Learning made easy\\n\\n\\n\\n\\n\\n\\nCOMPUTER VISION\\n\\n\\nDEEP LEARNING\\n\\n\\n\\nToday is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.\\n\\n\\n\\n\\n\\nSep 9, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInformation Retrieval with Vector Search\\n\\n\\n\\n\\n\\n\\nVECTOR SEARCH\\n\\n\\n\\nLet’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', metadata={'source': 'https://naomilago.com/', 'title': 'Naomi Lago - Data dives and beyond', 'description': ' Welcome to my personal blog. Here I’ll share my learning notes and some great resources. Join me as we journey through this fascinating realm, uncovering valuable resources together.', 'language': 'en'}),\n",
       " Document(page_content='Information Retrieval with Vector Search\\n\\n\\n\\n\\n\\n\\nVECTOR SEARCH\\n\\n\\n\\nLet’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.\\n\\n\\n\\n\\n\\nSep 8, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPerceptron: Fundamentals and Applications\\n\\n\\n\\n\\n\\n\\nDEEP LEARNING\\n\\n\\nDATA SCIENCE\\n\\n\\n\\nDive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.\\n\\n\\n\\n\\n\\nSep 7, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGetting geographic coordinates with Python and Google\\n\\n\\n\\n\\n\\n\\nGEOLOCATION\\n\\n\\nDATA SCIENCE\\n\\n\\n\\nHave you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.\\n\\n\\n\\n\\n\\nSep 6, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Natural Language Processing?\\n\\n\\n\\n\\n\\n\\nNLP\\n\\n\\n\\nIn this post I’ll be presenting an exciting subfield of AI that joins Computational Linguistics and Computer Science.\\n\\n\\n\\n\\n\\nSep 5, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Data Science, after all?', metadata={'source': 'https://naomilago.com/', 'title': 'Naomi Lago - Data dives and beyond', 'description': ' Welcome to my personal blog. Here I’ll share my learning notes and some great resources. Join me as we journey through this fascinating realm, uncovering valuable resources together.', 'language': 'en'}),\n",
       " Document(page_content='Sep 5, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Data Science, after all?\\n\\n\\n\\n\\n\\n\\nDATA SCIENCE\\n\\n\\n\\nI bet you may have heard about Data Sciece. But do you really understand what is it? In this post we’ll be exploring this field and its applications.\\n\\n\\n\\n\\n\\nSep 4, 2023\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\nNo matching items\\n\\n\\n\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/', 'title': 'Naomi Lago - Data dives and beyond', 'description': ' Welcome to my personal blog. Here I’ll share my learning notes and some great resources. Join me as we journey through this fascinating realm, uncovering valuable resources together.', 'language': 'en'}),\n",
       " Document(page_content='Oops, something lost\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOops, looks like the page is lost.\\nThis is not a fault, just an accident that was not intentional.', metadata={'source': 'https://naomilago.com/about.htmlhttps://naomilago.com/posts/so-what-is-data-science/', 'title': 'Oops, something lost', 'description': 'Oops, looks like the page is lost. Start your website on the cheap.', 'language': 'en-us'}),\n",
       " Document(page_content='Naomi Lago - What is Natural Language Processing?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nWhat is Natural Language Processing?\\n\\n\\nIn this post I’ll be presenting an exciting subfield of AI that joins Computational Linguistics and Computer Science.\\n        \\n\\n\\nNLP\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 5, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Have you ever heard about NLP? This acronym refer to Natural Language Processing and this is a well discussed field nowadays. That’s what we’ll be talking today.\\n\\n\\n\\xa0\\xa0\\xa0This is a field of research in Computer Science and Artificial Intelligence joined with Computational Linguistics. It can be abstracted to processing human languages - like English, Portuguese, Germna etc. This process involves translating the natural language to the machine language, with techniques that transform text into number for example.\\n\\n\\n\\xa0\\xa0\\xa0Using metadata, the computer can persist knowledge about our world and help us to optimize amazing things. Let’s see this example using the famous technology GPT-3:', metadata={'source': 'https://naomilago.com/posts/what-is-natural-language-processing/', 'title': 'Naomi Lago - What is Natural Language Processing?', 'description': ' In this post I’ll be presenting an exciting subfield of AI that joins Computational Linguistics and Computer Science.', 'language': 'en'}),\n",
       " Document(page_content='Using metadata, the computer can persist knowledge about our world and help us to optimize amazing things. Let’s see this example using the famous technology GPT-3:\\n\\n\\n\\n\\n\\nAsking for a dinamic code for a specific task\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0GPT-3 can answer questions more complex than the shown above, but we can note the precision in your answers, i.e.\\xa0knowing the user wants a code built with Python, not JavaScript, and writing the answer based on the language syntax the user requested. In addition, this model made sure to explain each part of what it delivered and all of this using an ethical and respectful voice.\\n\\n\\n\\xa0\\xa0\\xa0Well, NLP is not only applied in chatbots as we saw for sure. Now, let’s take a look at this table that show us a variety of applications in this field:\\n\\n\\n\\n\\n\\nNatural Language Processing in Action published by Manning\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Natural Language Processing is an amazing and complex field. There are many cool concepts behind and I’d like to share all that I’ve been seeing and learning.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/what-is-natural-language-processing/', 'title': 'Naomi Lago - What is Natural Language Processing?', 'description': ' In this post I’ll be presenting an exciting subfield of AI that joins Computational Linguistics and Computer Science.', 'language': 'en'}),\n",
       " Document(page_content='Copyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/what-is-natural-language-processing/', 'title': 'Naomi Lago - What is Natural Language Processing?', 'description': ' In this post I’ll be presenting an exciting subfield of AI that joins Computational Linguistics and Computer Science.', 'language': 'en'}),\n",
       " Document(page_content='Naomi Lago - Getting geographic coordinates with Python and Google\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nGetting geographic coordinates with Python and Google\\n\\n\\nHave you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.\\n        \\n\\n\\nGEOLOCATION\\nDATA SCIENCE\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 6, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Recently I faced a project that, in the Exploratory Data Analysis (EDA) step, I decided to plot a choropleth map that would show the population distribution by Brazilian states. In order to do so, I prior get the coordinates and follow by defining the libraries for the plot.\\n\\n\\n\\xa0\\xa0\\xa0Keeping in mind that there are two main processes, in this post I describe how I went through the process of getting the data by using a Google API at Google Maps Platform.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nEnvironment preparation\\n\\n\\nAPI integration\\n\\n\\nCoordinates collection\\n\\n\\nData Export\\n\\n\\nData handling', metadata={'source': 'https://naomilago.com/posts/getting-geographic-coordinates-with-python-and-google/', 'title': 'Naomi Lago - Getting geographic coordinates with Python and Google', 'description': ' Have you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.', 'language': 'en'}),\n",
       " Document(page_content='Table of Contents\\n\\n\\n\\n\\nEnvironment preparation\\n\\n\\nAPI integration\\n\\n\\nCoordinates collection\\n\\n\\nData Export\\n\\n\\nData handling\\n\\n\\nConclusion\\n\\n\\nReferences\\n\\n\\n\\n\\nEnvironment prepatation\\n\\n\\n\\n\\xa0\\xa0\\xa0For this content, I’ll be using a generic dataset containing two columns: UF and MUNICIPIO that refers to state and city respectfully.\\n\\n\\nThe shape is (82391, 2).\\n\\n\\n\\n\\n\\nSample with 5 entries\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0We want our query to the API request to do not contain null values and, for that reason, I’ll be verifying how many are unfilled and delete them in case are a fine amount.\\n\\n\\n\\n\\n\\nHandling null values\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0There were only one value in each column and they were removed. Now, let’s make the API integration.\\n\\n\\n\\nAPI integration\\n\\n\\n\\n\\xa0\\xa0\\xa0Before starting, it’s important to make sure that the library googlemaps is installed - what I’ll be responsible for the integration. Therefore, I can run the following snippet on my terminal:\\n\\n\\n\\n%pip install -Uqq googlemaps\\n\\nNote: you may need to restart the kernel to use updated packages.', metadata={'source': 'https://naomilago.com/posts/getting-geographic-coordinates-with-python-and-google/', 'title': 'Naomi Lago - Getting geographic coordinates with Python and Google', 'description': ' Have you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.', 'language': 'en'}),\n",
       " Document(page_content='%pip install -Uqq googlemaps\\n\\nNote: you may need to restart the kernel to use updated packages.\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0After configuring your cloud environment at Google, grab your API key. I recommend storing it in a file .json instead of inserting directly to your code. Another option is using the environment variables on your machine. For this example, I stored in my path /credentials/api_keys.json with the following format:\\n\\n\\n\\n\\n\\nJSON format for storing the API key\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0By doing that, we can now make our first request. Initially, I’ll be requesting only one static defined request as for ‘São Paulo, SP’ and store the results in the latitude and longitude variables.\\n\\n\\n\\n\\n\\nSample request\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0In te first lines, after importing the googlemaps library, I read the file where the key was stored and authenticate through the client. I also define the query and wait for the respons - storing in a variable called result. In order to get the proper coordinates, I can filter by getting only these attributes.\\n\\n\\n\\xa0\\xa0\\xa0Now that the API was tested and integrated - the core of our task - we can keep going by collecting for all the entries of the dataset.\\n\\n\\n\\nCoordinates collection', metadata={'source': 'https://naomilago.com/posts/getting-geographic-coordinates-with-python-and-google/', 'title': 'Naomi Lago - Getting geographic coordinates with Python and Google', 'description': ' Have you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.', 'language': 'en'}),\n",
       " Document(page_content='Now that the API was tested and integrated - the core of our task - we can keep going by collecting for all the entries of the dataset.\\n\\n\\n\\nCoordinates collection\\n\\n\\n\\n\\xa0\\xa0\\xa0The idea now consists in get through each line in our entries and use city and state as origins for the queries. In order to do that, there was defined two empty lists to store the responses on latitude and longitude and the same previous logic was set in a loop; that’ll be responsible for getting cities and states of each line, create the query, make the request and return the data we need.\\n\\n\\n\\n\\n\\nRunning through every entry and storing the results\\n\\n\\n\\n\\nNote that I added a try/except just in case a request on a specific line doesn’t find the coordinates, fill with a null value and don’t stop the run - that it’ll make the code crash.\\n\\n\\n\\nData export\\n\\n\\n\\n\\xa0\\xa0\\xa0In the end, we’ll have the coordinated stored via the two lists we defined earlier. To prevent this data to be lost and also use them in another place on another time, I’ll be exporting the results in a .json file.\\n\\n\\n\\n\\n\\nSaving the results\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0So, a dictionary as defined containig the two lists. We can now successfully treat our data and have this task done.\\n\\n\\n\\nData handling', metadata={'source': 'https://naomilago.com/posts/getting-geographic-coordinates-with-python-and-google/', 'title': 'Naomi Lago - Getting geographic coordinates with Python and Google', 'description': ' Have you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.', 'language': 'en'}),\n",
       " Document(page_content='Saving the results\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0So, a dictionary as defined containig the two lists. We can now successfully treat our data and have this task done.\\n\\n\\n\\nData handling\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that we have a json file with the coordinates, we can initially import this file as dataframe and concatenate in our main one:\\n\\n\\n\\n\\n\\nJoining the results to the main dataframe\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, let’s verify if there are any null value and remove them just in case. These values are coming when the request didn’t return any coordinates, falling in the exception and filling with None. We saw this here.\\n\\n\\n\\n\\n\\nHandling null values from the API\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0As we can see, we got 192 cases where it didn’t return any result. They were removed as the proportion is still small compared to the dataframe size.\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0In the end, we were able to complete the task and now we have two new columns: latitude and longitude. We can use them to further geographic analyses and ploting te coordinates in maps.\\n\\n\\n\\n\\n\\nFinal dataframe with results\\n\\n\\n\\n\\nReferences\\n\\n\\n\\n\\xa0\\xa0\\xa0Python: A programming language that allows working fastly and integrate systenms in a optimized way.', metadata={'source': 'https://naomilago.com/posts/getting-geographic-coordinates-with-python-and-google/', 'title': 'Naomi Lago - Getting geographic coordinates with Python and Google', 'description': ' Have you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.', 'language': 'en'}),\n",
       " Document(page_content='Final dataframe with results\\n\\n\\n\\n\\nReferences\\n\\n\\n\\n\\xa0\\xa0\\xa0Python: A programming language that allows working fastly and integrate systenms in a optimized way.\\n\\n\\n\\xa0\\xa0\\xa0Pandas: A tool fast, powerful, flexible and easy to use for analyses and data wrangling - open source and built on top of Python.\\n\\n\\n\\xa0\\xa0\\xa0NumPy: A Python library that offers a multidimensional matrix object, many derived objects and a variety of routines for matrices operations.\\n\\n\\n\\xa0\\xa0\\xa0Google Maps Platform: A Cloud Computing platform by Google that offers mapping services, including geolocation APIs.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/getting-geographic-coordinates-with-python-and-google/', 'title': 'Naomi Lago - Getting geographic coordinates with Python and Google', 'description': ' Have you wondered how to get geographic coordinates using Python and Google? In this post, I’ll show you how I solved this case with only Brazilian cities and states.', 'language': 'en'}),\n",
       " Document(page_content='Naomi Lago - Perceptron: Fundamentals and Applications\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nPerceptron: Fundamentals and Applications\\n\\n\\nDive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.\\n        \\n\\n\\nDEEP LEARNING\\nDATA SCIENCE\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 7, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0The Artificial Neural Networks (ANN), inspired by the human brain operation, are inovative computational models and they perform fundamental roles in a variety of field in Computer Science and Artificial Intelligence (AI). They consists of multiple processing units, known as artificial neurons, interconnected to make information processing - and, with the constant advances and the accessibility to Big Data, these networks has been extremely effective when handling complex tasks in Machine Learning.', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='In this post, I’ll talk about a fundamental concept in this field: the Perceptron. We’ll be exploring its characteristics, fundamentals and applications, despite getting to know its importance in AI. Understand the Perceptron is essential to the comprehension when developing ANN, offering valuable insights about its role in complex problem solvings.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\nWeight and Bias\\n\\n\\nPerceptron operation\\n\\n\\nImplementing a classifier\\n\\n\\n\\nSetting up\\n\\n\\nForward\\n\\n\\nUpdate\\n\\n\\nTraining\\n\\n\\nEvaluation\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\n\\xa0\\xa0\\xa0The Perceptron is a concept that was developed by Frank Rosenlatt back in 1958 as a physical analog computer capable of calculating a weighted sum and adapting its weights.\\n\\n\\n\\xa0\\xa0\\xa0The basis is an approximate imitation of the functioning of a live neuron cell. As electrical signals flow into the cell through dentrites towards the nucleous, an electric charge begins to accumulate. When the cell reaches a certain level of charge, it fire - sending an electrical signal down the axon. However, because dentrites are not all the same, the cell is more “sensitive” to signals through certain dentrites than others, so a smaller amount of signal in those pathways is required to trigger the axon.\\n\\n\\n\\n\\n\\nSource: Manning Publisher', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='Source: Manning Publisher\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Indeed, the biology that governs these relationships is beyong the scope of this text, but the key concept to be noted here is how the cell weights the received signals when deciding when to fire. The neuron will dynamically adjust these weights in the decision-making process throughout its life, and in this post, we will mimic this process.\\n\\n\\n\\nWeight and Bias\\n\\n\\n\\n\\xa0\\xa0\\xa0Let’s imagine that you are trying to predict the outcome of a badminton game based in various factors such as the wheather, player quality, and coach experience. With a bias and the weights assigned to each factor, a perceptron could be a way to make this decision.\\n\\n\\n\\xa0\\xa0\\xa0We can think of the weight as the importance we assign to each factor in predictiong the outcome. For example, you can give a higher weight to the weather if you believe it is a crucial factor - since, even though badminton is often played indoors, when played outdoors, wind can have a significant impact on the shuttlecock’s trajectory. In this way, the weight represents the relative relevance of each factor in the decision-making process.', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='The bias, on the other hand, is a kind of adjustment or tendency we apply to our prediction, regardless of the specific factors involved. Continuing with the example of the badminton game, we can think of bias as the personal inclination we have when making a prediction - even without considering external factors. For instance, if you are a fan of a particular team, you may have a positive bias towards it, meaning that your prediction tends to be more optimistic than that of the others.\\n\\n\\n\\xa0\\xa0\\xa0These concepts are applied in a perceptron to aid the classification or prediction of outcomes based on different inputs.\\n\\n\\n\\nPerceptron operation\\n\\n\\n\\n\\xa0\\xa0\\xa0The operation of a Perceptron can be divided into 6 stages, ranging from receiving inputs to the output. Below, we can observe the basic formula and understand each of them.\\n\\n\\n\\n\\n\\nSource: Nomidl (adapted)\\n\\n\\n\\n\\n\\nInput:\\xa0Receives a vector of inputs, representing the attributes or characteristics of the problem at hand.\\n\\n\\nWeighting:\\xa0Each input is multiplied by its corresponding weight.\\n\\n\\nSummation:\\xa0The weighted values of the inputs are summed.\\n\\n\\nBias:\\xa0The sum value is added to a bias.', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='Weighting:\\xa0Each input is multiplied by its corresponding weight.\\n\\n\\nSummation:\\xa0The weighted values of the inputs are summed.\\n\\n\\nBias:\\xa0The sum value is added to a bias.\\n\\n\\nActivation:\\xa0The total value obtained passes through a function that defined the threshold for neuron activation. This function can be binary, returning 0 or 1, or continuous, returning values within a range.\\n\\n\\nOutput:\\xa0The output is determined by the activation function. Depening on the task at hand, the output can represent a class or a numeric estimate.\\n\\n\\n\\n\\xa0\\xa0\\xa0During training, the weights are adjusted iteratively based on classification errors or deviations between the output and the desired value. This process of weight adjustment can be performed by algorithms such as the Perceptron or backpropagation.\\n\\n\\n\\nImplementing a classifier\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, using the concepts we’ve discussed earlier, let’s implement a binary classifier in Python. I’ll follow the hypothetical example of a Badminton match, considering temperature and player experience. Te data has been normalized within a range of -4 to 4, taking into account that neural networks can be sensitive to large values, although this is often scaled down to a smaller range, like -1 to 1.\\n\\n\\n\\xa0\\xa0\\xa0Below, for reference, I provide the general formula ready to pass through the activation function.', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content=\"Below, for reference, I provide the general formula ready to pass through the activation function.\\n\\n\\\\[b + \\\\sum_{i=0}^{n}x_iw_i\\\\]\\n\\n\\nSetting up\\n\\n\\n\\n\\xa0\\xa0\\xa0Let’s start importing the libraries and reading our data using pandas. Below, you can find the code, a sample of 3 records and a table representing the data.\\n\\n\\n\\nSetting up code\\nimport matplotlib.pyplot as plt\\nfrom loguru import logger\\nimport pandas as pd\\nimport numpy as np\\n\\n%matplotlib inline\\n\\nuri = 'assets/badminton_data.txt'\\ndf = pd.read_csv(uri, sep='\\\\t')\\ndf = df.sample(len(df), random_state=20)\\n\\nif len(df) != 0:\\n  print(f'Dataset imported successfully with a shape of {df.shape}')\\nelse:\\n  logger.error(f'Something went wrong when importing the dataset.')\\n    \\ndf.sample(3, random_state=10)\\n\\n\\nDataset imported successfully with a shape of (20, 3)\\n\\n\\n\\n\\n\\n\\n\\n\\ntemperature\\nexperience\\nclass\\n\\n\\n\\n\\n12\\n0.83\\n3.94\\n1\", metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='Dataset imported successfully with a shape of (20, 3)\\n\\n\\n\\n\\n\\n\\n\\n\\ntemperature\\nexperience\\nclass\\n\\n\\n\\n\\n12\\n0.83\\n3.94\\n1\\n\\n\\n8\\n-0.10\\n-3.43\\n0\\n\\n\\n14\\n1.14\\n3.91\\n1\\n\\n\\n\\n\\n\\n\\n\\n\\nThe temperature is measured in degrees Fahrenheit, the experience in years, and the class refers to the whether the player on the match or not.\\n\\n\\n\\xa0\\xa0\\xa0Observing the plot and looking at our hypothetical situation, we can see that those with more experience tend to win, for example.\\n\\n\\n\\nClass dispersion code\\nX_train = df[[\"temperature\", \"experience\"]].values\\ny_train = df[\"class\"].values\\n\\nplt.figure(figsize=(4, 3.5))\\n\\nplt.plot(\\n    X_train[y_train == 0, 0],\\n    X_train[y_train == 0, 1],\\n    marker=\"D\",\\n    markersize=6,\\n    linestyle=\"\",\\n    label=\"Class 0\",\\n)', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='plt.plot(\\n    X_train[y_train == 1, 0],\\n    X_train[y_train == 1, 1],\\n    marker=\"^\",\\n    markersize=6,\\n    linestyle=\"\",\\n    label=\"Class 1\",\\n)\\n\\nplt.legend(loc=2)\\n\\nplt.xlim([-5, 5])\\nplt.ylim([-5, 5])\\n\\nplt.xlabel(\"\\\\nFeature $x_1$\\\\n\", fontsize=12)\\nplt.ylabel(\"\\\\nFeature $x_2$\\\\n\", fontsize=12)\\n\\nplt.suptitle(\\'Classes dispersion\\')\\n\\nplt.grid()\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that we have our data in hand, let’s initialize a class that will represent our model. First, I will define the constructor and its attributes.', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='Now that we have our data in hand, let’s initialize a class that will represent our model. First, I will define the constructor and its attributes.\\n\\n\\nclass Perceptron:\\n  def __init__(self, n_attributes):\\n    self.n_attributes = n_attributes\\n    self.weights = [0.0 for _ in range(n_attributes)]\\n    self.bias = 0.0\\n    \\nmodel = Perceptron(n_attributes=2)\\nmodel\\n\\n<__main__.Perceptron at 0x7f5b5ff7cd30>\\n\\n\\n\\n\\nForward\\n\\n\\n\\n\\xa0\\xa0\\xa0This method is responsible for calculating the weighted sum, adding the bias and applying the activation function. Let’s see how this can be implemented.', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content=\"Forward\\n\\n\\n\\n\\xa0\\xa0\\xa0This method is responsible for calculating the weighted sum, adding the bias and applying the activation function. Let’s see how this can be implemented.\\n\\n\\nclass Perceptron:\\n  def __init__(self, n_attributes):\\n    self.n_attributes = n_attributes\\n    self.weights = [0.0 for _ in range(n_attributes)]\\n    self.bias = 0.0\\n    \\n  def forward(self, x):\\n    weighted_sum = self.bias\\n    for i, _ in enumerate(self.weights):\\n      weighted_sum += x[i] * self.weights[i]\\n      \\n    if weighted_sum > 0.0:\\n      prediction = 1\\n    else:\\n      prediction = 0\\n      \\n    return prediction\\n      \\nmodel = Perceptron(n_attributes=2)\\n\\nx = [0.77, -1.14]\\nprint('Expectation: 0')\\nprint(f'Prediction: {model.forward(x)}')\\n\\nExpectation: 0\\nPrediction: 0\", metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='Expectation: 0\\nPrediction: 0\\n\\n\\n\\n\\nUpdate\\n\\n\\n\\n\\xa0\\xa0\\xa0Anoter important method for a Perceptron is the update method. The objective here is to adjust the synaptic weights of the model to improve its ability to make correct predictions. The update is performed based on the comparison between the output and the expected value, and one of the most commonly used rules is the Widrow-Hoff rule, known as the Delta rule. To demonstrate its application, I will use a simplified form:', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='class Perceptron:\\n  def __init__(self, n_attributes):\\n    self.n_attributes = n_attributes\\n    self.weights = [0.0 for _ in range(n_attributes)]\\n    self.bias = 0.0\\n    \\n  def forward(self, x):\\n    weighted_sum = self.bias\\n    for i, _ in enumerate(self.weights):\\n      weighted_sum += x[i] * self.weights[i]\\n      \\n    if weighted_sum > 0.0:\\n      prediction = 1\\n    else:\\n      prediction = 0\\n      \\n    return prediction\\n  \\n  def update(self, x, y_true):\\n    prediction = self.forward(x)\\n    error = y_true - prediction\\n    \\n    self.bias += error\\n    for i, _ in enumerate (self.weights):\\n      self.weights[i] += error * x[i]\\n      \\n    return error', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='self.weights[i] += error * x[i]\\n      \\n    return error\\n      \\nmodel = Perceptron(n_attributes=2)', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content=\"x = [0.77, -1.14]\\nprint('Expectation: 0')\\nprint(f'Prediction: {model.forward(x)}')\\nprint('\\\\n')\\n\\nprint(f'Update with WRONG prediction: {model.update(x, y_true=1)}')\\nprint(f'Weights: {model.weights}')\\nprint(f'Bias: {model.bias}')\\nprint('-' * 35, '\\\\n')\\n\\nprint(f'Update with CORRECT prediction: {model.update(x, y_true=0)}')\\nprint(f'Weights: {model.weights}')\\nprint(f'Bias: {model.bias}')\\nprint('-' * 35, '\\\\n')\\n\\nExpectation: 0\\nPrediction: 0\\n\\n\\nUpdate with WRONG prediction: 1\\nWeights: [0.77, -1.14]\\nBias: 1.0\\n----------------------------------- \\n\\nUpdate with CORRECT prediction: -1\\nWeights: [0.0, 0.0]\\nBias: 0.0\\n----------------------------------- \\n\\n\\n\\n\\n\\nTraining\", metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content=\"Update with CORRECT prediction: -1\\nWeights: [0.0, 0.0]\\nBias: 0.0\\n----------------------------------- \\n\\n\\n\\n\\n\\nTraining\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that our model is built, let’s move towards finalization by starting the training - which will iterate through the defined number of epochs, making predictions, and updating the parameters to minimize errors.\\n\\n\\ndef training(model, x_values, y_values, epochs):\\n    print(f'STARTING THE TRAINING WITH {epochs} EPOCHS')\\n\\n    for epoch in range(epochs):\\n        error_count = 0\\n\\n        for x, y in zip(x_values, y_values):\\n            error = model.update(x, y)\\n            error_count += abs(error)\\n\\n        print(f'Epoch: {epoch + 1} | Errors: {error_count}')\\n        print('-'*25)\\n\\n        if error_count == 0:\\n            break\", metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='if error_count == 0:\\n            break\\n\\ntraining(\\n    model=model,\\n    x_values=X_train,\\n    y_values=y_train,\\n    epochs=5\\n)\\n\\nSTARTING THE TRAINING WITH 5 EPOCHS\\nEpoch: 1 | Errors: 3\\n-------------------------\\nEpoch: 2 | Errors: 2\\n-------------------------\\nEpoch: 3 | Errors: 1\\n-------------------------\\nEpoch: 4 | Errors: 1\\n-------------------------\\nEpoch: 5 | Errors: 0\\n-------------------------\\n\\n\\n\\n\\nEvaluation\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally, let’s evaluate our model using accuracy - a simple and intuitive measure that indicates the success rate of a model in relation to the expected results. In an ideal and hypothetical world like this problem, we achieved an accuracy of 100%, but it’s important to remember that in real-world data, this value can often fall within a range of 80-90%.', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content=\"def compute_accuracy(model, x_values, y_values):\\n  correct = 0.0\\n  \\n  for x, y in zip(x_values, y_values):\\n    prediction = model.forward(x)\\n    correct += int(prediction == y)\\n    \\n  return correct / len(y_values)\\n\\ntraining_accuracy = compute_accuracy(model, X_train, y_train)\\nprint(f'The training accuracy was {training_accuracy  *100}% ⭐')\\n\\nThe training accuracy was 100.0% ⭐\\n\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0In tis article, we explored the fundamental concept of the Perceptron and its significance in the field of AI. We discussed how it works, covering the steps of weighting, summation, bias, activation, and output. Additionally, we examined the concepts of weight and bias, which play a crucial role in its decision-making process.\\n\\n\\n\\xa0\\xa0\\xa0Next, we provided a practical implementation of a binary classifier using a Perceptron and Python. We used a hypothetical example of prediction the outcomes of a Badminton match, considering the attributes of temperature and player experience. We demonstrated the process of initialization, forward pass, update, training, and model evaluation.\", metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='In conclusion, I state that the Perceptron is a powerful and versatile computational model that can be applied in various fields. Understanding it is essential for the development and application of neural networks, providing valuable insights into machine learning and its ability to solve complex problems.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/perceptron-fundamentals-and-applications/', 'title': 'Naomi Lago - Perceptron: Fundamentals and Applications', 'description': ' Dive with me into the fascinating role of the Perceptron in shaping AI’s cutting-edge applications and remarkable results.', 'language': 'en'}),\n",
       " Document(page_content='Naomi Lago - Information Retrieval with Vector Search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nInformation Retrieval with Vector Search\\n\\n\\nLet’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.\\n        \\n\\n\\nVECTOR SEARCH\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 8, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Natural Language Processing (NLP) plays a crucial role in information retrieval systems, enabling users to find relevant documents based on their queries. In this text, we will explore how NLP techniques can enhance the accuracy and efficiency of these systems, with a specific focus on the use of Vector Search. We will discuss key concepts, challenges, and advancements - as well as the role of technologies like FAISS, Pinecone, and Weaviate in improving the search experience.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\nThe role of vector search in NLP\\n\\n\\n\\nChallenges in information retrieval\\n\\n\\n\\nAd-hoc retrieval\\n\\n\\nShort query\\n\\n\\n\\nAdvancements in NLP', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='Table of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\nThe role of vector search in NLP\\n\\n\\n\\nChallenges in information retrieval\\n\\n\\n\\nAd-hoc retrieval\\n\\n\\nShort query\\n\\n\\n\\nAdvancements in NLP\\n\\n\\n\\nVector Space models\\n\\n\\nNeural Networks and Deep Learning\\n\\n\\nSemantic Analysis and Natural Language Understanding\\n\\n\\n\\nVector Search technologies\\n\\n\\n\\nPinecone\\n\\n\\nFAISS\\n\\n\\nWeaviate\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\n\\xa0\\xa0\\xa0The primary goal of an information retrieval system is to retrieve the most relevant documents that match the user’s query. However, traditional systems often struggle to accurately identify relevant documents, especially when queries are short or lack sufficient context.\\n\\n\\n\\xa0\\xa0\\xa0NLP techniques have emerged as a powerful solution to enhance these aspects, and by leveraging these algorithms, systems can analyze and understand natural language queries—enabling them to retrieve more satisfactory results. This is particularly important in scenarios where users provide short queries, such as in web search engines.\\n\\n\\n\\nThe role of vector search in NLP', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='The role of vector search in NLP\\n\\n\\n\\n\\xa0\\xa0\\xa0One of the key advancements in NLP is the adoption of Vector Search. This is because, by mapping text into a vector space, the system can perform efficient similarity searches for identification. This approach leverages semantic relationships between words, enabling a more nuanced retrieval based on context and meaning. Instead of relying solely on exact string matches, it considers the proximity of terms, discourse structure, and other linguistic features to determine relevance.\\n\\n\\n\\nChallenges in information retrieval\\n\\n\\n\\n\\xa0\\xa0\\xa0While NLP techniques have shown promise in enhancing this task, there are several challenges that need to be addressed for optimal performance. Let’s explore some of these challenges in more detail.\\n\\n\\n\\nAd-hoc retrieval\\n\\n\\n\\n\\xa0\\xa0\\xa0One of the classic problems is the Ad-hoc retrieval problem. In this retrieval, users enter natural language queries to describe the information they are seeking. However, traditional systems can return both relevant and non-relevant documents due to the non-discriminatory nature of simple criteria like exact string matching.\\n\\n\\n\\xa0\\xa0\\xa0To overcome this problem, these systems employ techniques such as relevance feedback. Relevance feedback allows users to provide feedback on the initial search results, enabling the system to refine and improve the query formulation, resulting in more relevant documents.\\n\\n\\n\\nShort query', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='Short query\\n\\n\\n\\n\\xa0\\xa0\\xa0Short queries pose another challenge when users often provide queries with only a few words, making it difficult for traditional systems to accurately understand their intent. NLP techniques such as query expansion can help address this challenge by adding additional context and relevant documents to the original query.\\n\\n\\n\\xa0\\xa0\\xa0Expansion techniques leverage semantic relationships between words and documents to broaden the scope of the search. By incorporating synonyms, related terms, and contextual information, the system can retrieve more relevant documents even with short queries. This approach requires sophisticated algorithms for context analysis, understanding, and selecting appropriate expansion terms.\\n\\n\\n\\nAdvencements in NLP\\n\\n\\n\\n\\xa0\\xa0\\xa0 The field of NLP has witnessed significant advancements in recent years, leading to improved information retrieval systems. Let’s explore some of these advancements below:\\n\\n\\n\\nVector Space models\\n\\n\\n\\n\\xa0\\xa0\\xa0ChatGPT These models have revolutionized this field by enabling efficient similarity searches. They represent documents and queries as vectors in a high-dimensional space, where the distance between vectors indicates their semantic similarity. This approach allows these models to perform fast and accurate retrieval based on term proximity and their relationships.', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='A popular algorithm used in vector space models is TF-IDF (Term Frequency-Inverse Document Frequency) weighting, which assigns weights to terms based on their frequency in a document and their inverse frequency across the entire document collection. By incorporating this weighting, systems can assign greater importance to terms that are rare in the collection but frequent in a specific document, thereby improving the relevance of retrieved documents.\\n\\n\\n\\nNeural Networks and Deep Learning\\n\\n\\n\\n\\xa0\\xa0\\xa0Neural networks and deep learning techniques have also made significant contributions to NLP-based information retrieval. Models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) can effectively capture complex patterns and dependencies in textual data.\\n\\n\\n\\xa0\\xa0\\xa0These models can be trained on large volumes of data to learn representations that encode semantic information. By leveraging these representations, systems can perform more accurate matching and classification of documents based on their relevance to a query.\\n\\n\\n\\nSemantic Analysis and Natural Language Understanding\\n\\n\\n\\n\\xa0\\xa0\\xa0Semantic analysis and Natural Language Understanding (NLU) play a vital role in improving the relevance of retrieval systems. These techniques enable the system to interpret the meaning and context of queries, facilitating more precise retrieval.\\n\\n\\n\\xa0\\xa0\\xa0Semantic analysis techniques, such as named entity recognition and entity linking, allow the system to identify and understand mentioned entities. This understanding helps refine the formulation with more relevant returns for the user’s intent.', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='Semantic analysis techniques, such as named entity recognition and entity linking, allow the system to identify and understand mentioned entities. This understanding helps refine the formulation with more relevant returns for the user’s intent.\\n\\n\\n\\nNeural Networks and Deep Learning\\n\\n\\n\\n\\xa0\\xa0\\xa0To implement NLP-based information retrieval systems, several vector search technologies have emerged. Let’s explore three prominent ones in this space.\\n\\n\\n\\nPinecone\\n\\n\\n\\n\\xa0\\xa0\\xa0Pinecone is a Vector Search service that provides a scalable and efficient solution for NLP-based retrievals. It offers a managed service for indexing and searching high-dimensional vectors, making it easy for developers to incorporate these capabilities into their applications.\\n\\n\\n\\xa0\\xa0\\xa0Pinecone leverages advanced algorithms and indexing techniques to enable fast similarity search. By using this service, developers can focus on building their NLP models and applications without worrying about the complexities of managing and scaling the underlying infrastructure.\\n\\n\\n\\nFAISS\\n\\n\\n\\n\\xa0\\xa0\\xa0FAISS (Facebook AI Similarity Search) is an open-source library for efficient similarity search and clustering of dense vectors. It offers a variety of indexing and search algorithms optimized for large-scale vector retrieval. FAISS supports acceleration both by CPU and GPU, making it suitable for various implementation scenarios.', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='With FAISS, developers can build systems that can efficiently handle millions or even billions of vectors. The library offers various indexing strategies, including IVF (Inverted File Structure), PCA (Principal Component Analysis), and HNSW (Hierarchical Navigable Small World) to optimize the balance between memory usage and search speed.\\n\\n\\n\\nWeaviate\\n\\n\\n\\n\\xa0\\xa0\\xa0Weaviate is an open-source vector search engine and knowledge graph. It combines the power of semantic knowledge representation with efficient vector search capabilities.\\n\\n\\n\\xa0\\xa0\\xa0Weaviate allows users to represent entities and relationships in a graph-like structure, enabling complex semantic queries. By leveraging vector search, Weaviate can efficiently retrieve documents that match the user’s query based on their semantic similarity. This combination of knowledge graph and search makes it a powerful tool for building intelligent information retrieval systems.\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0NLP techniques, especially with the adoption of vector search, have revolutionized information retrieval systems. By leveraging semantic relationships between words and documents, these systems can enhance the accuracy and relevance of search results. Technologies like the ones presented have further advanced the field, providing scalable and efficient solutions.', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='As NLP continues to evolve, we can expect more advancements in these systems, and the future looks promising, opening up new possibilities for efficient and intelligent search experiences. Whether you’re building a search engine, recommendation system, or any application that requires retrieving information based on user queries, consider incorporating these techniques and technologies to enhance relevance and efficiency.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/information-retrieval-with-vector-search/', 'title': 'Naomi Lago - Information Retrieval with Vector Search', 'description': ' Let’s dive into the vector search to enhance the accuracy and efficiency of information retrieval, delivering more relevant results.', 'language': 'en'}),\n",
       " Document(page_content='Naomi Lago - Image classification with Deep Learning made easy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nImage classification with Deep Learning made easy\\n\\n\\nToday is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.\\n        \\n\\n\\nCOMPUTER VISION\\nDEEP LEARNING\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 9, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0You may have heard about Computer Vision before - that refers to a field of Artificial Intelligence that trains computers to interpret and understand the visual world. It enables computers to derive information from images, videos, and other inputs. Today we’ll dive in the practical manner in a high level by classifying images and using Fast AI, a library built on top of Pytorch.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\nGetting the images\\n\\n\\nData block\\n\\n\\nTraining\\n\\n\\nEvaluation\\n\\n\\nTesting\\n\\n\\n\\n\\nIntroduction', metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content='Table of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\nGetting the images\\n\\n\\nData block\\n\\n\\nTraining\\n\\n\\nEvaluation\\n\\n\\nTesting\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\n\\xa0\\xa0\\xa0To classify images, I will not be using vanilla PyTorch or TensorFlow. Instead, I will introduce the amazing library from FastAI, which makes deep learning more accessible. The goal is to accomplish a task to understand the capabilities of computers today and generate enough interest to dive deeper into the subject. For that, we’ll be classifying whether an image is a watermelon or a strawberry.\\n\\n\\nI am currently using an NVIDIA RTX A6000 GPU, but I believe it can also be accomplished with a less powerful GPU. It is recommended to run this task on a GPU rather than a CPU, though.\\n\\n\\n\\xa0\\xa0\\xa0So, without any more further ado, let’s start by downloading some libraries and importing them:\\n\\n\\n\\n%pip install -Uqq loguru fastai duckduckgo_search\\n\\nNote: you may need to restart the kernel to use updated packages.\\n\\n\\n\\nfrom fastdownload import download_url\\nfrom duckduckgo_search import DDGS\\nfrom fastai.vision.all import *\\nfrom fastcore.all import *\\nfrom loguru import logger\\nfrom time import sleep\\nimport fastcore\\nimport warnings\\nimport socket\\n\\nwarnings.filterwarnings(\"ignore\")', metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content='warnings.filterwarnings(\"ignore\")\\n\\ntry:\\n  socket.setdefaulttimeout(1)\\n  socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((\\'1.1.1.1\\', 53))\\n  logger.success(\\'Socket configured ✔\\')\\nexcept socket.error as ex:\\n  raise Exception(\\'No internet connection...\\')\\n\\n2023-09-09 15:28:41.891 | SUCCESS  | __main__:<module>:16 - Socket configured ✔\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Below, you can find the description of each library installed.\\n\\n\\n\\nLoguru:\\xa0A Python logging library that simplifies and enhances the logging process with a clean and intuitive syntax, making it easier to set up and customize logging in your applications.\\n\\n\\nFastai:\\xa0A high-level deep learning library built on top of PyTorch that provides easy-to-use APIs for training and deploying deep neural networks, making it accessible for both beginners and experts in machine learning.\\n\\n\\nDuckDuckGo Search:\\xa0 An internet search engine that emphasizes user privacy by not tracking user activities or storing personal information, offering a privacy-focused alternative to other search engines like Google.', metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content=\"DuckDuckGo Search:\\xa0 An internet search engine that emphasizes user privacy by not tracking user activities or storing personal information, offering a privacy-focused alternative to other search engines like Google.\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Another important thing to mention is the try/except part on this setup. It is important because it performs a simple check to determine if the host has internet connectivity before proceeding with further execution of the program. This is vital as I’ll be downloading images later.\\n\\n\\n\\nGetting the images\\n\\n\\n\\n\\xa0\\xa0\\xa0So, in order to train a model, we’ll need some data so it can learn from them, as this is a supervised learning problem. I’ll be creating a function to search for images and returning an L type.\\n\\n\\nAn L type is is a function that converts a regular Python list into a fastai list, offering extra functionality compared to a regular list, such as filtering and mapping\\n\\n\\n\\ndef search_images(keyword: str, max: int = 50) -> fastcore.foundation.L:\\n  print(f'{max} images of {keyword} coming...')\\n  \\n  return L(DDGS().images(keyword, safesearch='On')).itemgot('image')[:max]\\n\\n\\n\\n\\xa0\\xa0\\xa0Let’s now try downloading a strawberry and a watermelon photo and check if our function was correctly implemented.\", metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content=\"Let’s now try downloading a strawberry and a watermelon photo and check if our function was correctly implemented.\\n\\n\\n\\nurls = search_images(keyword = 'strawberry', max = 3)\\ndestination = './assets/strawberry/strawberry.jpg'\\n\\ndownload_url(urls[0], destination, show_progress=False)\\n\\nimg = Image.open(destination)\\nimg.to_thumb(256, 256)\\n\\n3 images of strawberry coming...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nurls = search_images(keyword = 'watermelon', max = 10)\\ndestination = './assets/watermelon/watermelon.jpg'\\n\\ndownload_url(urls[6], destination, show_progress=False)\\n\\nimg = Image.open(destination)\\nimg.to_thumb(256, 256)\\n\\n10 images of watermelon coming...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Awesome, now let’s properly download these images in a greater amount and in two diferent folder - these folder will be the class names later on.\\n\\n\\nNote that I used a sleep of 5 second, so I don’t get request troubles.\\n\\n\\n\\nsearches: tuple = 'strawberry', 'watermelon'\\npath = Path('./assets')\", metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content=\"Note that I used a sleep of 5 second, so I don’t get request troubles.\\n\\n\\n\\nsearches: tuple = 'strawberry', 'watermelon'\\npath = Path('./assets')\\n\\nfor search in searches:\\n  destination = (path/search)\\n  destination.mkdir(exist_ok = True, parents = True)\\n  \\n  download_images(destination, urls = search_images(f'{search} photos'))\\n  sleep(5)\\n  download_images(destination, urls = search_images(f'{search} black and white'))\\n  sleep(5)\\n  download_images(destination, urls = search_images(f'{search} cartoon'))\\n  sleep(5)\\n  download_images(destination, urls = search_images(f'{search} AI'))\\n  sleep(5)\\n  \\n  resize_images(path/search, max_size = 400, dest = path/search)\", metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content=\"50 images of strawberry photos coming...\\n50 images of strawberry black and white coming...\\n50 images of strawberry cartoon coming...\\n50 images of strawberry AI coming...\\n50 images of watermelon photos coming...\\n50 images of watermelon black and white coming...\\n50 images of watermelon cartoon coming...\\n50 images of watermelon AI coming...\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Let’s also ensure that all our images are valid, so I’ll make a quick check and unlink the broken ones.\\n\\n\\n\\nfailed = verify_images(get_image_files(path))\\nfailed.map(Path.unlink)\\n\\nif len(failed) > 0:\\n  print(f'There were {len(failed)} failed images.')\\nelse:\\n  print('There were no failed images.')\\n\\nThere were 4 failed images.\\n\\n\\n\\n\\nData block\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that we have the images downloaded, let’s use a DataBlock to define the data processing pipeline for creating data loaders - that are important for several key reasons including: efficient data loading, batching, data augmentation, shuffling, paralellism etc.\", metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content=\"dls = DataBlock(\\n  blocks = (ImageBlock, CategoryBlock),\\n  get_items = get_image_files,\\n  splitter = RandomSplitter(valid_pct = 0.2, seed = 20),\\n  get_y = parent_label,\\n  item_tfms = [Resize(192, method = 'squish')]\\n).dataloaders(path, bs = 32)\\n\\ndls.show_batch(max_n = 6)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTraining\\n\\n\\n\\n\\xa0\\xa0\\xa0Alright, so far we have downloaded our images and created our dataloader. Now it’s time to train our model and, for that, we’ll see how simplified it is to use Fast AI.\\n\\n\\n\\nlearn = vision_learner(dls, resnet18, metrics = [error_rate, accuracy])\\nlearn.fine_tune(15)\\n\\n\\n\\n\\n\\n\\n\\n\\nepoch\\ntrain_loss\\nvalid_loss\\nerror_rate\\naccuracy\\ntime\\n\\n\\n\\n\\n0\\n0.575720\\n0.110310\\n0.034483\\n0.965517\\n00:04\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nepoch\\ntrain_loss\\nvalid_loss\\nerror_rate\\naccuracy\\ntime\", metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content='epoch\\ntrain_loss\\nvalid_loss\\nerror_rate\\naccuracy\\ntime\\n\\n\\n\\n\\n0\\n0.115316\\n0.086936\\n0.025862\\n0.974138\\n00:03\\n\\n\\n1\\n0.065104\\n0.045375\\n0.021552\\n0.978448\\n00:02\\n\\n\\n2\\n0.055632\\n0.150614\\n0.030172\\n0.969828\\n00:03\\n\\n\\n3\\n0.071165\\n0.094440\\n0.030172\\n0.969828\\n00:03\\n\\n\\n4\\n0.070642\\n0.131956\\n0.017241\\n0.982759\\n00:02\\n\\n\\n5\\n0.052290\\n0.057936\\n0.017241\\n0.982759\\n00:03\\n\\n\\n6\\n0.046295\\n0.045453\\n0.017241\\n0.982759\\n00:03\\n\\n\\n7\\n0.045184\\n0.042271\\n0.017241\\n0.982759\\n00:02\\n\\n\\n8\\n0.043151\\n0.054159\\n0.034483\\n0.965517\\n00:02', metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content='8\\n0.043151\\n0.054159\\n0.034483\\n0.965517\\n00:02\\n\\n\\n9\\n0.035403\\n0.045328\\n0.021552\\n0.978448\\n00:02\\n\\n\\n10\\n0.029595\\n0.028453\\n0.017241\\n0.982759\\n00:03\\n\\n\\n11\\n0.022235\\n0.046433\\n0.017241\\n0.982759\\n00:02\\n\\n\\n12\\n0.017884\\n0.039667\\n0.017241\\n0.982759\\n00:03\\n\\n\\n13\\n0.017663\\n0.032925\\n0.017241\\n0.982759\\n00:03\\n\\n\\n14\\n0.020569\\n0.036629\\n0.017241\\n0.982759\\n00:02\\n\\n\\n\\n\\n\\n\\n\\n\\nEvaluation\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally we have our model trained to recognize images and say whether it’s a strawberry or watermelon picture. Let’s go through some visualizations on its performance by viewing the confusion matrix, the classification report and a loss plot.\\n\\n\\n\\ninterp = ClassificationInterpretation.from_learner(learn)\\nlosses,idxs = interp.top_losses()', metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content=\"interp = ClassificationInterpretation.from_learner(learn)\\nlosses,idxs = interp.top_losses()\\n\\nassert  len(dls.valid_ds)==len(losses)==len(idxs)\\n\\ninterp.plot_confusion_matrix(figsize=(4, 4.3), cmap='Reds')\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ninterp.print_classification_report()\\n\\n\\n\\n\\n\\n\\n              precision    recall  f1-score   support\\n\\n  strawberry       1.00      0.97      0.98       117\\n  watermelon       0.97      1.00      0.98       115\", metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content=\"accuracy                           0.98       232\\n   macro avg       0.98      0.98      0.98       232\\nweighted avg       0.98      0.98      0.98       232\\n\\n\\n\\n\\n\\ninterp = ClassificationInterpretation.from_learner(learn)\\nlosses, idxs = interp.top_losses()\\n\\nassert len(dls.valid_ds) == len(losses) == len(idxs)\\n\\nplt.figure(figsize=(4.5, 4.3))\\nplt.plot(losses[:20], c='#9a031e')\\nplt.title('Losses throughout the steps')\\nplt.xlabel('Step')\\nplt.ylabel('Loss')\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTesting\", metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content=\"Testing\\n\\n\\n\\n\\xa0\\xa0\\xa0We’re getting to the end, as we’ve already developed a model that can differentiate images from two distinct fruits. Now I want to finish by downloading a test image that the model hasn’t seen before and even get the probabilities.\\n\\n\\n\\xa0\\xa0\\xa0I’ll be choosing an image from Lexica Art and the image is as follow:\\n\\n\\n\\ndownload_url(\\n  'https://image.lexica.art/full_jpg/d814231d-332d-4569-84d7-820ff4742e38', \\n  './assets/test/test_image.jpg', \\n  show_progress=False)\\n\\nImage.open('./assets/test/test_image.jpg').to_thumb(256, 256)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ncategory, _, probabilities = learn.predict(\\n  PILImage.create('./assets/test/test_image.jpg')\\n  )\\n\\nprint(f'PREDICTION REPORT:\\\\n')\\nprint(f'Category: {category}')\\nprint(f'Probability: {torch.max(probabilities):.3f}')\\n\\n\\n\\n\\n\\n\\nPREDICTION REPORT:\\n\\nCategory: strawberry\\nProbability: 1.000\", metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content='PREDICTION REPORT:\\n\\nCategory: strawberry\\nProbability: 1.000\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/image-classification-with-deep-learning-made-easy/', 'title': 'Naomi Lago - Image classification with Deep Learning made easy', 'description': ' Today is the day where we’ll explore the deep learning in a high level and in a more practical way - by building an image classifier from scratch.', 'language': 'en'}),\n",
       " Document(page_content='Naomi Lago - Vector Search with Facebook AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nVector Search with Facebook AI\\n\\n\\nIn today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.\\n        \\n\\n\\nNLP\\nVECTOR SEARCH\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 10, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0In the last article, I talked about information retrieval with Vector Search, and one of the technologies I mentioned was the FAISS library from Facebook. It‚Äôs an open-source library for efficient similarity search and clustering of dense vectors.', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='In the last article, I talked about information retrieval with Vector Search, and one of the technologies I mentioned was the FAISS library from Facebook. It‚Äôs an open-source library for efficient similarity search and clustering of dense vectors. Today, let‚Äôs explore an implementation of this masterpiece on a set of texts - exemplifying, but not limiting, the use of this tool that can bring excellent results.\\n\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\nImplementation\\n\\n\\n\\nSetting up\\n\\n\\nVectorization\\n\\n\\nFAISS configuration\\n\\n\\nSearch\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\xa0\\xa0\\xa0Well, as I said, we will talk about FAISS - which stands for Facebook AI Similarity Search. This is a technology developed by Facebook‚Äôs Artificial Intelligence Research team (FAIR) and was released in March 2017. It is delivered in the form of a Python library and designed to be efficient in searching for and retrieving similar vectors in large datasets, making it very useful in recommendation systems, computer vision, natural language processing (NLP), or anomaly detection.\\n\\n\\n\\xa0\\xa0\\xa0FAISS offers a wide range of indexing methods, including exhaustive search, k-means, product quantization, and HNSW. These methods enable faster and more accurate searches, even in high-dimensional spaces.', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='FAISS offers a wide range of indexing methods, including exhaustive search, k-means, product quantization, and HNSW. These methods enable faster and more accurate searches, even in high-dimensional spaces.\\n\\n\\n\\xa0\\xa0\\xa0Below, we can observe a representation of how this works under the hood. After indexing our vectorized training data, we can perform searches with new vectors that will use cosine distance, inner product, or L2 distance for the search, which then gives us an output in ascending order with the smallest similarities (indicating they are close and related).\\n\\n\\n\\n\\n\\nFAISS workflow\\n\\n\\n\\n\\nImplementation\\n\\n\\n\\n\\xa0\\xa0\\xa0Alright, let‚Äôs now get practical. As I mentioned before, today we‚Äôre going to implement this algorithm on a set of texts - which means we‚Äôll be searching for texts that are similar to each other.\\n\\n\\n\\nSetting up\\n\\n\\nThe codes are available under an open-source license on GitHub.\\n\\n\\n\\xa0\\xa0\\xa0To get started, I recommend creating a virtual environment for your project using tools like pip, conda, poetry, etc. Next, install the necessary libraries, which in this case will be:\\n\\n\\n\\nsentence_transformers:\\xa0Responsible for vectorizing our texts\\n\\n\\nfaiss:\\xa0Responsible for vector search', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content=\"sentence_transformers:\\xa0Responsible for vectorizing our texts\\n\\n\\nfaiss:\\xa0Responsible for vector search\\n\\n\\n\\n\\xa0\\xa0\\xa0Then, you can import the libraries, load your dataframe, and assign an incremental identification column, which can even be the index.\\n\\n\\n\\n%pip install faiss-gpu -Uqq\\n\\nNote: you may need to restart the kernel to use updated packages.\\n\\n\\n\\nfrom sentence_transformers import SentenceTransformer\\nfrom sentence_transformers import InputExample\\nimport pandas as pd\\nimport numpy as np\\nimport faiss\\nimport torch\\n\\ndf = pd.read_csv('./assets/similarity_search.csv')[['text', 'id']]\\n\\nif len(df) != 0:\\n  print(f'Dataset imported succcesfully with a shape of {df.shape} \\uf8ffüéâ')\\n\\nif df.id.min() > 0:\\n  df.id = df.id.apply(lambda x: x - 1)\\nelse:\\n  print('ID Starting with zero!')\\n  \\ndf.head(5)\\n\\nDataset imported succcesfully with a shape of (280, 2) \\uf8ffüéâ\\n\\n\\n\\n\\n\\n\\n\\n\\ntext\\nid\", metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content=\"Dataset imported succcesfully with a shape of (280, 2) \\uf8ffüéâ\\n\\n\\n\\n\\n\\n\\n\\n\\ntext\\nid\\n\\n\\n\\n\\n0\\nThe COVID-19 pandemic has had a significant im...\\n0\\n\\n\\n1\\nArtificial intelligence is transforming variou...\\n1\\n\\n\\n2\\nSocial media platforms play a crucial role in ...\\n2\\n\\n\\n3\\nRenewable energy sources like solar and wind p...\\n3\\n\\n\\n4\\nCryptocurrencies such as Bitcoin have gained w...\\n4\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVectorization\\n\\n\\n\\n\\xa0\\xa0\\xa0Now that we have our data, let‚Äôs instantiate our vectorization model using SentenceTransformer. It takes the model‚Äôs name as an argument, the device being used, or a cache directory.\\n\\n\\n\\xa0\\xa0\\xa0The model‚Äôs name is the only mandatory argument, and you can find it on the Hugging Face model hub if you‚Äôre unsure which one to choose. The device used is important because if you‚Äôre working with large volumes or vector operations, a GPU can be beneficial, and the cache directory is used to store model information to avoid downloading it every time you call it.\\n\\n\\n\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\", metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content=\"device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n\\nprint(f'Using the {str.upper(device)} device!')\\n\\nmodel = SentenceTransformer(\\n  'distilbert-base-nli-stsb-mean-tokens',\\n  device = device,\\n  cache_folder = './assets/cache/'\\n)\\n\\nUsing the CUDA device!\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0To test our vectorization model, let‚Äôs vectorize our first 5 texts. Note that the vectors, also called embeddings, have already been generated:\\n\\n\\n\\ntexts = df.text.values.tolist()\\ntexts[:3]\\n\\n['The COVID-19 pandemic has had a significant impact on global economies.',\\n 'Artificial intelligence is transforming various industries, including healthcare and finance.',\\n 'Social media platforms play a crucial role in connecting people around the world.']\\n\\n\\n\\n\\nembeddings = model.encode(texts)\\nembeddings[:3]\", metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='embeddings = model.encode(texts)\\nembeddings[:3]\\n\\narray([[ 0.94750345, -1.0846483 , -0.22848324, ..., -0.06836035,\\n        -0.14919858,  0.6607197 ],\\n       [ 0.28361621, -0.14619698,  0.76421624, ..., -0.09583294,\\n        -0.00354804,  0.03140309],\\n       [ 0.0471359 , -0.09198052,  0.03990437, ..., -0.05552321,\\n        -1.0880417 , -0.33173537]], dtype=float32)\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0With that done, we are ready for the FAISS configuration that comes next.\\n\\n\\n\\nFAISS configuration', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='With that done, we are ready for the FAISS configuration that comes next.\\n\\n\\n\\nFAISS configuration\\n\\n\\n\\n\\xa0\\xa0\\xa0First, let‚Äôs use our ‚Äúid‚Äù column as the index and save this transformation in a variable, as in line 1. Next, we‚Äôll create an index identifier that contains the values from the ‚Äúid‚Äù column, which will serve as a mapping to the FAISS index‚Äôs indices - helping us associate the dataframe‚Äôs identifiers with the vector points.\\n\\n\\n\\xa0\\xa0\\xa0As a next step, let‚Äôs normalize the vectors generated earlier using L2 normalization - preserving the relative direction between the vectors. In line 4, I made a copy to preserve the original array in case it‚Äôs needed later.\\n\\n\\n\\xa0\\xa0\\xa0Finally, let‚Äôs create a FAISS index of type IndexFlatIP, which will be used to perform queries on the points using the inner product. The parameter defines the dimension of the space, and the choice of this index type was simply because it is efficient and common.', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content=\"df_to_index = df.set_index(['id'], drop = False)\\nid_index = np.array(df_to_index.id.values).flatten().astype('int')\\n\\nnormalized_embeddings = embeddings.copy()\\nfaiss.normalize_L2(normalized_embeddings)\\nindex_flat = faiss.IndexFlatIP(len(embeddings[0]))\\n\\n\\n\\n\\xa0\\xa0\\xa0Next, we‚Äôll create an IndexIDMap object, which will be created from the index_flat and will allow us to associate identifiers with indexed vectors. This association is useful for retrieving specific information from the indexed vectors based on the identifiers.\\n\\n\\n\\xa0\\xa0\\xa0To complete the FAISS configuration, we‚Äôll add the normalized vectors and their corresponding identifiers to the IndexIDMap object, enabling queries and specific information retrieval.\\n\\n\\n\\nindex_content = faiss.IndexIDMap(index_flat)\\nindex_content.add_with_ids(normalized_embeddings, id_index)\\n\\n\\n\\nSearch\", metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='index_content = faiss.IndexIDMap(index_flat)\\nindex_content.add_with_ids(normalized_embeddings, id_index)\\n\\n\\n\\nSearch\\n\\n\\n\\n\\xa0\\xa0\\xa0Indeed, when it comes to vector search, data must be prepared to match the FAISS standard before conducting searches. This also involves formatting and organizing the output for a final delivery. Preprocessing and post-processing steps are essential to ensure the effectiveness and usability of vector search systems.\\n\\n\\n\\xa0\\xa0\\xa0With this, we define a function that will take care of these steps for us. As parameters, I‚Äôve defined query for the search texts/terms and k for the number of results we want.\\n\\n\\n\\xa0\\xa0\\xa0First, we vectorize our text using SentenceTransformer, and then we normalize it using the FAISS normalizer itself, as shown in lines 2 and 3. Now, we search for the top-k results similar to our query term, as demonstrated in line 5.\\n\\n\\n\\xa0\\xa0\\xa0The identifiers and similarities are then extracted from the result stored in top_k, and a message with the search text is printed on the screen, as seen in lines 6 and 7. Finally, in lines 11 and 12, the results are obtained from the dataframe that had the identifiers transformed into indices, and a new column corresponding to the similarities of the points found with the query is added.', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='In conclusion, the results are returned as a new dataframe with redefined indices, discarding the previous ones.\\n\\n\\n\\ndef search(query: str, k: int = 5) -> pd.core.frame.DataFrame:\\n  vector = model.encode([query])\\n  faiss.normalize_L2(vector)\\n  \\n  top_k = index_content.search(vector, k)\\n  ids = top_k[1][0].tolist()\\n  similarities = top_k[0][0].tolist()\\n  \\n  print(f\\'Searching for \"{query}\"...\\')\\n  \\n  results = df_to_index.loc[ids]\\n  results[\\'similarity\\'] = similarities\\n  output = results.reset_index(drop = True)[[\\'id\\', \\'text\\', \\'similarity\\']]\\n  \\n  return output\\n\\n\\n\\n\\xa0\\xa0\\xa0Let‚Äôs see it in practice. Below, I conducted three different searches with different numbers of returns. Let‚Äôs see how it behaved.\\n\\n\\n\\nsearch(\\'What am I thinking? I love cars\\', 3)\\n\\nSearching for \"What am I thinking? I love cars\"...\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\ntext\\nsimilarity', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='search(\\'What am I thinking? I love cars\\', 3)\\n\\nSearching for \"What am I thinking? I love cars\"...\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\ntext\\nsimilarity\\n\\n\\n\\n\\n0\\n6\\nSelf-driving cars have the potential to enhanc...\\n0.456869\\n\\n\\n1\\n91\\nThe ethical considerations of AI-powered auton...\\n0.395022\\n\\n\\n2\\n228\\nThe use of AI in personalized entertainment re...\\n0.382918\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsearch(\\'Biology is interesting, send me 5 related topics please\\', 5)\\n\\nSearching for \"Biology is interesting, send me 5 related topics please\"...\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\ntext\\nsimilarity\\n\\n\\n\\n\\n0\\n67\\nThe exploration of deep-sea ecosystems reveals...\\n0.528142\\n\\n\\n1\\n160\\nAdvancements in bioinformatics enable faster a...\\n0.474495\\n\\n\\n2\\n38\\nEthical considerations in gene editing and clo...\\n0.464431\\n\\n\\n3\\n64\\nThe ethical implications of gene editing and C...\\n0.449795\\n\\n\\n4\\n132\\nThe potential of bioprinting technology in reg...\\n0.427724\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsearch(\\'Can you suggest me something about Deep Learning?\\', 6)', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='4\\n132\\nThe potential of bioprinting technology in reg...\\n0.427724\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsearch(\\'Can you suggest me something about Deep Learning?\\', 6)\\n\\nSearching for \"Can you suggest me something about Deep Learning?\"...\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\ntext\\nsimilarity\\n\\n\\n\\n\\n0\\n255\\nThe challenges of interpretability in deep lea...\\n0.564732\\n\\n\\n1\\n197\\nAdvancements in quantum machine learning algor...\\n0.487619\\n\\n\\n2\\n184\\nThe integration of AI in education transforms ...\\n0.471418\\n\\n\\n3\\n113\\nThe potential of quantum sensors opens up new ...\\n0.467105\\n\\n\\n4\\n106\\nThe role of AI in personalized education and a...\\n0.454311\\n\\n\\n5\\n137\\nThe potential of 3D bioprinting in tissue engi...\\n0.447400\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0We have explored the FAISS tool and seen that it offers various indexing methods that enable fast and accurate searches, making it applicable in various domains. We demonstrated practical implementation on a set of texts, but we know that vector search can be applied to any type of data source as long as vectorization is possible.', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='FAISS stands out for its ability to handle high-dimensional spaces, so remember that our implementation was just a test and usage example. Its flexibility and performance make it a promising option, and its use in conjunction with vectorization techniques, such as SentenceTransformer, can bring significant benefits in terms of efficiency and accuracy.\\n\\n\\xa0\\xa0\\xa0Below, I provide the link to the GitHub repository with the project and the data used, as well as to the official website and documentation.\\n\\n\\n\\n\\n\\n\\n\\nReferences\\n\\n\\n\\n\\nGithub repository\\n\\n\\nFAISS official webpage\\n\\n\\nFAISS documentation\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I‚Äôll see you in the next one ‚≠ê\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/vector-search-with-facebook-ai/', 'title': 'Naomi Lago - Vector Search with Facebook AI', 'description': ' In today‚Äôs post, I invite you to dive with me into exploring a great tool for information retrieval, specifically vector search, using a fantastic technology by Facebook AI.', 'language': 'en'}),\n",
       " Document(page_content='Naomi Lago - Recognizing Handwriting Digits\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nRecognizing Handwriting Digits\\n\\n\\nToday, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.\\n        \\n\\n\\nDEEP LEARNING\\nDATA SCIENCE\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nSeptember 11, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0In the ever-evolving realm of machine learning and image processing, one dataset has stood the test of time as a benchmark for countless algorithms and models: the MNIST dataset. Short for the Modified National Institute of Standards and Technology database, MNIST is a treasure trove of handwritten digits, ranging from 0 to 9, encapsulated within a repository of 70,000 grayscale images. In this blog post, we will delve into the intricacies of MNIST, exploring its significance, structure, and applications.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nSetting up\\n\\n\\nOne Hot Encoder (OHE)\\n\\n\\nDataset class\\n\\n\\nData loader object', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"Table of Contents\\n\\n\\n\\n\\nSetting up\\n\\n\\nOne Hot Encoder (OHE)\\n\\n\\nDataset class\\n\\n\\nData loader object\\n\\n\\nCross-entropy loss\\n\\n\\nNeural network\\n\\n\\nTraining\\n\\n\\nTesting\\n\\n\\nConclusion\\n\\n\\n\\n\\nSetting up\\n\\n\\n\\n\\xa0\\xa0\\xa0Before properly starting, I‚Äôll be importing the necessary libraries and also loading our data.\\n\\n\\n\\n%pip install -Uqq plotly seaborn\\n\\nNote: you may need to restart the kernel to use updated packages.\\n\\n\\n\\n\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom sklearn.metrics import confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport torch.nn.functional as F\\nimport plotly.express as px\\nfrom torch.optim import SGD\\nfrom tqdm import tqdm\\nimport seaborn as sns\\nimport torch.nn as nn\\nimport numpy as np\\nimport torchvision\\nimport torch\\n\\n%matplotlib inline\\n\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\\nprint(f'Using {str.upper(device)} device \\uf8ffüåü')\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"%matplotlib inline\\n\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\\nprint(f'Using {str.upper(device)} device \\uf8ffüåü')\\n\\nx, y = torch.load('./MNIST/processed/training.pt')\\nprint(x.shape, y.shape)\\n\\nUsing CUDA device \\uf8ffüåü\\ntorch.Size([60000, 28, 28]) torch.Size([60000])\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0We can then visualize a sample:\\n\\n\\n\\nfig = plt.figure(figsize=(4, 3.5))\\n\\ndef show_number(i: int = 0):\\n    plt.imshow(x[i].numpy(), cmap='Blues')\\n    plt.title(f'The number is {y[i].numpy()}')\\n    plt.colorbar()\\n\\n    plt.show()\\n            \\nshow_number(20)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOne Hot Encoder\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='plt.show()\\n            \\nshow_number(20)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOne Hot Encoder\\n\\n\\n\\n\\xa0\\xa0\\xa0A next step is using One Hot Encoder to transform categorical data into numerical data. In this case, representing the digits 0-9 it‚Äôll allow each label to be represented as a binary vector that is all 0 except the index of the integer (label) itself that‚Äôd be marked as 1.\\n\\n\\n\\n# Performing a test before properly apply to our dataset\\n\\noriginal_y = torch.tensor([2, 4, 3, 0, 1])\\nnew_y = F.one_hot(original_y)\\nnew_y\\n\\ntensor([[0, 0, 1, 0, 0],\\n        [0, 0, 0, 0, 1],\\n        [0, 0, 0, 1, 0],\\n        [1, 0, 0, 0, 0],\\n        [0, 1, 0, 0, 0]])\\n\\n\\n\\n\\n# Applying in the MNIST data\\n\\nnew_y = F.one_hot(y, num_classes=10)', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='# Applying in the MNIST data\\n\\nnew_y = F.one_hot(y, num_classes=10)\\n\\ny.reshape(-1, 1).shape[-1], new_y.shape[-1] # Expected (1, 10)\\n\\n(1, 10)\\n\\n\\n\\n\\nDataset class\\n\\n\\n\\n\\xa0\\xa0\\xa0This is another important step as a class is used to represent a collection of data that can be used for training a model. It is an abstract class that can be inherited to create a custom dataset that can be used to load and manipulate sets containing data in many forms.', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"class CTDataset(Dataset):\\n    def __init__(self, filepath):\\n        self.x, self.y = torch.load(filepath)\\n        self.x = self.x / 255.\\n        self.y = F.one_hot(self.y, num_classes=10).to(float)\\n    \\n    def __len__(self): \\n        return self.x.shape[0]\\n    \\n    def __getitem__(self, ix): \\n        return self.x[ix], self.y[ix]\\n\\ntrain_dataset = CTDataset('./MNIST/processed/training.pt')\\ntest_dataset = CTDataset('./MNIST/processed/test.pt')\\n\\n\\n\\nData loader object\\n\\n\\n\\n\\xa0\\xa0\\xa0Here‚Äôs the time when we create the dataloader objects. These are important components of Deep Learning pipelines that help to load and preprocess data for training. They are used to handle large datasets and perform data ugmentation, shuffling, and other preprocessing tasks.\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"They are importat for some reasons including: Standardisation, Efficiency and Flexibility.\\n\\n\\n\\ntrain_dataloader = DataLoader(train_dataset, batch_size=5)\\n\\nfor x, y in train_dataloader:\\n    x = x.to(device)\\n    y = y.to(device)\\n    print(x.shape)\\n    print(y.shape)\\n    break    \\n    \\nprint(f'\\\\n{len(train_dataloader)}')\\n\\ntorch.Size([5, 28, 28])\\ntorch.Size([5, 10])\\n\\n12000\\n\\n\\n\\n\\nCross-entropy loss\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, Cross-entropy loss is a widely used loss function in classification tasks. It has several advantages that make it popular, including:\\n\\n\\n\\nPenalizes incorrect predictions\\n\\n\\nMeasures model performance\\n\\n\\nWorks with multiple clases\\n\\n\\nPopular and well-understood\\n\\n\\n\\n\\nL = nn.CrossEntropyLoss().to(device)\\nL\\n\\nCrossEntropyLoss()\\n\\n\\n\\n\\nNeural network\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='Popular and well-understood\\n\\n\\n\\n\\nL = nn.CrossEntropyLoss().to(device)\\nL\\n\\nCrossEntropyLoss()\\n\\n\\n\\n\\nNeural network\\n\\n\\n\\n\\xa0\\xa0\\xa0As we‚Äôre approaching this problem using Deep Learning, a neural network is important to keep it functional - so, here I declare the network for this classifier:', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='class DigitsClassifier(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.Matrix1 = nn.Linear(28**2,100)\\n        self.Matrix2 = nn.Linear(100,50)\\n        self.Matrix3 = nn.Linear(50,10)\\n        self.R = nn.ReLU()\\n        \\n    def forward(self,x):\\n        x = x.view(-1,28**2)\\n        x = self.R(self.Matrix1(x))\\n        x = self.R(self.Matrix2(x))\\n        x = self.Matrix3(x)\\n        \\n        return x.squeeze()\\n        \\n        # output = x.squeeze()', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='return x.squeeze()\\n        \\n        # output = x.squeeze()\\n        \\n        # return output.argmax(axis = 1)', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='model = DigitsClassifier().to(device)\\nmodel\\n\\nDigitsClassifier(\\n  (Matrix1): Linear(in_features=784, out_features=100, bias=True)\\n  (Matrix2): Linear(in_features=100, out_features=50, bias=True)\\n  (Matrix3): Linear(in_features=50, out_features=10, bias=True)\\n  (R): ReLU()\\n)\\n\\n\\n\\n\\nxs, ys = train_dataset[0:5]\\n\\nmodel(xs.to(device))', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='tensor([[-0.0759, -0.0578, -0.0325,  0.1515, -0.0489, -0.0079, -0.0817,  0.1410,\\n         -0.0657, -0.0913],\\n        [-0.0584, -0.0314, -0.0718,  0.1930, -0.0209, -0.0429, -0.0243,  0.1449,\\n         -0.0411, -0.0707],\\n        [-0.0568, -0.0475, -0.0563,  0.1828, -0.0044, -0.0150, -0.0350,  0.1090,\\n         -0.0087, -0.0581],\\n        [-0.0622, -0.0535, -0.0551,  0.1234, -0.0365, -0.0321, -0.0423,  0.1067,', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"-0.0618, -0.0971],\\n        [-0.0458, -0.0897, -0.0495,  0.1648,  0.0038, -0.0291, -0.0159,  0.0708,\\n         -0.0066, -0.0851]], device='cuda:0', grad_fn=<SqueezeBackward0>)\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"L(model(xs.to(device)), ys.to(device))\\n\\ntensor(2.3381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward1>)\\n\\n\\n\\n\\nTraining\\n\\n\\n\\n\\xa0\\xa0\\xa0Another essential part of a machine learning process is training. It involves optimizing the parameters of a model to minimize the difference between the predicted output and the true output. In the given code, I‚Äôll be defining a function to do it for us that takes as input a data loader, a model, and the number of epochs.\\n\\n\\nDuring training, the function iterates over the data loader and updates the model parameters using backpropagation and gradient descent. The loss and epoch data are recorded for each iteration, which will be used to visualize the training progress and evaluate the performance.\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='def train_model(\\n    dataloader: torch.utils.data.dataloader.DataLoader, \\n    model: DigitsClassifier, \\n    n_epochs: int = 30) -> tuple[np.ndarray, np.ndarray]:\\n    \\n    optimizer = SGD(model.parameters(), lr=1e-2)\\n    L = nn.CrossEntropyLoss()', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"losses = []\\n    epochs = []\\n    \\n    for epoch in range(n_epochs):\\n        print(f'Epoch: {epoch + 1}/{n_epochs} | Loss: {np.mean(losses)}')\\n        N = len(dataloader)\\n                \\n        for i, (x, y) in enumerate(dataloader):\\n            x = x.to(device)\\n            y = y.to(device)\\n            \\n            optimizer.zero_grad() \\n            loss_value = L(model(x), y) \\n            \\n            if loss_value > torch.max(y).item():\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"if loss_value > torch.max(y).item():\\n                torch.save(model.state_dict(), './MNIST/models/mnist_model.pth')\\n            \\n            loss_value.backward() \\n            optimizer.step()\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='epochs.append(epoch + i / N)\\n            losses.append(loss_value.item())\\n            \\n    return np.array(epochs), np.array(losses)\\n\\nepoch_data, loss_data = train_model(train_dataloader, model)\\n\\n/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\\n  return _methods._mean(a, axis=axis, dtype=dtype,\\n/usr/local/lib/python3.9/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\\n  ret = ret.dtype.type(ret / rcount)', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='Epoch: 1/30 | Loss: nan\\nEpoch: 2/30 | Loss: 0.444299907778694\\nEpoch: 3/30 | Loss: 0.30499113273745887\\nEpoch: 4/30 | Loss: 0.2411346833651296\\nEpoch: 5/30 | Loss: 0.20264087833581762\\nEpoch: 6/30 | Loss: 0.17611513000308468\\nEpoch: 7/30 | Loss: 0.15636798159883528\\nEpoch: 8/30 | Loss: 0.1409401671701855\\nEpoch: 9/30 | Loss: 0.1283711091479482\\nEpoch: 10/30 | Loss: 0.1178500554705612\\nEpoch: 11/30 | Loss: 0.10884905771381158\\nEpoch: 12/30 | Loss: 0.10104153238275904\\nEpoch: 13/30 | Loss: 0.09419568914679677\\nEpoch: 14/30 | Loss: 0.08814844876493146\\nEpoch: 15/30 | Loss: 0.08278729691344265', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='Epoch: 14/30 | Loss: 0.08814844876493146\\nEpoch: 15/30 | Loss: 0.08278729691344265\\nEpoch: 16/30 | Loss: 0.07798079398823612\\nEpoch: 17/30 | Loss: 0.073632377445332\\nEpoch: 18/30 | Loss: 0.06971223861976696\\nEpoch: 19/30 | Loss: 0.06615196071459868\\nEpoch: 20/30 | Loss: 0.06291801628939395\\nEpoch: 21/30 | Loss: 0.05995343980754616\\nEpoch: 22/30 | Loss: 0.057242643725737494\\nEpoch: 23/30 | Loss: 0.0547542955197936\\nEpoch: 24/30 | Loss: 0.052467638791425374\\nEpoch: 25/30 | Loss: 0.050359719916244224\\nEpoch: 26/30 | Loss: 0.04841136845960758\\nEpoch: 27/30 | Loss: 0.04660600281967505\\nEpoch: 28/30 | Loss: 0.04492906776820902', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='Epoch: 27/30 | Loss: 0.04660600281967505\\nEpoch: 28/30 | Loss: 0.04492906776820902\\nEpoch: 29/30 | Loss: 0.04336751992388834\\nEpoch: 30/30 | Loss: 0.04190996822760929', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"fig = plt.figure(figsize=(4, 3.5))\\n\\nplt.plot(epoch_data, loss_data, color='darkblue')\\nplt.xlabel('Epoch Number')\\nplt.ylabel('Cross Entropy')\\nplt.title('Cross Entropy (per batch)')\\n\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nepoch_data_avgd = epoch_data.reshape(20,-1).mean(axis=1)\\nloss_data_avgd = loss_data.reshape(20,-1).mean(axis=1)\\n\\nfig = plt.figure(figsize=(4, 3.5))\\n\\nplt.plot(epoch_data_avgd, loss_data_avgd, 'o--', color='darkblue')\\nplt.xlabel('Epoch Number')\\nplt.ylabel('Cross Entropy')\\nplt.title('Cross Entropy (avgd per epoch)')\\n\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nxs, ys = train_dataset[0: 2000]\\nxs = xs.to(device)\\nys = ys.to(device)\\n\\nyhats = model(xs).argmax(axis = 1)\\nyhats\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"yhats = model(xs).argmax(axis = 1)\\nyhats\\n\\ntensor([5, 0, 4,  ..., 5, 2, 0], device='cuda:0')\\n\\n\\n\\n\\nfig, axes = plt.subplots(3, 4, figsize=(12, 9))\\n\\nfor i in range(12):\\n    row = i // 4\\n    col = i % 4\\n    \\n    ax = axes[row, col]\\n    \\n    ax.imshow(xs.to('cpu')[i], cmap='Blues')\\n    ax.set_title(f'Predicted Digit: {yhats[i]}')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTesting\\n\\n\\n\\n\\xa0\\xa0\\xa0Now it‚Äôs time to test our model with unseen data. For this, I‚Äôll be using the test data loader and also plotting the predictions and the confusion matrix.\\n\\n\\n\\nxs, ys = test_dataset[:2000]\\nyhats = model(xs.to(device)).argmax(axis = 1)\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"xs, ys = test_dataset[:2000]\\nyhats = model(xs.to(device)).argmax(axis = 1)\\n\\n\\n\\nfig, axes = plt.subplots(3, 4, figsize=(12, 9))\\n\\nfor i in range(12):\\n    row = i // 4\\n    col = i % 4\\n    \\n    ax = axes[row, col]\\n    \\n    ax.imshow(xs.to('cpu')[i], cmap='Blues')\\n    ax.set_title(f'Predicted Digit: {yhats[i]}')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA confusion matrix is an important tool in machine learning that is used to evaluate the performance of a classification model. It is a table that compares the predicted values with the actual values and shows how many predictions are correct and incorrect per class.\\n\\n\\n\\nimages, true = test_dataset[:]\\npredictions = model(images.to(device)).argmax(axis = 1)\\ntrue = torch.argmax(true, dim=1)\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content=\"images, true = test_dataset[:]\\npredictions = model(images.to(device)).argmax(axis = 1)\\ntrue = torch.argmax(true, dim=1)\\n\\n\\n\\nscm = confusion_matrix(true.tolist(), predictions.tolist())\\nscm_normalized = np.round(scm/np.sum(scm, axis=1).reshape(-1, 1), 2)\\n\\nplt.figure(figsize=(8, 5))\\n\\nsns.heatmap(\\n    scm_normalized, \\n    cmap='Blues', \\n    annot=True, \\n    cbar_kws={\\n        'orientation': 'vertical'\\n    }\\n)\\n\\nplt.xticks(fontsize=12)\\nplt.yticks(fontsize=12)\\n\\nplt.title('Confusion Matrix\\\\n', fontsize=18)\\nplt.xlabel('\\\\nPrediction\\\\n', fontsize=16)\\nplt.ylabel('\\\\nTrue\\\\n', fontsize=16)\\n\\nplt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConclusion\", metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='plt.show()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0Today, we‚Äôve explored how to develop a solution for the MNIST dataset, and it‚Äôs important to note that there are also many other ways to approach this task. We could use TensorFlow instead, apply early stopping techniques, delve deeper into the evaluations, and more. However, we‚Äôve learned how to utilize PyTorch to aid in our deep learning development.\\n\\n\\nThe code developed here is available on my Github.\\n\\n\\n\\n\\xa0\\xa0\\xa0Thanks for reading, I‚Äôll see you in the next one ‚≠ê\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/recognizing-handwriting-digits/', 'title': 'Naomi Lago - Recognizing Handwriting Digits', 'description': ' Today, we will dive into deep learning once again as we construct a model capable of recognizing handwritten digits using the renowned MNIST dataset.', 'language': 'en'}),\n",
       " Document(page_content='Naomi Lago - Preprocessing Unstructured Data\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNaomi Lago\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n \\n\\nGithub\\n\\n\\n \\n\\nLinkedIn\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nPreprocessing Unstructured Data\\n\\n\\nReady for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.\\n        \\n\\n\\nDATA SCIENCE\\nNLP\\n\\n\\n\\n\\n\\nAuthor\\n\\nNaomi Lago \\n\\n\\n\\nPublished\\n\\nMay 1, 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Extracting and normalizing content from a diverse document types - like images, PDFs, PowerPoints, Word, etc., is a crucial step extensively used in RAG systems to enhance their performance by ensuring that the information fed into them is not only relevant but also contextually rich and well structured. By diving into this text, we’ll explore how this preprocessing step can be done using an API to set up a pipeline that handles three types of documents: Images, PDFs, and HTML.\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\nFine-Tuning vs RAG\\n\\n\\nUnstructured API', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='Table of Contents\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\nFine-Tuning vs RAG\\n\\n\\nUnstructured API\\n\\n\\n\\nPreprocessing a PowerPoint file\\n\\n\\n\\n\\nModularizing\\n\\n\\n\\nDefining the core components\\n\\n\\n\\nImage component\\n\\n\\nPDF component\\n\\n\\nHTML component\\n\\n\\n\\nAgnostic function\\n\\n\\nNext Steps\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\n\\xa0\\xa0\\xa0In the field of information processing and retrieval, the ability to extract, normalize, and structure content from various document types is essential for Retrieved-Augmented Generation (RAG) systems, requiring specialized methods for extraction and normalization of each type. Before we dive into the main goal of this dive, let’s first make a clear difference among LLMs, Fine-Tuning, and RAGs - which are things that people still mix up a bit.\\n\\n\\n\\nFine-Tuning vs RAG\\n\\n\\n\\n\\xa0\\xa0\\xa0Before getting to know more about each particularity between these two concepts, let’s state that Large Language Models (LLMs) are pre-trained models on vast datasets, enabling them to understand language structure, context, and semantics. They are foundation for both Fine-Tuning and RAG, offering a broad understanding of language - hence they can be fine-tuned for specific tasks or combined with RAG to enhance their performance.', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='When talking about Fine-Tuning, we’re talking about adapting an LLM behavior, writing style, or domain-specific knowledge to specific nuances, tones, or terminologies. It is particularly effective for achieving a deep alignment with vocabulary and conventions, making it ideal for specialized applications requiring high expertise. Hence, it is best when seeking to tailor the outputs closely to specific requirements and having extensive specific data.\\n\\n\\n\\n\\n\\nFine-tuning workflow\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Now RAG, in other hand, focus on enhancing LLMs by connecting them to external knowledge sources through retrieval mechanisms - excelling at incorporating dynamic external data, making it suitable for scenarios requiring up-to-date information or where context is crucial. It offers transparency in response generation by breaking down the process into distinct stages, which is beneficial for applications where interpretability is a priority.\\n\\n\\n\\n\\n\\nRAG workflow\\n\\n\\n\\n\\nUnstructured API', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='RAG workflow\\n\\n\\n\\n\\nUnstructured API\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, let’s talk about the solution we’re exploring today: the Unstructured API - a powerful tool that provides advanced document understanding and preprocessing capabilities, enabling RAG systems to better retrieve abd generate information from a wide range of document types across various domains. I’ve chosen this service because it offers a comprehensive set of features that can be easily integrated into existing pipelines, it’s used by leaders in AI such as Weaviate, LangChain, Yurts AI, etc., but most importantly, it has a free API for study purposes.\\n\\n\\n\\n\\nYour browser does not support the video tag.\\n\\n\\nUnstructured\\n\\n\\n\\n\\nFor further guidance on how to install and set up your environment, please refer to their documentation.\\n\\n\\n\\nPreprocessing a PowerPoint file\\n\\n\\n\\n\\xa0\\xa0\\xa0Alright, now let’s get a feel of how we can use the API on a PowerPoint file. Now, using a sample file I made on how to multiply matrices, I’ll import the necessary modules, read the secrets, and partition the file.\\n\\n\\nYou can download the PowerPoint file used here.\\n\\n\\n\\nfrom unstructured.partition.pptx import partition_pptx\\nfrom unstructured_client import UnstructuredClient\\nfrom loguru import logger\\nimport json', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"You can download the PowerPoint file used here.\\n\\n\\n\\nfrom unstructured.partition.pptx import partition_pptx\\nfrom unstructured_client import UnstructuredClient\\nfrom loguru import logger\\nimport json\\n\\nkeys = json.load(open('keys.json'))\\n\\nunstructured_api_key = keys['UNSTRUCTURED_API_KEY']\\nunstructured_base_url = keys['UNSTRUCTURED_BASE_URL']\\n\\ns = UnstructuredClient(\\n    api_key_auth=unstructured_api_key,\\n    server_url=unstructured_base_url,\\n)\\n\\npath = './how-to-multiply-matrices.pptx'\\n\\nres = partition_pptx(file=open(path, 'rb'))\\n\\nres[1:4]\\n\\n[<unstructured.documents.elements.PageBreak at 0x196b15facb0>,\\n <unstructured.documents.elements.Title at 0x196b15f9120>,\\n <unstructured.documents.elements.NarrativeText at 0x196b15f91e0>]\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='We can see that it’s pretty much it! We get a response from the API with a a list of Unstructured elements - which already spoils us that there are things related to page breaks, titles and narrative texts. Before seeing this result more clearly, refer to this table of available types of elements that can be processed:\\n\\n\\n\\n\\n\\nElement types\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally, let’s print the content of an element to see what we got here:\\n\\n\\n\\nprint(json.dumps(res[4].to_dict(), indent=2))', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='Element types\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally, let’s print the content of an element to see what we got here:\\n\\n\\n\\nprint(json.dumps(res[4].to_dict(), indent=2))\\n\\n{\\n  \"type\": \"NarrativeText\",\\n  \"element_id\": \"d7eb94d4cac7d41d58597655b1da567c\",\\n  \"text\": \"Not every pair of matrices can be multiplied. If you have two matrices, the number of columns in the first matrix MUST equal the number of rows in the second matrix\",\\n  \"metadata\": {\\n    \"category_depth\": 0,\\n    \"page_number\": 2,\\n    \"languages\": [\\n      \"eng\"\\n    ],\\n    \"parent_id\": \"5bc89aa807bab5a1ab4499371c48d959\",\\n    \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\\n  }\\n}\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0By transforming into a Python dictionary, we can see some valuable information:\\n\\n\\n\\ntype: The element type (referred from the table above).', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"By transforming into a Python dictionary, we can see some valuable information:\\n\\n\\n\\ntype: The element type (referred from the table above).\\n\\n\\nelement_id: A unique identifier for the document element.\\n\\n\\ntext: The actual content or text of the element, if any.\\n\\n\\nmetadata: Additional information about the element, divided into:\\n\\n\\n\\npage_number: The page number where the document is located.\\n\\n\\nlanguages: A sequence of language codes indicated the languages used in the element.\\n\\n\\nparent_id: The identifier of the parent element, if any.\\n\\n\\nfiletype: The element’s MIME type, indicating the file format.\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0How about we apply some conditions to gather what we are looking for? Maybe some e-mail, or even an element talking about the impossibility of multiplying some matrices?\\n\\n\\n\\n# Trying to find an e-mail, if any.\\n\\nresult = [elem for elem in res if elem.to_dict()['type'] == 'EmailAddress']\\n\\nif len(result) > 0: print(f'Found {len(result)} e-mail address elements! \\\\n')\\n\\nprint(json.dumps(result[0].to_dict(), indent=2))\\n\\nFound 1 e-mail address elements!\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='print(json.dumps(result[0].to_dict(), indent=2))\\n\\nFound 1 e-mail address elements! \\n\\n{\\n  \"type\": \"EmailAddress\",\\n  \"element_id\": \"53b62c1be18aa256c1a9891d48c3bab1\",\\n  \"text\": \"info@naomilago.com\",\\n  \"metadata\": {\\n    \"languages\": [\\n      \"eng\"\\n    ],\\n    \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\\n  }\\n}\\n\\n\\n\\n\\n# Trying to find something about the impossibility of multiplying matrices.\\n\\nfrom pprint import pprint\\n\\nresult = [elem for elem in res if \\'cannot be multiplied\\' in elem.to_dict()[\\'text\\'].lower()]', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='from pprint import pprint\\n\\nresult = [elem for elem in res if \\'cannot be multiplied\\' in elem.to_dict()[\\'text\\'].lower()]\\n\\nif len(result) > 0: \\n  print(f\\'\\'\\'Found {len(result)} result on page {\\n          result[0].to_dict()[\"metadata\"][\"page_number\"]\\n        }! \\\\n\\'\\'\\')\\n  \\npprint(result[0].to_dict()[\\'text\\'], width=132)\\n\\nFound 1 result on page 4! \\n\\n(\\'On the other hand, a 5 x 2 matrix cannot be multiplied by another 5 x 2 matrix even though they both have the same dimensions. \\'\\n \\'Here is a visual representation:\\')\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0Alright, alright! This was funny but still hard coded as the search for a specific case for matrices multiplication was matched exactly with our query. Maybe in the future we can dive into embeddings, and join knowledges from the vector databases that I talked in some of the previous dives we’ve had, such as:\\n\\n\\n\\nInformation Retrieval with Vector Search\\n\\n\\nVector Search with Facebook AI\\n\\n\\n\\n\\nModularizing', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"Information Retrieval with Vector Search\\n\\n\\nVector Search with Facebook AI\\n\\n\\n\\n\\nModularizing\\n\\n\\n\\n\\xa0\\xa0\\xa0Now, let’s make something more fun: modules. With them, we can package code into functional units, making it easier to understand, maintain, and reuse.\\n\\n\\n\\nDefining the core components\\n\\n\\n\\n\\xa0\\xa0\\xa0Before making the main agnostic function, that will be the module that preprocess files whether they are Images, PDFs, or HTML, we need to look into these individual components to check if there’s any particular need, as well as also build a core function for its type.\\n\\n\\n\\nImage component\\n\\n\\n\\n\\xa0\\xa0\\xa0The first I’ll do is an image preprocessor - that uses this image below as a sample, but can be used with any other image files.\\n\\n\\n\\n\\n\\nSample image\\n\\n\\n\\n\\nfrom unstructured_client import shared\\n\\npath = './data-preparation-process.jpg'\\n\\nreq = shared.PartitionParameters(\\n  files=shared.Files(\\n    content=open(path, 'rb').read(), \\n    file_name=path\\n  )\\n)\\n\\nres = s.general.partition(req)\\n\\n[_ for _ in res.elements if _['type'] == 'ListItem'][:2]\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"[{'type': 'ListItem',\\n  'element_id': '24ff8a800f313d0d3ce3eb8713d36748',\\n  'text': '- Articulating the problem',\\n  'metadata': {'filetype': 'image/jpeg',\\n   'languages': ['eng'],\\n   'page_number': 1,\\n   'parent_id': 'ca8e9b8913f7d28ac3bff568efbc7f3d',\\n   'filename': 'data-preparation-process.jpg'}},\\n {'type': 'ListItem',\\n  'element_id': 'bd5eebbcc1c1948d37d0f883c5c450d5',\\n  'text': '- Defining data required',\\n  'metadata': {'filetype': 'image/jpeg',\\n   'languages': ['eng'],\\n   'page_number': 1,\\n   'parent_id': 'ca8e9b8913f7d28ac3bff568efbc7f3d',\\n   'filename': 'data-preparation-process.jpg'}}]\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"def image_handler(\\n    path: str, \\n    client: UnstructuredClient, \\n    verbose: bool = False\\n  ) -> list[dict]:\\n  \\n  '''\\n  Handles images and returns a list of elements from the Unstructured API.\\n  \\n  Parameters: \\n    path (str, required): A valid path to an image file.\\n    client (UnstructuredClient, required): An instance of the UnstructuredClient class.\\n    verbose (bool, optional): Whether to log the process and its results.\\n    \\n  Returns:\\n    list[dict]: A list of dictionary elements from the API.\\n  '''\\n  \\n  if verbose: logger.info('Partitioning the image parameters...')\\n  \\n  req = shared.PartitionParameters(\\n    files=shared.Files(\\n      content=open(path, 'rb').read(), \\n      file_name=path\\n    )\\n  )\\n  \\n  if verbose: logger.info('Sending the request to the API...')\\n\\n  res = client.general.partition(req)\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"res = client.general.partition(req)\\n\\n  if verbose: logger.success('The image was successfully partitioned! \\\\n')\\n\\n  return list(res.elements)\\n\\n\\nres = image_handler(\\n  path='./badminton.png', \\n  client=s, \\n  verbose=True\\n)\\n\\nlogger.info(f'The partitioned Image has {len(res):,} elements!')\\nres[3:4]\\n\\n2024-05-01 19:10:56.231 | INFO     | __main__:image_handler:19 - Partitioning the image parameters...\\n2024-05-01 19:10:56.232 | INFO     | __main__:image_handler:28 - Sending the request to the API...\\n2024-05-01 19:11:00.598 | SUCCESS  | __main__:image_handler:32 - The image was successfully partitioned! \\n\\n2024-05-01 19:11:00.599 | INFO     | __main__:<module>:7 - The partitioned Image has 16 elements!\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"2024-05-01 19:11:00.599 | INFO     | __main__:<module>:7 - The partitioned Image has 16 elements!\\n\\n\\n[{'type': 'NarrativeText',\\n  'element_id': 'a9e65672c3378de02797f52e2b127f79',\\n  'text': 'WHAT DO WE KNOW ABOUT BADMINTON? It 1s a racket sport played using rackets to hit a shuttlecock across a net',\\n  'metadata': {'filetype': 'image/png',\\n   'languages': ['eng'],\\n   'page_number': 1,\\n   'parent_id': '1ea170c90190d980e2355ac3f4d4bbb4',\\n   'filename': 'badminton.png'}}]\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0In the above code, I first tried outside of any function just to make sure I was using the right imports - needing to also use the shared module from the client in order to partition the parameters and then pass into the client API. Finally, I turned it into a function component, containing a docstring and loggers for further documentation and debugging.\\n\\n\\n\\nPDF component\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='PDF component\\n\\n\\n\\n\\xa0\\xa0\\xa0Now it’s time for the PDF component - one of the most common type of unstructured data used for this purpose. As I’m a real passionate about Deep Learning, I’ll be using the famous open-source Dive into Deep Learning book that can be found here.\\n\\n\\nNOTE: Since the book has +1k pages, it can take a while to process it all with the free API.', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"def pdf_handler(\\n    path: str, \\n    client: UnstructuredClient, \\n    verbose: bool = False\\n  ) -> list[dict]:\\n  \\n  '''\\n  Handles PDFs and returns a list of elements from the Unstructured API.\\n  \\n  Parameters: \\n    path (str, required): A valid path to a PDF file.\\n    client (UnstructuredClient, required): An instance of the UnstructuredClient class.\\n    verbose (bool, optional): Whether to log the process and its results.\\n    \\n  Returns:\\n    list[dict]: A list of dictionary elements from the API.\\n  '''\\n  \\n  if verbose: logger.info('Partitioning the PDF parameters...')\\n  \\n  req = shared.PartitionParameters(\\n    files=shared.Files(\\n      content=open(path, 'rb').read(), \\n      file_name=path\\n    )\\n  )\\n  \\n  if verbose: logger.info('Sending the request to the API...')\\n\\n  res = client.general.partition(req)\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"res = client.general.partition(req)\\n\\n  if verbose: logger.success('The PDF was successfully partitioned! \\\\n')\\n\\n  return list(res.elements)\\n\\n\\nres = pdf_handler(\\n  path='./dive-into-deep-learning.pdf', \\n  client=s, \\n  verbose=True\\n)\\n\\nlogger.info(f'The partitioned PDF has {len(res):,} elements!')\\n[_ for _ in res if _['type'] == 'ListItem'][5:6]\\n\\n2024-05-01 19:11:00.637 | INFO     | __main__:pdf_handler:19 - Partitioning the PDF parameters...\\n2024-05-01 19:11:00.653 | INFO     | __main__:pdf_handler:28 - Sending the request to the API...\\n2024-05-01 19:14:59.602 | SUCCESS  | __main__:pdf_handler:32 - The PDF was successfully partitioned! \\n\\n2024-05-01 19:14:59.609 | INFO     | __main__:<module>:7 - The partitioned PDF has 23,042 elements!\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"2024-05-01 19:14:59.609 | INFO     | __main__:<module>:7 - The partitioned PDF has 23,042 elements!\\n\\n\\n[{'type': 'ListItem',\\n  'element_id': '60de89542339e0337ca6cfbd58127812',\\n  'text': '3. Tweak the knobs to make the model perform better as assessed on those examples.',\\n  'metadata': {'languages': ['eng'],\\n   'page_number': 44,\\n   'parent_id': 'b605350bc00209520b7cd8f546322663',\\n   'filename': 'dive-into-deep-learning.pdf',\\n   'filetype': 'application/pdf'}}]\\n\\n\\n\\n\\nHTML component\\n\\n\\n\\n\\xa0\\xa0\\xa0Finally, we can make our HTML component handler. For this, I’ll use a sample from the own Unstructured documentation welcome page.\\n\\n\\n\\nfrom unstructured.partition.html import partition_html\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"Finally, we can make our HTML component handler. For this, I’ll use a sample from the own Unstructured documentation welcome page.\\n\\n\\n\\nfrom unstructured.partition.html import partition_html\\n\\ndef html_handler(\\n    path: str, \\n    verbose: bool = False\\n  ) -> list[dict]:\\n  \\n  '''\\n  Handles HTMLs and returns a list of elements from the Unstructured API.\\n  \\n  Parameters: \\n    path (str, required): A valid path to an HTML file.\\n    verbose (bool, optional): Whether to log the process and its results.\\n    \\n  Returns:\\n    list[dict]: A list of dictionary elements from the API.\\n  '''\\n  \\n  if verbose: logger.info('Partitioning the HTML content...')\\n  \\n  res = [_.to_dict() for _ in partition_html(file=open(path, 'rb'))]\\n\\n  if verbose: logger.success('The HTML was successfully partitioned! \\\\n')\\n\\n  return res\\n\\n\\nres = html_handler(\\n  path='./unstructured-documentation.html', \\n  verbose=True\\n)\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"return res\\n\\n\\nres = html_handler(\\n  path='./unstructured-documentation.html', \\n  verbose=True\\n)\\n\\nlogger.info(f'The partitioned HTML has {len(res):,} elements!')\\n[_ for _ in res if _['type'] == 'ListItem'][0]\\n\\n2024-05-01 19:14:59.645 | INFO     | __main__:html_handler:19 - Partitioning the HTML content...\\n2024-05-01 19:14:59.744 | SUCCESS  | __main__:html_handler:23 - The HTML was successfully partitioned! \\n\\n2024-05-01 19:14:59.749 | INFO     | __main__:<module>:6 - The partitioned HTML has 27 elements!\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"2024-05-01 19:14:59.749 | INFO     | __main__:<module>:6 - The partitioned HTML has 27 elements!\\n\\n\\n{'type': 'ListItem',\\n 'element_id': '6669f7a5f388bd16dbfb213a8649e875',\\n 'text': 'Unstructured-IO/unstructured',\\n 'metadata': {'category_depth': 1,\\n  'link_texts': [None],\\n  'link_urls': ['https://github.com/Unstructured-IO/unstructured'],\\n  'page_number': 1,\\n  'languages': ['eng'],\\n  'parent_id': 'e52b003250d3fee283c6679e2ebb8427',\\n  'filetype': 'text/html'}}\\n\\n\\n\\n\\nAgnostic function\\n\\n\\n\\n\\xa0\\xa0\\xa0Last but not least, we can create our main agnostic function that will be able to handle all the types of files we’ve seen above, and will be the one that could be used to preprocess these data.\\n\\n\\nNOTE: These file extensions are just examples, and you can use any other type of file that the API supports. For a complete list of supported file types, please refer to the documentation.\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"def data_preprocessor(\\n    path: str, type: str,\\n    client: UnstructuredClient, \\n    verbose: bool = True\\n  ) -> list[dict]:\\n  \\n  '''\\n  Preprocesses unstructured data and returns a list of elements from the Unstructured API.\\n  \\n  Parameters: \\n    path (str, required): A valid path to an HTML file.\\n    type (str, required): The type of the unstructured data.\\n    client (UnstructuredClient, required): An instance of the UnstructuredClient class.\\n    verbose (bool, optional): Whether to log the process and its results.\\n    \\n  Returns:\\n    list[dict]: A list of dictionary elements from the API.\\n  '''\\n  \\n  if str.lower(type) in {'image', 'pdf', 'html'}:\\n    if str.lower(type) == 'image':\\n      if verbose: logger.info(f'Processing an image...')\\n      return image_handler(path=path, client=client, verbose=verbose)\\n    elif str.lower(type) == 'pdf':\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"return image_handler(path=path, client=client, verbose=verbose)\\n    elif str.lower(type) == 'pdf':\\n      if verbose: logger.info(f'Processing a PDF...')\\n      return pdf_handler(path=path, client=client, verbose=verbose)\\n    elif str.lower(type) == 'html':\\n      if verbose: logger.info(f'Processing an HTML...')\\n      return html_handler(path=path, verbose=verbose)\\n  else:\\n    if verbose: logger.error(f'{str.upper(type)} is not a currently supported file!')\\n    raise SystemError\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"That’s pretty amuch it, and now we can test it out on other files - to make sure it works well on a variety of files within our list of possible extensions.\\n\\n\\n\\nimage_res = data_preprocessor(\\n  path='./technical-writer.png',\\n  type='image',\\n  client=s,\\n)\\n\\npdf_res = data_preprocessor(\\n  path='./snow-white-and-the-seven-dwarfs.pdf',\\n  type='pdf',\\n  client=s,\\n)\\n\\nhtml_res = data_preprocessor(\\n  path='./dia-do-trabalhador.html',\\n  type='html',\\n  client=s,\\n)\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"html_res = data_preprocessor(\\n  path='./dia-do-trabalhador.html',\\n  type='html',\\n  client=s,\\n)\\n\\n2024-05-01 19:14:59.783 | INFO     | __main__:data_preprocessor:22 - Processing an image...\\n2024-05-01 19:14:59.784 | INFO     | __main__:image_handler:19 - Partitioning the image parameters...\\n2024-05-01 19:14:59.784 | INFO     | __main__:image_handler:28 - Sending the request to the API...\\n2024-05-01 19:15:02.577 | SUCCESS  | __main__:image_handler:32 - The image was successfully partitioned!\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='2024-05-01 19:15:02.578 | INFO     | __main__:data_preprocessor:25 - Processing a PDF...\\n2024-05-01 19:15:02.579 | INFO     | __main__:pdf_handler:19 - Partitioning the PDF parameters...\\n2024-05-01 19:15:02.580 | INFO     | __main__:pdf_handler:28 - Sending the request to the API...\\n2024-05-01 19:15:03.094 | SUCCESS  | __main__:pdf_handler:32 - The PDF was successfully partitioned! \\n\\n2024-05-01 19:15:03.095 | INFO     | __main__:data_preprocessor:28 - Processing an HTML...\\n2024-05-01 19:15:03.095 | INFO     | __main__:html_handler:19 - Partitioning the HTML content...\\n2024-05-01 19:15:03.341 | SUCCESS  | __main__:html_handler:23 - The HTML was successfully partitioned! \\n\\n\\n\\n\\n\\nimage_res[4]', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"image_res[4]\\n\\n{'type': 'Image',\\n 'element_id': '2634c3dd82e6aad631e772d02e4c46ef',\\n 'text': 'How might | prepare for What skills may be necessary? this career? -Build a writing portfolio Strong writing skills Good interviewing skills -Practice writing by doing unpaid or freelance worl Ability to explain math and science in simple language',\\n 'metadata': {'filetype': 'image/png',\\n  'languages': ['eng'],\\n  'page_number': 1,\\n  'filename': 'technical-writer.png'}}\\n\\n\\n\\n\\npdf_res[3]\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"{'type': 'NarrativeText',\\n 'element_id': '49f8e5d385825f66d315ece00dc9f77a',\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"'text': '“Snow White”’s historical credentials are well known: as the Disney studio’s first feature-length film, it marked a significant turning point for Walt Disney himself, for the Disney studio, for the art of anima- tion, and to some extent for American films in gen- eral. Like most celebrated “firsts,” it wasn’t really the first animated feature. But it’s fair to say that no earli- er feature had showcased the full range of animation technique in the way that “Snow White” did, nor so combined it with rich color, an infectious musical score, and an absorbing, carefully developed story. Instead of creating an art-house curio, bidding for attention on its novelty value alone, Walt boldly jumped into the center of the arena, crafting an ani- mated feature that could compete with the major stu- dios’ live-action features on their own terms. The sheer audacity of this concept in 1937 is impressive enough, but Walt didn’t stop with the concept. So fully did “Snow White” realize its goals that it scored a spectacular worldwide success at the box office, forcing the rest of the film industry to pay attention, and forever changing the course of the Disney stu- dio.',\\n 'metadata': {'languages': ['eng'],\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"'metadata': {'languages': ['eng'],\\n  'page_number': 1,\\n  'parent_id': 'd7452354e7eacad5675c5086fe277cc9',\\n  'filename': 'snow-white-and-the-seven-dwarfs.pdf',\\n  'filetype': 'application/pdf'}}\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content=\"html_res[-51]\\n\\n{'type': 'NarrativeText',\\n 'element_id': 'dd03218422acdfe04a165e071f121caf',\\n 'text': 'Os cinquenta mil manifestantes escutaram então os discursos de vários líderes social-democratas e liberais, entre os quais August Palm e Hjalmar Branting.[13]',\\n 'metadata': {'emphasized_text_contents': ['[', ']'],\\n  'emphasized_text_tags': ['span', 'span'],\\n  'link_texts': ['August Palm', 'Hjalmar Branting', None],\\n  'link_urls': ['/wiki/August_Palm',\\n   '/wiki/Hjalmar_Branting',\\n   '#cite_note-forstamaj-13'],\\n  'page_number': 1,\\n  'languages': ['por'],\\n  'parent_id': 'b4b4563a9eb59295e2032feb8d15ae05',\\n  'filetype': 'text/html'}}\", metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='Yay! We got our agnostic function ready-to-use. I also made sure to include another language (portuguese) in the HTML content to remember that the Unstructured services handles different languages very well, and the content itself is related to the International Workers’ Day that is celebrated today.\\n\\n\\n\\n\\nNext Steps\\n\\n\\n\\n\\xa0\\xa0\\xa0Before we finish today’s dive, I would like to talk about the next steps that could be followed after this preprocessing - such as connecting to a vector database, generating embeddings to feed RAG systems, or even fine-tune some LLMs around this world. For these cases, chunking is also an important thing and using parents’ information within each element is also very useful for making a strong knowledge base. Now, let’s see the next steps that could be undertaken post-processing:\\n\\n\\n\\nConnecting to a Vector Database: After preprocessing, the data is prepared for efficient storage and retrieval. A vector database is crucial for managing these embeddings, facilitating quick and accurate information retrieval.\\n\\n\\nGenerating Embeddings for RAG Systems: The preprocessed data is transformed into embeddings, which are semantically rich representations. These embeddings are vital for understanding the context and relevance of the information within the RAG system.', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='Fine-Tuning Large Language Models (LLMs): With the embeddings generated and stored in the vector database, the next step involves fine-tuning LLMs. This process enhances the model’s ability to generate coherent and contextually accurate responses to user queries.\\n\\n\\n\\n\\xa0\\xa0\\xa0Each of these plays a pivotal role in the overall process, contributing to the development of a robust and efficient information retrieval and generation system. By following these steps, we can leverage the power of preprocessing, vector databases, embeddings, and LLMs to create a comprehensive solution for handling and processing large volumes of data.\\n\\n\\n\\nConclusion\\n\\n\\n\\n\\xa0\\xa0\\xa0What a dive, huh?! We started by understanding the difference between Fine-Tuning and RAG systems, then talked about this amazing service called Unstructured as a powerful tool for advanced document preprocessing and understanding. With the practical examples, we could see, through demonstrations, how to handle different file types using their API - using a modular approach for efficient content extraction and setting the stage for further enhancements.\\n\\n\\n\\xa0\\xa0\\xa0Looking ahead, we can already outline potential next steps that are essential for enhancing these systems’ performance, enabling them to retrieve and generate contextually rich responses from a vast array of data sources. Personally, I love Information Retrieval, so I would be really glad to bring one or two of these steps in a future dive - so, stay tuned :)', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='Thanks for reading, I’ll see you in the next one ⭐\\n\\n \\n\\n \\n\\n\\n\\n      \\xa0\\n    \\n\\nCopyright 2024, Naomi Lago\\nCookie Preferences', metadata={'source': 'https://naomilago.com/posts/preprocessing-unstructured-data/', 'title': 'Naomi Lago - Preprocessing Unstructured Data', 'description': ' Ready for another dive? Today we’ll be exploring a vital component in building today’s powerful LLMs, playing a significant role in RAG systems.', 'language': 'en'}),\n",
       " Document(page_content='naomilago (Naomi Lago) ¬∑ GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nCopilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Copilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nFor\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Education\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy Solution\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Search or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n              Sign in\\n            \\n\\n\\n              Sign up\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nnaomilago\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    256\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nnaomilago\\n \\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Projects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nnaomilago\\n \\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Naomi Lago\\n        \\n\\n          naomilago\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\nData Scientist @ Nestl√© \\n\\n\\n\\n\\n\\n\\n177\\n          followers\\n        · \\n206\\n          following\\n\\n\\n\\n\\n\\nNestl√©\\n\\n\\nS√£o Paulo, BR\\n\\n\\n\\n  13:11\\n  (UTC -03:00)\\n\\n\\n\\n\\n\\nhttps://naomilago.com\\n\\nX\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n@naomilago\\n\\n\\n\\n\\nAchievementsBetaSend feedbackAchievementsBetaSend feedback\\nHighlights\\n\\n\\n\\n\\n  Pro\\n\\n\\n\\n\\nOrganizations\\n\\n\\n \\n\\n\\n\\n\\n        Block or Report', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"Pro\\n\\n\\n\\n\\nOrganizations\\n\\n\\n \\n\\n\\n\\n\\n        Block or Report\\n      \\n\\n\\n\\n\\n\\n\\n\\nBlock or report naomilago\\n\\n\\n\\n\\n\\n\\nBlock user\\n\\n            Prevent this user from interacting with your repositories and sending you notifications.\\n          Learn more about blocking users.\\n        \\n\\n              You must be logged in to block users.\\n            \\n\\n\\n\\n        Add an optional note:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you.\\n\\n\\n\\n          Block user\\n        \\n \\n\\nReport abuse\\n\\n        Contact GitHub support about this user‚Äôs behavior.\\n        Learn more about reporting abuse.\\n      \\nReport abuse\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\", metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    256\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnaomilago/README.md\\n\\n\\nHey there, welcome to my Github profile \\uf8ffüëã\\n\\nMy name is Naomi Lago and my pronouns are she/her. I study web development since 2018 and I'm always looking for new ways to improve. I'm currently undergraduating in Mathematics and working as a Data Scientist at Nestl√©. I'm also interested in Cybersecurity and Cloud.\\n\\uf8ffüå± I‚Äôm currently studying Python, Deep Learning, Machine Learning, and focusing on Natural Language Processing (NLP).\\n\\uf8ffüì´ You can find me @\\n\\n\\n\\uf8ffüåé Website\\n\\n\\n\\uf8ffüë©\\uf8ffüèº‚Äç\\uf8ffüíª LinkedIn\\n\\n\\n\\uf8ffüí¨ Telegram\\n\\n\\n\\uf8ffüë• Facebook\", metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"\\uf8ffüë©\\uf8ffüèº‚Äç\\uf8ffüíª LinkedIn\\n\\n\\n\\uf8ffüí¨ Telegram\\n\\n\\n\\uf8ffüë• Facebook\\n\\n\\n\\uf8ffüì© Email\\n\\n\\n‚ö° Fun facts:\\n\\n\\n\\uf8ffüß† I love Psychology. I love the idea of studying human minds and behaviors, so me as a social psychologist may still be a possible thing.\\n\\n\\n\\uf8ffüéª And I'm also passionate about violins. I studied for one year long when I was 15 and I take it as a hobby.\\n\\n\\n\\n\\n\\n\\n\\n\\n      Popular repositories\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Pet-Paws\\n              \\nPet-Paws \\nPublic\\n\\n\\n\\n            This project is a bridge between those who want to donate or adopt pets. Don't buy, ADOPT!\\n          \\n\\n\\n\\nTypeScript\\n\\n\\n\\n\\n\\n                15\", metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='TypeScript\\n\\n\\n\\n\\n\\n                15\\n              \\n\\n\\n\\n\\n                1\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                BeTheHero\\n              \\nBeTheHero \\nPublic\\n\\n\\n\\n            Projeto desenvolvido paralelo a Semana Omnistack 11.0 pela Rocketeat \\n          \\n\\n\\n\\nJavaScript\\n\\n\\n\\n\\n\\n                6\\n              \\n\\n\\n\\n\\n                1\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                DesafiosLaunchbase\\n              \\nDesafiosLaunchbase \\nPublic\\n\\n\\n\\n\\n\\n\\n\\nJavaScript', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='JavaScript\\n\\n\\n\\n\\n\\n                4\\n              \\n\\n\\n\\n\\n                2\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                NaomiLago\\n              \\nNaomiLago \\nPublic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                4\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                fluttery\\n              \\nfluttery \\nPublic\\n\\n\\n\\n            This repository aims to store the source code of a quiz app about Flutter.\\n          \\n\\n\\n\\nDart\\n\\n\\n\\n\\n\\n                4', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"Dart\\n\\n\\n\\n\\n\\n                4\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                sentimetrix\\n              \\nsentimetrix \\nPublic\\n\\n\\n\\n            Sentimetrix is a sentiment analysis tool that provides real-time insights into your audience's sentiment. Our AI-powered algorithms categorize opinions, attitudes, and emotions from various sources‚Ä¶\\n          \\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n                4\\n              \\n\\n\\n\\n\\n                1\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Something went wrong, please refresh the page to try again.\\n          If the problem persists, check the GitHub status page\\n          or contact support.\", metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Footer\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can‚Äôt perform that action at this time.', metadata={'source': 'https://github.com/naomilago/', 'title': 'naomilago (Naomi Lago) ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='naomilago (naomilago) / Repositories ¬∑ GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nCopilot\\n        Write better code with AI', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Copilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nFor\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Education\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy Solution\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Search or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n              Sign in\\n            \\n\\n\\n              Sign up\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nnaomilago\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    256\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nnaomilago\\n \\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Stars\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nnaomilago\\n \\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Naomi Lago\\n        \\n\\n          naomilago\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\uf8ffüåç\\n\\n\\nBuilding my world\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\nData Scientist @ Nestl√© \\n\\n\\n\\n\\n\\n\\n177\\n          followers\\n        · \\n206\\n          following\\n\\n\\n\\n\\n\\nNestl√©\\n\\n\\nS√£o Paulo, BR\\n\\n\\n\\n  13:12\\n  (UTC -03:00)\\n\\n\\n\\n\\n\\nhttps://naomilago.com\\n\\nX\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n@naomilago\\n\\n\\n\\n\\nAchievementsBetaSend feedbackAchievementsBetaSend feedback\\nHighlights\\n\\n\\n\\n\\n  Pro\\n\\n\\n\\n\\nOrganizations\\n\\n\\n \\n\\n\\n\\n\\n        Block or Report\\n      \\n\\n\\n\\n\\n\\n\\n\\nBlock or report naomilago\\n\\n\\n\\n\\n\\n\\nBlock user', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"Pro\\n\\n\\n\\n\\nOrganizations\\n\\n\\n \\n\\n\\n\\n\\n        Block or Report\\n      \\n\\n\\n\\n\\n\\n\\n\\nBlock or report naomilago\\n\\n\\n\\n\\n\\n\\nBlock user\\n\\n            Prevent this user from interacting with your repositories and sending you notifications.\\n          Learn more about blocking users.\\n        \\n\\n              You must be logged in to block users.\\n            \\n\\n\\n\\n        Add an optional note:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you.\\n\\n\\n\\n          Block user\\n        \\n \\n\\nReport abuse\\n\\n        Contact GitHub support about this user‚Äôs behavior.\\n        Learn more about reporting abuse.\\n      \\nReport abuse\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\", metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    74\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    256\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Type\\n\\n                All\\n              \\n\\n\\n\\n\\n\\nSelect type\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n\\nSources\\n\\n\\n\\n\\n\\n\\nForks\\n\\n\\n\\n\\n\\n\\nArchived\\n\\n\\n\\n\\n\\n\\nCan be sponsored\\n\\n\\n\\n\\n\\n\\nMirrors\\n\\n\\n\\n\\n\\n\\nTemplates\\n\\n\\n\\n\\n\\n\\n Language\\n\\n                  All\\n                \\n\\n\\n\\n\\n\\nSelect language\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n\\nHTML\\n\\n\\n\\n\\n\\n\\nJavaScript\\n\\n\\n\\n\\n\\n\\nCSS\\n\\n\\n\\n\\n\\n\\nTypeScript\\n\\n\\n\\n\\n\\n\\nPHP\\n\\n\\n\\n\\n\\n\\nDart\\n\\n\\n\\n\\n\\n\\n Sort', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Python\\n\\n\\n\\n\\n\\n\\nHTML\\n\\n\\n\\n\\n\\n\\nJavaScript\\n\\n\\n\\n\\n\\n\\nCSS\\n\\n\\n\\n\\n\\n\\nTypeScript\\n\\n\\n\\n\\n\\n\\nPHP\\n\\n\\n\\n\\n\\n\\nDart\\n\\n\\n\\n\\n\\n\\n Sort\\n\\n                Last updated\\n              \\n\\n \\n\\n\\nSelect order\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLast updated\\n\\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        diver\\nPublic\\n\\n\\n\\n\\n          This repository houses the code for Diver, my AI-powered bot designed to assist users in exploring the world of Data Science - with some technical knowledge and insights from my own blog at your fi‚Ä¶\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n        Updated May 9, 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        fitness-tracker\\nPublic\\n\\n\\n\\n\\n          Processing, analysing and modeling fitness data to classify barbell exercises and count repetitions\\n        \\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Feb 14, 2024', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"Python\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Feb 14, 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Falconize\\nPublic\\n\\n\\n\\n\\n          An Applied Data Science Capstone project offered by IBM on Coursera.\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Dec 27, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        NaomiLago\\nPublic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          4\\n        \\n\\n\\n\\n        Updated Oct 6, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Mentoria_Python\\nPublic\\n\\n\\n\\n\\n          This repo aims to be a place where I'll be sharing notebooks for the mentorships on Python programming\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Oct 6, 2023\", metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Jupyter Notebook\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Oct 6, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        detecting-programming-lanugages\\nPublic\\n\\n\\n\\n\\n          This repo aims to be a place where I can practice SpaCy for NER of programming languages\\n        \\n\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Sep 18, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        news_classifier\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for an NLP project that aims to classify news headlines\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Aug 23, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        naomilago.com\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for my personal website\\n        \\n\\n\\n\\n\\nHTML', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='This repository aims to store code for my personal website\\n        \\n\\n\\n\\n\\nHTML\\n\\n\\n\\n\\n\\n        Updated Aug 19, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        spam_classification\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for a spam_classification model\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Aug 18, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        lr_with_nn\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for a Linear Regression using Gradient Descent\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Aug 13, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        BERTIFIER\\nPublic', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Updated Aug 13, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        BERTIFIER\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the page of BERTIFIER - a simple page that hosts a notebook on BERTimbau (BERT in Portuguese) project\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n        Updated Aug 5, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        NLP-Text-Generation\\nPublic\\n\\n\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n        Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        micrograd\\nPublic\\n\\n\\n\\n\\n          This repo aims to store code for a refresher of backprob, gradients and mlp concepts\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        derivatives\\nPublic', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        derivatives\\nPublic\\n\\n\\n\\n\\n          This repo aims to be a place where I can train/practice derivative fundamentals\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        mnist\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for the MNIST image (of numbers) classification\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jul 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        similarity_search\\nPublic\\n\\n\\n\\n\\n          This repository aims to be a place where I can test/explore similarity search, using FAISS and ChromaDB\\n        \\n\\n\\n\\n\\nJupyter Notebook', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"Jupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jul 9, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        sentimetrix\\nPublic\\n\\n\\n\\n\\n          Sentimetrix is a sentiment analysis tool that provides real-time insights into your audience's sentiment. Our AI-powered algorithms categorize opinions, attitudes, and emotions from various sources‚Ä¶\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n          4\\n        \\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jun 30, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        topify\\nPublic\", metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Updated Jun 30, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        topify\\nPublic\\n\\n\\n\\n\\n          Topify is an open-source project that aims to simplify and automate topic classification for text-based data. This GitHub repository hosts the codebase for the Topify app, a powerful tool that util‚Ä¶\\n        \\n\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Jun 22, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        zero-shot-classification\\nPublic\\n\\n\\n\\n\\n          This repository is a place where I can practice Zero Shot Classification using Python\\n        \\n\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Jun 22, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        translatable\\nPublic', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"Updated Jun 22, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        translatable\\nPublic\\n\\n\\n\\n\\n          This repository aims to store code for a multilingual translation app\\n        \\n\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Jun 19, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        text-classification\\nPublic\\n\\n\\n\\n\\n          This repository aims to be a place to store code of a text-classification app. There's no name defined initially, but later there'll be a name, a description and a screenshot showing how to use.\\n        \\n\\n\\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Jun 18, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        pt_lemmatizer\\nPublic\\n\\n\\n\\n\\n          This repo aims to store code for a Portuguese Lemmatizer, a PyPI package.\\n        \\n\\n\\n\\n  python\\n\\n\\n  nlp\\n\\n\\n  lemmatizer\\n\\n\\n  portuguese\", metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='python\\n\\n\\n  nlp\\n\\n\\n  lemmatizer\\n\\n\\n  portuguese\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nMIT License\\n        \\n\\n\\n        Updated Jun 8, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        chatbot-nao-web\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the code for Nao chatbot working for web.\\n        \\n\\n\\n\\n\\nJavaScript\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Apr 9, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        text-summarizer\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the code of a text summarizer app\\n        \\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Mar 7, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        codex_chatbot\\nPublic', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Updated Mar 7, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        codex_chatbot\\nPublic\\n\\n\\n\\n\\n          This repository aims to store the code of a chatbot using the Codex from OpenAI\\n        \\n\\n\\n\\n\\nCSS\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Mar 7, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        buscador_de_similaridades\\nPublic\\n\\n\\n\\n\\n          Esta ferramenta de intelig√™ncia artificial foi projetada para ajudar os usu√°rios a identificar e explorar conex√µes entre palavras, frases e conceitos - com a mais recente tecnologia de processament‚Ä¶\\n        \\n\\n\\n\\n\\nHTML\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Mar 5, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        chatbot-nao\\nPublic', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content=\"Updated Mar 5, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        chatbot-nao\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the code of a chatbot called Nao. This is my first chatbot ever and I'm happy to share it open source.\\n        \\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 26, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        pride-research\\nPublic\\n\\n\\n\\n\\n          This notebook aims to store the notebook and the data I used to analyse demographic and social data around LGBTQ+ and black people. \\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 20, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        salary-predictor\\nPublic\", metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Updated Feb 20, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        salary-predictor\\nPublic\\n\\n\\n\\n\\n          This repo aims to store the notebook of a simple linear regression - where predicts the salary based on the years of experience. The dataset was provided by Allena on Kaggle.\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\n\\n          1\\n        \\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 18, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        mental-health-classifier\\nPublic\\n\\n\\n\\n\\n          This notebook aims to classify sentences on mental health - whether is healthy or not. It is based on the Mental Health Corpus dataset provided by Reihaneh Namdari on Kaggle.\\n        \\n\\n\\n\\n\\nJupyter Notebook\\n\\n\\n\\n\\nCreative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 17, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious Next', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'}),\n",
       " Document(page_content='Creative Commons Zero v1.0 Universal\\n        \\n\\n\\n        Updated Feb 17, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious Next\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can‚Äôt perform that action at this time.', metadata={'source': 'https://github.com/naomilago?tab=repositories', 'title': 'naomilago (naomilago) / Repositories ¬∑ GitHub', 'description': 'Data Scientist @ Nestl√© . naomilago has 74 repositories available. Follow their code on GitHub.', 'language': 'en'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "  chunk_size=300,\n",
    "  chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a href='https://python.langchain.com/docs/integrations/vectorstores/'>Vector Store</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x21aed7b3b20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=embeddings,\n",
    "  persist_directory='../data/vector_store'\n",
    ")\n",
    "\n",
    "vector_store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
